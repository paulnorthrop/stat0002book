```{r, figsetup9, include=FALSE}
knitr::opts_chunk$set(
  fig.align = 'center',
  out.width = '75%'
)
```

```{r, echo = FALSE}
include_cropped_graphics <- function(x) {
  knitr::include_graphics(knitr::plot_crop(x))
#  knitr::include_graphics(x)
}
```

# Correlation {#correlationchapter}

Now we consider the situation where objects are sampled randomly from a population,  and, for each object, we observe the values of random variables $X$ and $Y$.  Therefore, unlike regression sampling, where the values of $X$ are chosen by an experimenter, both $X$ and $Y$ are random variables. Unlike a regression problem, we treat $X$ and $Y$ symmetrically, that is, we do not identify one variable as a response and one as explanatory. We are interested in the relationship between $X$ and $Y$. How are they associated, and how strongly?

When estimating correlation from data we will concentrate on the sample correlation coefficient $r$ defined in equation \@ref(eq:corr) of Section \@ref(corr1), which is a measure of the strength of **linear** association.  However, the general comments that we make will also apply to Spearman's rank correlation coefficient $r_S$ (also defined in section \@ref(corr1).

## Correlation: a measure of linear association

**UK/USA/Canadian exchange rates**

Figure \@ref(fig:exchangeraw) is a plot the UK Pound/Canadian dollar exchange rate vs. the UK Pound/US dollar exchange rate for each day between 2nd January 1997 and 21 November 2000.

```{r echo=FALSE, exchangeraw, fig.show='hold', fig.cap='Value of 1 UK pound in Canadian dollars against value of 1 UK pound in US dollars.'}
include_cropped_graphics("images/exchange_raw.png")
```

The relationship between these two variables is complicated.  However, when dealing with exchange rates it is common to (a) work with $\log$(exchange rate), and (b) use the **change** in $\log$(exchange rate) from one day to the next. This produces quantities called **log-returns**.  In Figure \@ref(fig:exchangetrans) the log-returns for the UK Pound/Canadian dollar exchange rate ($Y$) are plotted against the log-returns $X$ for the UK Pound/US exchange rate ($X$).

```{r echo=FALSE, exchangetrans, fig.show='hold', fig.cap='Plot of $Y$ against $X$.  The vertical line is drawn at the sample mean of the $X$ data and the horizontal line at the sample mean of the $Y$ data.'}
include_cropped_graphics("images/exchange.png")
```

This plot has a much clearer pattern.  There is positive, approximately linear, association between $Y$ and $X$. Large values of the variables tend to occur together, as do small values. There are more points in the quadrants labelled B and C than in A and D. This positive association is due to the link between the US and Canadian financial markets. If you are investing in these markets it is important that you are aware of this positive association, since it means that when you lose money you are likely to lose it in both the US market and the Canadian market.

Figure \@ref(fig:exchangetrans) suggests that $X$ and $Y$ are approximately positively, linearly associated.  Correlation measures the strength of this linear association.

## Covariance and correlation

**Definition**. Let $\mu_X=\mbox{E}(X)$ and $\mu_Y=\mbox{E}(Y)$. The **covariance** $\mbox{cov}(X,Y)$ between two random variables $X$ and $Y$ is given by
$$
\mbox{cov}(X,Y)=\mbox{E}\left[\left(X-\mu_X\right)\left(Y-\mu_Y\right)\right]=\mbox{E}(XY)-\mbox{E}(X)\mbox{E}(Y). 
$$
The value of $\mbox{cov}(X,Y)$ depends on the scale of measurement of $X$ and $Y$.  For example, if we were to multiply all the values of $X$ by 2 then $\mbox{cov}(X,Y)$ is also multiplied by 2. Therefore we define a standardised version of covariance: correlation.

**Definition**.  The **correlation (coefficient)** between two random variables $X$ and $Y$ is given by
$$
\rho = \mbox{corr}(X,Y) = \frac{\mbox{cov}(X,Y)}{\sqrt{\mbox{var}(X)\mbox{var}(Y)}} 
= \frac{\mbox{E}\left[\left(X-\mu_X\right)\left(Y-\mu_Y\right)\right]}{\sqrt{\mbox{var}(X)\mbox{var}(Y)}}, 
$$
provided that both $\mbox{var}(X)$ and $\mbox{var}(Y)$ are positive. Correlation is dimensionless (it has no units) and its value does not change if we scale $X$ and/or $Y$.  Correlation (and covariance) are measures of **linear** association between random variables.  They are a sensible measure of the strength of association between two random variables only if there is a linear relationship between them.

Interpretation of $\rho$:

* $-1 \leq \rho \leq 1$.
* If $0 < \rho \leq 1$ then there is positive association between $X$ and $Y$.
The larger the value of $\rho$ the stronger the positive association.
* If $-1 \leq \rho < 0$ then there is negative association between $X$ and $Y$.
The smaller the value of $\rho$ the stronger the negative association.
* If $|\rho|=1$ there is a perfect linear relationship between $X$ and $Y$, that is
$Y=\alpha\,X+\beta$.  If $\rho=1$ then $\alpha>0$.  If $\rho=-1$ then $\alpha<0$.
* Values of $\rho$ close to $0$ do **not** indicate no association, just a lack of 
**linear** association.  If $\rho$ is close to zero it could be that $X$ and $Y$ are strongly non-linearly associated.
* If $\rho=0$ we say that $X$ and $Y$ are **uncorrelated**.

### Estimation

Suppose that we have a random sample $(X_1,Y_1), ..., (X_n,Y_n)$ from the joint distribution of $X$ and $Y$.  The **sample correlation coefficient**
$$
R = \hat{\rho} = \frac{\displaystyle\sum_{i=1}^n (X_i-\bar{X})(Y_i-\bar{Y})}
{\sqrt{\displaystyle\sum_{i=1}^n (X_i-\bar{X})^2 \displaystyle\sum_{i=1}^n(Y_i-\bar{Y})^2}} 
= \frac{C_{XY}}{\sqrt{C_{XX}\,C_{YY}}}, 
$$
is an estimator of the correlation coefficient $\rho$.

For the data in Figure \@ref(fig:exchangetrans) we get $r=0.82$.  This confirms the quite strong positive linear association apparent in the plot.  Why do these data give a value of $r$ which is positive? We consider contributions of individual data points to the numerator
\[  C_{xy}=\displaystyle\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y}) \]
of $r$.  Points in quadrant  

* B will have $x_i-\bar{x}>0$ and $y_i-\bar{y}>0$. Therefore $(x_i-\bar{x})(y_i-\bar{y})>0$.  
* C will have $x_i-\bar{x}<0$ and $y_i-\bar{y}<0$. Therefore $(x_i-\bar{x})(y_i-\bar{y})>0$.  
* A will have $x_i-\bar{x}<0$ and $y_i-\bar{y}>0$. Therefore $(x_i-\bar{x})(y_i-\bar{y})<0$.  
* D will have $x_i-\bar{x}>0$ and $y_i-\bar{y}<0$. Therefore $(x_i-\bar{x})(y_i-\bar{y})<0$.  

Therefore, since $\sqrt{C_{xx}\,C_{yy}}>0$, and $C_{xy}$ is a sum of contributions from all pairs $(x_i,y_i)$:

* If most of the observed points lie in quadrants B and C then $r$ is positive.
* If most of the observed points lie in quadrants A and D then $r$ is negative.
* If the numbers of points in each quadrant are approximately equal then $r$ is close to 0.

### Links between regression and correlation

Regression and correlation answer different questions.  However, there are links between them.  If we regress $Y$ on $x$ then

1. the sample coefficient of determination $R^2$ ($R^2$ is just a name: this is not literally the square of a quantity $R$) is equal to $r^2$, the square of the sample correlation coefficient,
2. the estimate of the regression slope $\beta$ is related to the sample correlation coefficient via
\[ \hat{\beta} = r \,\sqrt{\frac{C_{yy}}{C_{xx}}}. \]

These relationships make general sense. 

* If $R^2$ is large then the linear regression of $Y$ on $x$ explains a lot of variability in the $Y$ data and so the correlation between $X$ and $Y$ must be strong.
* The value of $\hat{\beta}$ and the value of $r$ have the same sign.

## Use and misuse of correlation

We must be careful to use correlation only when it is appropriate.  When it is used we must be careful to interpret its value carefully.  In this section we give at some examples to show why we must use correlation with care.  

One golden rule is that we should **always plots the data**.

### Do not use correlation for regression sampling schemes

We should **not** to use correlation to summarise association between $Y$ and $X$ when regression sampling has been used, that is, where the values of $X$ are **chosen** and then the values of $Y$ are observed.   We use simulation to show us why this is.  Figure \@ref(fig:xdesigncorr0) shows a random sample of size 100 from a joint distribution of $X$ and $Y$ for which $\rho=0.5$.  The sample correlation coefficient is 0.51. 

```{r echo=FALSE, xdesigncorr0, fig.show='hold', fig.cap='A random sample of size 100 from a distribution with $\\rho=0.5$.', out.width='50%'}
include_cropped_graphics("images/design_corr0.png")
```

In Figure \@ref(fig:xdesigncorr) regression samples of size 10 have been taken from the **same** joint distribution, but

* on the left the values of $x$ are **chosen** to be $-2$ and $+2$.  We get $r=0.84$.
* on the right the values of $x$ are **chosen** to be $-0.1$ and $+0.1$. We get $r=-0.46$.

```{r echo=FALSE, xdesigncorr, fig.show='hold', fig.cap='Left: $x$-values chosen to be only -2 or 2.  Right: $x$-values chosen to be only -0.1 or 0.1.', out.width = '100%'}
include_cropped_graphics("images/design_corr.png")
```

The values chosen for $x$ have a huge effect on $r$. By choosing the values of $x$ to be spread far apart we can make $r$ as close to $+1$ as we wish. If the values of $x$ are chosen to be close together, $r$ will be quite variable, but close to zero on average.

**Summary**

If regression sampling is used the value of $r$ depends on the values of $x$ chosen. 

### Examples of correlations of different strengths {#correxamples}

Figure \@ref(fig:corr8) gives random samples from distributions with correlations $\rho=0$ (2 of them), $\rho=\pm 0.3, \rho=\pm 0.7$ and $\rho=\pm 1$.  The data in the bottom plots in Figure \@ref(fig:corr8) are both sampled from distributions with $\rho=0$.  However, the associations between the variables are very different.  On the left there is no association.  On the right there is quite strong non-linear association.  

```{r echo=FALSE, corr8, fig.show='hold', fig.cap='Samples from distributions with correlations $\rho=0$ (2 of them), $\rho=\\pm 0.3, \rho=\\pm 0.7$ and $\rho=\\pm 1$.', out.width = '80%'}
include_cropped_graphics("images/corr8c.png")
include_cropped_graphics("images/corr8d.png")
```

### Beware missing data codes

Most real datasets have **missing observations**.  For example, people may decide not to answer some of the questions on a questionnaire.  It is quite common to use a missing value code (such as -9) to identify a missing value.  We need to be careful not to confuse a numerical missing value code with actual data.

Consider the following situation, which is based on a real example.Someone is given data on two variables $X$ and $Y$.  They calculate the sample correlation coefficient $r$ and find that it is 0.95.  They go back to the person who gave them the data and tell them that$X$ and $Y$ are strongly positively associated.  The person is surprised: they expected the variables to be negatively associated.  So they produce a plot of the data, shown on the left side of Figure \@ref(fig:corrmissing).

```{r echo=FALSE, corrmissing, fig.show='hold', fig.cap='Example data. Left: including the \'datum\' (-9,9). Right: with the missing value removed.', out.width = '50%'}
include_cropped_graphics("images/corr_missing1.png")
include_cropped_graphics("images/corr_missing2.png")
```

It is then clear that the large positive value of $r=0.95$ is due the missing value (-9,-9) being included in the plot.  Once this point is removed the sample correlation is negative, $r=-0.75$, as expected.  A plot of the data with the missing value removed in shown on the right side of Figure \@ref(fig:corrmissing).

### More guessing sample correlations

Figure \@ref(fig:guesscorr) gives some scatter plots with their sample correlation coefficients.  

```{r echo=FALSE, guesscorr, fig.show='hold', fig.cap='Some scatter plots and their sample correlation coefficient.', out.width = '80%'}
include_cropped_graphics("images/guess_corr.png")
```

Based on these plots, you will probably be surprised by the fact that for the data plotted in Figure \@ref(fig:guesscorrnew2) the value of the sample correlation coefficient is 0.991.  Can you guess what has caused this surprising value?

```{r echo=FALSE, guesscorrnew2, fig.show='hold', fig.cap='Can you guess the value of the sample correlation coefficient?', out.width = '50%'}
include_cropped_graphics("images/guess_corr_new2.png")
```

### Summary

Correlation can be used when

* the data are a random sample from the joint distribution of $(X,Y)$,
* the association between $X$ and $Y$ is approximately linear.

Correlation should **not** be used, that is, it can be misleading, if

* the values of either of the variables are controlled by an experimenter,
* $X$ and $Y$ are associated non-linearly.

### Anscombe's datasets

It is very important to plot data before fitting a linear regression model or estimating a correlation coefficient. In 1973 F.J. Anscombe (@Anscombe1973) created 4 datasets to illustrate the need for this.  His data are given in Table \@ref(fig:anscombetable).  You discussed these data, and some other examples, in Tutorial 1.

```{r echo=FALSE, anscombetable, fig.show='hold', fig.cap='Anscombe\'s datasets.', out.width = '60%'}
include_cropped_graphics("images/anscombetable.png")
```

The datasets have many things in common.  For each of the 4 datasets:

* the sample size is 11,
* $\bar{x}=9.00$, $s_x=3.32$, $\bar{y}=7.50$ and $s_y=2.03$,
* the least squares linear regression line is $y=3+0.5\,x$, with $RSS$=13.75,
* the sample correlation coefficient $r=0.816$.
* (By the way, the values of Spearman's rank correlation coefficient $r_S$ are
0.818, 0.691, 0.991 and 0.500 respectively.)

Based on these sample statistics we might be tempted to conclude that the association between $Y$ and $X$ is similar for each of the 4 datasets.  However, when we look at scatter plots of the data (Figure \@ref(fig:anscombeplot)) it is clear that this is not the case. This demonstrates that summary statistics alone do not describe adequately a probability distribution.

```{r echo=FALSE, anscombeplot, fig.show='hold', fig.cap='Scatter plots of Anscombe\'s datasets. Top left: approximately linear association.  Top right: perfect non-linear association. Bottom left: Perfect linear association apart from 1 outlier.  Bottom right: the increase of 0.5 units in $y$ for each unit increase in $x$ suggested by the regression equation is due to a single observation.', out.width = '80%'}
include_cropped_graphics("images/anscombe.png")
```

### We must interpret correlation with care.

Suppose that we find that two variables $X$ and $Y$ have a sample correlation which is not close to 0.
There are several ways that an apparent linear association can arise.

1. Changes in $X$ cause changes in $Y$.
2. Changes in $Y$ cause changes in $X$.
3. Changes in a third variable $Z$ causes changes in both $X$ and $Y$.
4. The observed association is just a coincidence.

Points 3. and 4. show that correlation does **not** imply causation. 

We finish with an example which may illustrate point 3.  We have already seen the time series plots in Figure \@ref(fig:fluandFTSE). 

```{r echo=FALSE, fluandFTSE, fig.show='hold', fig.cap='Weekly FTSE 100 closing prices against numbers of people seeing their doctor about flu over 4-week periods.', out.width = '80%'}
include_cropped_graphics("images/ftse_weekly_tufte.png")
include_cropped_graphics("images/flu_tufte.png")
```

For the period of time over which these series overlap, we extract from the FTSE 100 data the weekly closing price that corresponds to each of the 4-weekly flu values.  In Figure \@ref(fig:fluvsFTSE) we plot (on a log-log scale) these FTSE values against the flu values.

```{r echo=FALSE, fluvsFTSE, fig.show='hold', fig.cap='Weekly FTSE 100 closing prices against numbers of people seeing their doctor about flu over 4-week periods.', out.width = '60%'}
include_cropped_graphics("images/flu_FTSE.png")
```

We see that there is weak negative association ($r=-0.32$) between the FTSE 100 and flu values. Does this suggest that flu causes the FTSE 100 share index to drop, or vice versa? This is not a ridiculous suggestion, but the time series plots suggest that the negative association is due to the fact that the FTSE 100 index has **increased** over time, whereas the number of people seeing their doctor about flu has **decreased** over time. There is a third variable, time, which causes changes in both the FTSE 100 and the incidence of flu.