```{r, figsetup6, include=FALSE}
knitr::opts_chunk$set(
  fig.align = 'center',
  out.width = '75%'
)
```

# Statistical Inference {#inference}

Statistics is the science of collecting, analysing and interpreting data.
Statistical inference makes use of information from a sample to draw conclusions 
(inferences) about the population from which the sample was taken.

## The story so far

In Chapter \@ref(descriptive) we considered ways to describe and summarise **sample data**.  In Chapter \@ref(probability) we introduced the concept of the **probability** of an event and in Chapter \@ref(rvs) we defined a **random variable** to be a mapping of each value in the sample space to a real number.   In Chapter \@ref(simple) we considered some examples of some simple **probability distributions** for random variables that may describe the behaviour of a random quantity under certain special situations.  When we use a probability distribution in this way we may refer to it as a **probability model**.

## Sample and populations

Suppose that we are interested in the distribution of some aspect of a population, for example, the outcomes of successive tosses of a coin. In many cases it is not possible to collect information on the entire population. Therefore, a subset of the population, a **sample**, is selected. The aim is to generalise from the particular sample collected to the population from which it came.  The sample should be representative of the population. This is often achieved most straightforwardly by random sampling, where each member of the population has an equal chance of being chosen and different selections from the population are independent.

## Probability models

Often we makes inferences about a population using probability models. We view the data $X_1, X_2,...,X_n$ as random variables sampled randomly from a probability distribution.  The probability distribution often involves unknown constants, or **parameters**.  We use the data to estimate the values of these parameters. We should also quantify how uncertain we are about the values of the parameters. Generally speaking, the more data we have the more certain we can be about the approximate value of the parameters.  This process of is called **statistical inference**, because we are trying to infer the unknown population distribution and its parameters based on sample statistics.

Consider the coin-tossing example near the start of Chapter \@ref(probability).  The population of interest is the infinite set of outcomes which would be produced if Kerrich were able to toss the coin forever.  The sample is results of the 10,000 tosses which Kerrich actually carried out.  The population parameter of interest is the proportion $p$ of tosses on which a head is obtained.  If we assume that successive coin tosses are independent and that the probability $p$ of a head is the same on each toss, then the distribution of the total number $X$ of heads is binomial($10000, p$).  This binomial distribution is our probability model and $p$ is its unknown parameter.  Possible questions of interest are:
What is our 'best' estimate of $p$?  Can we provide an interval to quantify our uncertainty about $p$?  Is it plausible that $p=1/2$?

### Summary {-}

*  We are interested in the distribution of a random variable $X$.
*  Data: a random sample $X_1=x_1, \ldots, X_n=x_n$.
*  We assume that $X$ has a probability distribution with parameter $\theta$.  We might write $X \sim p(x; \theta)$.
*  We use data to make inferences about $\theta$ and therefore about the
distribution of $X$.

This is summarised in Figure \@ref(inference).

```{r echo=FALSE, inference, fig.show='hold', fig.cap='A schematic to describe the idea of statistical inference.', out.width = '80%'}
knitr::include_graphics("images/inference.png")
```

## Fitting models

## Uncertainty in estimation

## What makes an estimator good? {#good}

## Assessing goodness-of-fit