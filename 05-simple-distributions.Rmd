---
output:
  pdf_document: default
  html_document: default
---
```{r setup5, echo = FALSE, warning = FALSE, message = FALSE}

library(knitr)
library(dplyr)
library(huxtable)
options(
        huxtable.knit_print_df       = FALSE, 
        huxtable.add_colnames        = TRUE,  # needed when run by testthat
        huxtable.latex_use_fontspec  = TRUE,
        huxtable.bookdown            = TRUE
      )

#is_latex <- guess_knitr_output_format() == "latex"
## is_latex <- TRUE
#knitr::knit_hooks$set(
#  barrier = function(before, options, envir) {
#    if (! before && is_latex) knitr::asis_output("\\FloatBarrier")
#  }
#)
```

```{r, figsetup5, include=FALSE}
knitr::opts_chunk$set(
  fig.align = 'center',
  out.width = '75%'
)
```

```{r, echo = FALSE}
include_cropped_graphics <- function(x) {
  knitr::include_graphics(knitr::plot_crop(x))
#  knitr::include_graphics(x)
}
```

# Simple distributions {#simple}

We use a simple dataset to introduce some commonly-used simple distributions. We will study the **discrete** distributions: Bernoulli, binomial, geometric and Poisson. We will also study the **continuous** distributions: uniform, exponential, normal.

## Australian births data

Steele, S. (December 21, 1997), Babies by the Dozen for Christmas: 24-Hour Baby Boom, The Sunday Mail (Brisbane), page 7

According to this article, a record 44 babies (18 girls and 26 boys) were born in one 24-hour period at the Mater Mothers' Hospital, Brisbane, Australia, on 18th December 1997. The article listed the time of birth, the sex, and the weight in grams for each of the 44 babies.  These data are given in Table \@ref(tab:abirths).

```{r,echo = FALSE}
adat <- stat0002::aussie_births
amat <- matrix(NA, nrow = 12, ncol = 11)
for (i in 1:4) {
  amat[3 * (i - 1)  + 1, ] <- as.character(adat[1:11 + 11 * (i - 1), "time"])
  amat[3 * (i - 1)  + 2, ] <- as.character(adat[1:11 + 11 * (i - 1), "sex"])
  amat[3 * (i - 1)  + 3, ] <- as.character(adat[1:11 + 11 * (i - 1), "weight"])
}
rnames <- c("time", "sex", "weight")
amat <- cbind(rnames, amat)
```

```{r, echo=FALSE}
a_hux <- amat %>% 
  as_hux() %>%
  set_label("tab:abirths") %>%
  set_top_border(everywhere, everywhere, brdr(0.1, "solid", "black")) %>%
  set_bottom_border(everywhere, everywhere, brdr(0.1, "solid", "black")) %>%
  set_top_border(c(1, 4, 7, 10), everywhere, brdr(3, "double", "black")) %>%
  set_left_border(everywhere, -1, brdr(0.1, "solid", "black")) %>%
  set_right_border(everywhere, -12, brdr(0.1, "solid", "black")) %>%
  set_background_color(everywhere, everywhere, "white") %>%
  set_width(0.9) %>% 
  set_align(everywhere, everywhere, "center") %>% 
  set_number_format(everywhere, everywhere, 0) %>%
  set_bottom_padding(0) %>%
  set_top_padding(0) %>%
  set_caption("Australian births data.  Times are minutes since midnight.  Weights are in grams.")
a_hux    
```

## The Bernoulli distribution

Consider the first birth.  The outcome is either a boy $B$ or a girl $G$.  We define a random variable 

\begin{equation}
X_1 =
\begin{cases} 
0 & \text{ if the first baby is a girl} \\
1 & \text{ if the first baby is a boy}.
\end{cases}
\end{equation}

$X_1$ is the outcome of a **Bernoulli trial**, that is, an experiment which has exactly two possible outcomes.  Here, we have mapped the birth of a girl to the value 0 and the birth of a boy to the value 1.  This is arbitrary: we could have done this the other way round.

Let 
\begin{eqnarray*}
P(X_1=0)&=&1-p \\
P(X_1=1)&=&p. 
\end{eqnarray*}
Therefore,

\begin{equation}
X_1 =
\begin{cases} 
0 & \text{ with probability (w.p.)} \,\,1-p \\
1 & \text{ with probability (w.p.)} \,\,p.
\end{cases}
\end{equation}

$X_1$ has a Bernoulli distribution with parameter $p$, $(0 \leq p \leq 1)$, or, for short,
\[ X_1 \sim
\,\mbox{Bernoulli}(p). 
\]
Here, "$\sim$" means "is distributed as".

Similarly, we define the random variables
\begin{eqnarray*}
X_i &=& \left\{ \begin{array}{ll}0&\mbox{  if the $i$th baby is a girl}; \\
                                 1&\mbox{  if the $i$th baby is a boy,} \quad\mbox{for}\,\,\,i=1,\ldots,44.\end{array}\right.
\end{eqnarray*}
If we assume that the probability of a boy is the same for each birth then
\[ X_i \sim \mbox{Bernoulli}(p), \qquad \mbox{for}\,\,\,i=1,\ldots,44. \]

In this example the sample values $x_1, \ldots, x_{44}$ of $X_1,\ldots,X_{44}$ are
\[ x_1=0, \quad x_2=0, \quad x_3=1, \quad \ldots \quad x_{44}=0. \]

### Summary of the Bernoulli($p$) distribution

*  **Situation**: an experiment with exactly 2 possible outcomes (a Bernoulli trial), mapped to the values 0 and 1.
*  If $X$ is a discrete random variable with p.m.f.

\[ P(X=x) \,=\, p^x\,(1-p)^{1-x}, \qquad x=0,1, \]
for some $p$, $0 \leq p \leq 1$, then $X \sim \mbox{Bernoulli}(p)$.

*  Parameter: $p \in [0,1]$.
*  $\mbox{E}(X)=p$, $\mbox{var}(X)=p\,(1-p)$. (**Exercise**)

## The binomial distribution {#binomial}

We have seen that the 44 births are 44 Bernoulli trials.  We define a random variable $Y$ equal to the total number of boys in the 44 births, that is,
\[ Y = X_1 + X_2 + \cdots + X_{44} = \sum_{i=1}^{44} X_i. \]
We assume that the outcome of each birth is indepdendent of the outcomes of all the other births, that is, the random variables $X_1, \ldots, X_{44}$ are mutually independent. Therefore, we have assumed that $X_1, \ldots, X_{44}$ are independent and identically distributed, or i.i.d. for short.  `Identically distributed' means that $X_1, \ldots, X_{44}$ have exactly the same distribution, including the values of any unknown parameters.  We may write

\[ X_i \,\stackrel{{\rm i.i.d.}}{\sim}\, \mbox{Bernoulli}(p), \qquad i=1, ... , 44. \]
or
\[ X_i \,\stackrel{{\rm indep}}{\sim}\, \mbox{Bernoulli}(p), \qquad i=1, ... , 44. \]

$X_1, ... , X_{44}$ are a **random sample** from a Bernoulli($p$) distribution.

What is $P(Y=0)$?  

For $Y=0$ to occur we need all of the 44 births to be girls, that is,
\[ X_1=X_2=\cdots=X_{44}=0.\]  

We have assumed that the events $X_1=0, X_2=0, \ldots, X_{44}=0$ are independent of each other.  Using the multiplication rule for independent events gives
\begin{eqnarray*}
P(Y=0)&=&P(X_1=X_2=\cdots=X_{44}=0), \\
      &=&P(X_1=0) \times P(X_2=0) \times \cdots \times P(X_{44}=0), \\
      &=&(1-p) \times (1-p) \times \cdots \times (1-p), \\
      &=&(1-p)^{44}.
\end{eqnarray*}

What is $P(Y=1)$?  

For $Y=1$ to occur we need 43 girls and 1 boy to be born.  One way for this to happen is
\[ X_1=1, X_2=0, X_3=0, \ldots, X_{44}=0. \]
Under the assumption of independence, this combination of events has probability $p\,(1-p)^{43}$.
In fact, there are 44 different ways to get $Y=1$: the 1 boy could be born 1st, 2nd, 3rd, \ldots, 44th.
Therefore,
\[ P(Y=1) = 44\,p\,(1-p)^{43}. \]
We can continue this argument to find that
\[ 
P(Y=y)={44 \choose y} 
p^y\,(1-p)^{44-y}, \qquad y=0,1,\ldots,44. 
\]
The combinatorial term ${44 \choose y}={}^{44}C_y=\frac{44!}{(44-y)!y!}$ 
gives the number of ways in which $y$ boys births can be positioned amongst the 44 births.

$Y$ has a binomial distribution with parameters $44$ and $p$, $(0 \leq p \leq 1)$, or, for short,
\[ Y \sim \,\mbox{binomial}(44,p). \]
In this example the observed value of $Y$ is $y=26$.

As is usually the case, in this example we know the value of the first parameter, the number of trials, but we do not know the value of the second parameter, the probability $p$ that a trial results in a 1. Suppose that the 44 babies born on 18th December 2007 at this hospital in Brisbane are representative of babies from some wider population, perhaps the population of babies born (or to be born) in a December in Australia in the 21st century. We want to use these data to estimate the probability $p$ that a baby chosen randomly from this population is a boy, or equivalently, the proportion $p$ of babies in this population who are boys.

**Exercise**. We use $\hat{p} = 26/44$ as an estimate of $p$.  Why is this a sensible estimate?  

Putting a "hat" $\hat{}$ on a quantity indicates that the quantity is being estimated using data.  Here, $\hat{p}$ means "an estimate of $p$".

Figure \@ref(fig:babybinompmfs) shows the p.m.f.s of 4 binomial(44,$p$) distributions, for different values of $p$. One way to think about choosing an estimate of $p$ is to see for which value of $p$ the observed data are most likely.  In this case we have only one sample value, $y=26$.  Looking at Figure \@ref(fig:babybinompmfs) we can see that $y=26$ is very unlikely for $p=0.1$ and $p=0.9$, more likely for $p=0.5$ and even more likely for $p=\hat{p}$.  In addition to making sense because it is the observed proportion of boy babies, $\hat{p}$ has the property that the observed data are more likely for $p=\hat{p}$ than for any other value of $p$.

```{r echo=FALSE, babybinompmfs, fig.show='hold', fig.cap='Binomial($44,p$) p.m.f.s: $p=0.1$ (top); $p=1/2$ (second from top); $p=0.591$ (second from bottom); $p=0.9$ (bottom).  The sample value $y=26$ has been shaded in black.'}
include_cropped_graphics("images/binom_pmf1.png")
include_cropped_graphics("images/binom_pmf2.png")
include_cropped_graphics("images/binom_pmf3.png")
include_cropped_graphics("images/binom_pmf4.png")
```

### A brief look at statistical inference about $p$ {}

**This section of the notes is not part of STAT0002.  It introduces ideas surrounding hypothesis testing and confidence intervals.  If you take STAT0003 then you will cover these ideas in more detail.  This section is included because you may find it interesting to think about these ideas now.**

We have observed more boys than girls.  If $p=1/2$ we would expect roughly equal numbers of boys and girls. Even if it is true that $p=1/2$, we would only occasionally get equal numbers of boys and girls born on each day. We might like to quantify how unlikely is the event of a 26:18 split in 44 independent Bernoulli(1/2) trials. If this is very unlikely we might think that perhaps $p \neq 1/2$ after all. This is an example of **statistical inference**, that is, making an inference about the true value of $p$.  We will study statistical inference in Chapter \@ref(inference). 

In this example we might want to infer whether or not $p=0.5$.  We wish to judge whether the estimate $\hat{p}$ is far enough from 0.5 for us to conclude that $p \neq 0.5$.  We need to take into account how reliable (a term that we will use in Chapter \@ref(inference) is how **precise**) the estimation of $p$ is.  For a given true value of $p$ the larger the sample size $n$ (here $n = 44$) the greater the precision. If you take STAT0003 then you will study this in some detail.  Here we introduce two main ways to look at this problem.  They will seem rather similar, but there is a subtle difference.

#### 1. If $p=0.5$ then how surprising is $\hat{p}=0.591$? {-}

Suppose that the **null hypothesis** $H_0: p=0.5$ is true.  How unlikely it is that a sample of size $44$ produces an estimate of $p$ as far, or further from, 0.5 than $\hat{p}=0.591$?

You could think of this as 'standing' on the null hypothesis $H_0$ and looking to see how far away are the data.

If $p=0.5$, $P(Y \geq 26\mbox{ or }Y \leq 18) = 0.15+0.15=0.30$.  Figure \@ref(fig:binominf) illustrates the calculation of this probability. This (the value 0.30 here) is called a **$p$-value**. A $p$-value is a measure of our surprise at seeing data if the null hypothesis is true.  The smaller the $p$-value the greater our surprise.  We could decide to reject the null hypothesis $H_0$ that $p=0.5$ if the $p$-value is sufficiently small.  Otherwise, we do not reject $H_0$. 

```{r echo=FALSE, binominf, fig.show='hold', fig.cap='A binomial($44,1/2$) p.m.f. with the probabilities satisfting $Y$ less than or equal to 18 or greater than or equal to 26 shaded in black.'}
include_cropped_graphics("images/binom_inference_1b.png")
```

A traditional cutoff for "sufficiently small" is 0.05, but it is not possible to argue that this is generally better than other choices of cutoff.  Based on this cutoff we would not reject $H_0$.

#### 2. Is $p=0.5$ plausible based on inferences made about $p$ using the data? {-}

We estimate $p$ and then see how close the estimate is to 0.5.  In this example $\hat{p}=26/44=0.591$. To quantify whether this is significantly different from 0.5 we can calculate an interval estimate of $p$, called a **confidence interval**, and see whether or not this interval contains 0.5.  If it does not contain 0.5 then we could decide to reject the null hypothesis $H_0$ that $p=0.5$. Otherwise, we do not reject $H_0$.

You could think of this as 'standing' on the data and looking to see how far away is the null hypothesis $H_0.$

A confidence interval is a realisation (the observed value) of a random interval that has a certain probability of covering the true value of $p$.  The interval is random because before we collect the data we do not know what the interval will be. For example, A 95\% confidence interval for $p$ has a probability of 0.95 of covering the true value of $p$. 

An approximate 95\% confidence interval for $p$ based on these data is $(0.45,0.74)$ which does contain 0.5.  Therefore, we do not reject $H_0$.

In modern Statistics method 2. (**interval estimation**) is often preferred to 1. (**hypothesis testing**) because it gives an interval estimate for $p$ rather than just a decision of whether or not the data suggest that $p=0.5$.

### Summary of the binomial($n,p$) distribution

*  **Situation**: the number of 1s in $n$ independent Bernoulli trials, 
each trial having the same probability $p$ of obtaining a 1.
*  If $Y$ is a discrete random variable with p.m.f.
\[ P(Y=y) \,=\, {n \choose y} p^y\,(1-p)^{n-y}, \qquad y=0,1,\ldots,n, \]
for some $p$, $0 \leq p \leq 1$, then $Y \sim \mbox{binomial}(n,p)$.
*  Parameters: $n$ (usually known) and $p \in [0,1]$.
*  $\mbox{E}(Y)=n\,p$, $\mbox{var}(Y)=n\,p\,(1-p)$. (**Exercise**. Note that $Y=X_1 + \cdots + X_n$, where $X_i, i=1, \ldots, n$ are independent Bernoulli($p$) random variables.)

A binomial(1,$p$) distribution is the same as a Bernoulli($p$) distribution.

## The geometric distribution

The ordering of the arrivals of the boy and girl babies is 
\[ G G B B B G G B B B B B G G B G G B B B B G G G G B B B G B G B B B B B G B B B B G G G \]

Suppose that are interested in the arrival of the 1st baby boy.  We define the random variable $W_1$ to be the number of births up to and including the birth of the 1st boy. In the current example, the 1st boy is born on the 3rd birth, so the sample value of $W_1$ is $w_1=3$.

Now we define $W_2$ to be the number of births, after the birth of the 1st boy, up to and including the birth of the 2nd boy.  In this example the sample value of $W_2$ is $w_2=1$.

Similarly, we define $W_i$ to be the number of births, after the birth of the $(i-1)$th boy, up to and including the birth of the $i$th boy.  

This leads to values $w_1, w_2, \ldots, w_{26}$:
\[ 3, 1, 1, 3, 1, 1, 1, 1, 3, 3, 1, 1, 1, 5, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1 \]
The last 3 observations ($GGG$) do not contribute here because we do not know when the next boy arrived.  The frequencies of these values appear in column 2 of Table \@ref(tab:geom).  In column 4 we have divided these **observed frequencies** by 26 to obtain the **relative frequencies**.  We explain the contents of columns 3 and 5 of this table later.

```{r, echo = FALSE}
sex <- stat0002::aussie_births$sex
# Find the numbers of births that we wait for each boy to arrive
temp <- diff(c(0, which(sex == "boy")))
# Add dummy values to make sure that zero frequencies appear
temp <- c(1:(max(temp) + 1), temp)
# Table of frequencies
geom_freq <- table(temp) - 1             
names(geom_freq)[6] <- "6+"

obs_freq <- as.numeric(geom_freq)
rel_freq <- obs_freq / sum(obs_freq)

# Estimated probabilities 
# R's dgeom function is based on the number of failures before the 
# first success, not the number of trials.  Therefore, we subtract 1 below.
phat <- mean(sex == "boy")
est_prob <- dgeom((1:5) - 1, prob = phat)
# Add the estimates probability of 6 or more births
est_prob <- c(est_prob, 1 - sum(est_prob))

# Estimated expected frequencies
exp_freq <- 26 * est_prob

geom_df <- round(cbind(1:6, obs_freq, exp_freq, rel_freq, est_prob), 3)
```

```{r, echo=FALSE}
geom_hux <- geom_df %>% 
  as_hux() %>%
  insert_row("number of births", "observed", "estimated expected", "relative", "estimated", after = 0) %>%
  insert_row("until boy born", "frequency", "frequency", "frequency", "probability", after = 1) %>%
  set_contents(8, 1, "6+") %>%
  set_label("tab:geom") %>%
  set_right_padding(-c(1, 2), 3, 60) %>% 
  set_align(everywhere, everywhere, "center") %>% 
  set_align(-c(1,2), 3, ".") %>%
  set_top_border(3, everywhere, brdr(0.1, "solid", "black")) %>%
  set_top_border(-3, everywhere, brdr(0.1, "solid", "white")) %>%
  set_left_border(everywhere, -c(1, 6), brdr(0.1, "solid", "black")) %>%
  set_background_color(everywhere, everywhere, "white") %>%
  set_width(0.9) %>% 
  set_number_format(everywhere, 3:5, 2) %>%
  set_bottom_padding(0) %>%
  set_top_padding(0) %>%
  insert_row("total", "26", "26", "1", "1", after = 8) %>%        
  set_number_format(9, 3:5, 0) %>%
set_caption("Observed frequencies and relative frequencies and their fitted values under a geometric(0.591) distribution.")
geom_hux    
```

**Question**: Why have I written '$\geq$ 6' in the first column of Table \@ref(tab:geom) rather than '6'?

The observed frequencies are plotted in Figure \@ref(fig:geombar).

```{r echo=FALSE, geombar, fig.show='hold', fig.cap='Bar plot of numbers of births between successive baby boys (including the next baby boy).'}
include_cropped_graphics("images/baby_geom_bar.png")
```

Now we find the probability distribution of $W_1, W_2, \ldots$. First we consider $W_1$. Recall that the outcomes $X_1,\ldots,X_{44}$ of the 44 births are assumed to be independent Bernoulli trials.

What is $P(W_1=1)$?.  

For $W_1=1$ to occur we need the next birth to be a boy. Therefore, $P(W_1=1)=p$.

What is $P(W_1=2)$?.  

For $W_1=2$ to occur we need a girl followed by a boy.  Therefore, $P(W_1=2)=(1-p)\,p$.

We can continue this argument to find

\begin{equation}
P(W_1=w) = (1-p)^{w-1}\,p, \qquad w=1,2,\ldots.  
(\#eq:geom)
\end{equation}

$W_1$ has a geometric distribution with parameter $p$, $(0 < p \leq 1)$, or, for short,
\[ W_1 \sim \mbox{geometric}(p). \]
We exclude $p=0$ because in this case we would never get a 1 and the \@ref(eq:geom) would sum to zero not one.

Since $W_2$ relates to exactly the same situation as $W_1$, that is, the number of births until the next boy is born, $W_2$ has the same distribution as $W_1$. (We may write $W_1 \,\stackrel{{\rm d}}{=}\, W_2$ for "$W_1$ has the same distribution as $W_2$".) Similarly $W_3,W_4,\ldots$ also have the same distribution as $W_1$.

The outcomes of the births are assumed to be mutually independent.  
\[ W_i \,\stackrel{{\rm i.i.d.}}{\sim}\, \mbox{geometric}(p), \qquad i=1,\ldots,26. \]
$W_1, \ldots, W_{26}$ are a **random sample** from a geometric($p$) distribution.

We do not know the value of $p$ but we have an estimate $\hat{p}=26/44$ of $p$.  We substitute this estimate of $p$ into equation \@ref(eq:geom) to calculate the **estimated probabilities** in column 5 of Table \@ref(tab:geom).
Multiplying the estimated probabilities by 26 gives the **expected** frequencies in column 3.  That is,

Estimated probabilities: $(1-\hat{p})^{w-1}\,\hat{p}, \quad w=1,2,\ldots.$

Estimated expected frequencies: $26 \times (1-\hat{p})^{w-1}\,\hat{p}, \quad w=1,2,\ldots.$

Figure \@ref(fig:geomfit) shows that the observed and estimated expected frequencies are in reasonably close agreement. We do not expect exact agreement.  Formal methods for assessing how closely observed and estimated expected frequencies agree are not part of STAT0002.  Figure \@ref(fig:geomfit) also shows the general shape of the geometric distribution.  It is positively skewed.

```{r echo=FALSE, geomfit, fig.show='hold', fig.cap='Bar plot of numbers of births between successive baby boys (including the next baby boy).'}
include_cropped_graphics("images/baby_geom_fit.png")
```

### Summary of the geometric($p$) distribution

* **Situation**: the number of trials up to and including the first value of 1 in a sequence of independent Bernoulli trials, each trial having the same probability $p$ of obtaining a 1.
*  If $W$ is a discrete random variable with p.m.f.
\[ P(W=w) \,=\, (1-p)^{w-1}\,p, \qquad w=1,2,\ldots \]
for some $p$, $0 < p \leq 1$, then $W \sim \mbox{geometric}(p)$.
*  Parameter: $p \in (0,1]$.
*  $\mbox{E}(W)=\displaystyle\frac1p$, $\mbox{var}(W)=\displaystyle\frac{1-p}{p^2}$. (**Exercise**)

An alternative formulation is where a random variable $V$ is defined as the number of trials performed **before** the next 1 occurs.  Therefore, the support of $V$ is $0, 1, 2, ...$ and $V = W - 1$. The `dgeom()` function in R relates to a random variable like $V$, not $W$. 

## The Poisson distribution

Now we look at the numbers $N_1, \ldots, N_{24}$ of births that occur in each hour of the 24 hours on the day December 18, 1997.  This produces a **count** for each hour.  We have split the 24-hour period into 24 time periods of the same length.  We could equally have chosen to split it into 12 time periods, each of length 2 hours. 

The frequencies of these counts appear in column 2 of Table \@ref(tab:pois).  In column 4 we have divided these **observed frequencies** by 24 to obtain the **relative frequencies**.  We explain the contents of columns 3 and 5 of this table later.

```{r, echo = FALSE}
time <- stat0002::aussie_births$time
# Hour of birth
temp <- floor(time /60) + 1
temp <- table(c(c(0:max(temp), temp))) - 1
pois_freq <- table(c(1:(max(temp) + 1), temp)) - 1
names(pois_freq)[6] <- "5+"
obs_freq <- as.numeric(pois_freq)
rel_freq <- obs_freq / sum(obs_freq)
# Estimated probabilities 
muhat <- 44/24
pois_fit <- dpois(0:4, muhat)
est_prob <- c(pois_fit, 1 - sum(pois_fit))
exp_freq <- est_prob * 24
pois_df <- round(cbind(0:5, obs_freq, exp_freq, rel_freq, est_prob), 3)
```

```{r, echo=FALSE}
pois_hux <- pois_df %>% 
  as_hux() %>%
  insert_row("number of births", "observed", "estimated expected", "observed", "estimated", after = 0) %>%
  insert_row("in 1 hour", "frequency", "frequency", "proportion", "probability", after = 1) %>%
  set_contents(8, 1, "5+") %>%
  set_label("tab:pois") %>%
  set_right_padding(-c(1, 2), 3, 60) %>% 
  set_align(everywhere, everywhere, "center") %>% 
  set_align(-c(1,2), 3, ".") %>%
  set_top_border(3, everywhere, brdr(0.1, "solid", "black")) %>%
  set_top_border(-3, everywhere, brdr(0.1, "solid", "white")) %>%
  set_left_border(everywhere, -c(1, 6), brdr(0.1, "solid", "black")) %>%
  set_background_color(everywhere, everywhere, "white") %>%
  set_width(0.9) %>% 
  set_number_format(everywhere, 3:5, 2) %>%
  set_bottom_padding(0) %>%
  set_top_padding(0) %>%
  insert_row("total", "24", "24", "1", "1", after = 8) %>%        
  set_number_format(9, 3:5, 0) %>%
set_caption("Observed frequencies and relative frequencies and their fitted values for a Poisson(1.83) distribution.")
pois_hux    
```

The observed frequencies are plotted in Figure \@ref(fig:poisbar).

```{r echo=FALSE, poisbar, fig.show='hold', fig.cap='Bar plot of numbers of births in each hour of the 24 hours of December 18, 1997.'}
include_cropped_graphics("images/baby_pois_bar.png")
```

We make assumptions about the way in which the births occur over time.  Suppose that births occur 

*  one at a time
*  independently of each other
*  uniformly, that is, at a constant rate $\lambda$ per hour
*  randomly.

This is an informal description of a **Poisson process** of rate $\lambda$.

It can be shown (STAT0003, STAT0007) that if births arrive in a Poisson process of rate $\lambda$ per hour then the number of births that occur in a time
interval of length $h$ hours has a particular kind of distribution called a Poisson distribution with mean $\mu=\lambda h$.  In our example, we have $h=1$, because we have counted the number of births that occur in time periods of length 1 hour.

Consider $N_1$, the number of births in the first hour.  Under the assumptions of the Poisson process, $N_1$ has a Poisson distribution with mean parameter $\lambda$, ($\lambda >0$), or, for short,
$$N_1 \sim \mbox{Poisson}(\lambda).$$  
The probability mass function (p.m.f.) of $N_1$ is given by
\begin{equation}
P(N_1=n) = \frac{\lambda^n\,e^{-\lambda}}{n!}, \qquad n=0,1,2,\ldots. (\#eq:pois)
\end{equation}

These arguments also apply to $N_2, N_3, ... , N_{24}$.  Therefore,
\[ N_i \,\stackrel{{\rm i.i.d.}}{\sim}\, \mbox{Poisson}(\lambda), \qquad i=1,\ldots,24, \]
that is, $N_1,\ldots,N_{24}$ are a random sample from a Poisson ($\lambda$) distribution.

The sample values $n_1, n_2, \ldots, n_{24}$ of $N_1, N_2, \ldots, N_{24}$ are
\[ n_1=1, \quad n_2=3, \quad \ldots, \quad n_{24}=2. \]

We do not know the value of $\lambda$, but we can estimate it using $\hat{\lambda} = 44 / 24 \approx 1.83$.  

**Exercise**. Why is this a sensible estimate of $\lambda$?

We substitute this estimate of $\lambda$ into equation \@ref(eq:pois) to calculate the estimated probabilities in column 5 of Table Table \@ref(tab:pois). Multiplying the estimated probabilities by 24 gives the estimated expected frequencies in column 3.  That is,

Estimated probabilities: $\displaystyle\frac{\hat{\lambda}^n\, e^{-\hat{\lambda}}}{n!}, \quad n=0,1,2,\ldots.$

Estimated expected frequencies: $24 \times \displaystyle\frac{\hat{\lambda}^n\,e^{-\hat{\lambda}}}{n!}, \quad n=0,1,2,\ldots.$

Figure \@ref(fig:poisfit) shows that the observed and estimated expected frequencies 
are in reasonably close agreement.

```{r echo=FALSE, poisfit, fig.show='hold', fig.cap='Plot of observed frequencies and the estimated expected frequencies under a Poisson(1.83) distribution.'}
include_cropped_graphics("images/baby_pois_fit.png")
```

Figure \@ref(fig:poispmfs) shows 3 different Poisson p.m.f.s.   As $\lambda$ increases the Poisson p.m.f. becomes increasingly symmetric.  The middle plot shows the p.m.f. of the Poisson distribution we have fitted to the hourly baby totals.  This plot has approximately the same shape as the frequency distribution of hourly baby totals in Figure \@ref(fig:poisbar).

```{r echo=FALSE, poispmfs, fig.show='hold', fig.cap='Poisson p.m.f.s: mean = 0.5 (top); mean = 1.83 (middle); mean = 5 (bottom).'}
include_cropped_graphics("images/poisson_pmf1.png")
include_cropped_graphics("images/poisson_pmf2.png")
include_cropped_graphics("images/poisson_pmf3.png")
```

### Summary of the Poisson($\mu$) distribution

*  Situation: the number events occuring during a time period of fixed duration.
*  If $N$ is a discrete random variable with p.m.f.
\[ P(N=n) = \frac{\mu^n\, e^{-\mu}}{n!}, \qquad n=0,1,2,\ldots, \]
for some $\mu$, then $N \sim \mbox{Poisson}(\mu)$.
*  Parameter: $\mu > 0$. 

[$\mu = 0$ is valid but  not interesting: $P(N = 0) = 1$.]

*  $\mbox{E}(N)=\mu$, $\mbox{var}(N)=\mu$.

## Summary of these discrete distributions

We have studied 4 common discrete distributions:

* **Bernoulli**. A single trial with a binary outcome (0/1 variable).
* **binomial**: The total number of 1s in a sequence of independent and Bernoulli trials, with a common probability of obtaining a 1.  
* **geometric**: The number of independent Bernoulli trials, with a common probability of obtaining a 1, until the first 1 is obtained.
* **Poisson**: used for counts of events that arrive in a Poisson process.

## The uniform distribution

## The exponential distribution

## The normal distribution {#normal}

## QQ plots