<!DOCTYPE html>
<html lang="en-gb" xml:lang="en-gb">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Random variables | STAT0002 Introduction to Probability and Statistics</title>
  <meta name="description" content="Produces STAT0002 notes in an accessible format" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Random variables | STAT0002 Introduction to Probability and Statistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Produces STAT0002 notes in an accessible format" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Random variables | STAT0002 Introduction to Probability and Statistics" />
  
  <meta name="twitter:description" content="Produces STAT0002 notes in an accessible format" />
  

<meta name="author" content="Dr Paul Northrop" />


<meta name="date" content="2022-09-07" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="more-probability.html"/>
<link rel="next" href="simple.html"/>
<script src="libs/header-attrs-2.14/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">STAT0002 2020-21</a></li>

<li class="divider"></li>
<li><a href="index.html#the-purpose-of-these-notes" id="toc-the-purpose-of-these-notes">The purpose of these notes<span></span></a></li>
<li><a href="introduction.html#introduction" id="toc-introduction"><span class="toc-section-number">1</span> Introduction<span></span></a>
<ul>
<li><a href="introduction.html#real" id="toc-real"><span class="toc-section-number">1.1</span> Real statistical investigations<span></span></a></li>
<li><a href="introduction.html#shuttle" id="toc-shuttle"><span class="toc-section-number">1.2</span> Challenger Space Shuttle Catastrophe<span></span></a>
<ul>
<li><a href="introduction.html#uncertainty" id="toc-uncertainty"><span class="toc-section-number">1.2.1</span> Uncertainty<span></span></a></li>
</ul></li>
<li><a href="introduction.html#a-very-brief-introduction-to-stochastic-simulation" id="toc-a-very-brief-introduction-to-stochastic-simulation"><span class="toc-section-number">1.3</span> A very brief introduction to stochastic simulation<span></span></a></li>
</ul></li>
<li><a href="descriptive.html#descriptive" id="toc-descriptive"><span class="toc-section-number">2</span> Descriptive Statistics<span></span></a>
<ul>
<li><a href="descriptive.html#types-of-data" id="toc-types-of-data"><span class="toc-section-number">2.1</span> Types of data<span></span></a>
<ul>
<li><a href="descriptive.html#qualitative-or-categorical-data" id="toc-qualitative-or-categorical-data"><span class="toc-section-number">2.1.1</span> Qualitative or categorical data<span></span></a></li>
<li><a href="descriptive.html#quantitative-or-numerical-data" id="toc-quantitative-or-numerical-data"><span class="toc-section-number">2.1.2</span> Quantitative or numerical data<span></span></a></li>
</ul></li>
<li><a href="descriptive.html#describing-distributions" id="toc-describing-distributions"><span class="toc-section-number">2.2</span> Describing distributions<span></span></a>
<ul>
<li><a href="descriptive.html#example-oxford-births-data" id="toc-example-oxford-births-data">Example: Oxford births data<span></span></a></li>
</ul></li>
<li><a href="descriptive.html#summary-statistics" id="toc-summary-statistics"><span class="toc-section-number">2.3</span> Summary Statistics<span></span></a>
<ul>
<li><a href="descriptive.html#fivenumber" id="toc-fivenumber"><span class="toc-section-number">2.3.1</span> Five number summary<span></span></a></li>
<li><a href="descriptive.html#meanstdev" id="toc-meanstdev"><span class="toc-section-number">2.3.2</span> Mean and standard deviation<span></span></a></li>
<li><a href="descriptive.html#mode" id="toc-mode"><span class="toc-section-number">2.3.3</span> Mode<span></span></a></li>
<li><a href="descriptive.html#symmetry" id="toc-symmetry"><span class="toc-section-number">2.3.4</span> Symmetry<span></span></a></li>
<li><a href="descriptive.html#corr1" id="toc-corr1"><span class="toc-section-number">2.3.5</span> Correlation<span></span></a></li>
</ul></li>
<li><a href="descriptive.html#tables" id="toc-tables"><span class="toc-section-number">2.4</span> Tables<span></span></a>
<ul>
<li><a href="descriptive.html#frequency-distribution" id="toc-frequency-distribution"><span class="toc-section-number">2.4.1</span> Frequency distribution<span></span></a></li>
</ul></li>
<li><a href="descriptive.html#graphs" id="toc-graphs"><span class="toc-section-number">2.5</span> Graphs (1 variable)<span></span></a>
<ul>
<li><a href="descriptive.html#histogram" id="toc-histogram"><span class="toc-section-number">2.5.1</span> Histograms<span></span></a></li>
<li><a href="descriptive.html#stem" id="toc-stem"><span class="toc-section-number">2.5.2</span> Stem-and-leaf plots<span></span></a></li>
<li><a href="descriptive.html#dotplots" id="toc-dotplots"><span class="toc-section-number">2.5.3</span> Dotplots<span></span></a></li>
<li><a href="descriptive.html#boxplots" id="toc-boxplots"><span class="toc-section-number">2.5.4</span> Boxplots<span></span></a></li>
<li><a href="descriptive.html#barplots" id="toc-barplots"><span class="toc-section-number">2.5.5</span> Barplots<span></span></a></li>
<li><a href="descriptive.html#times-series-plots" id="toc-times-series-plots"><span class="toc-section-number">2.5.6</span> Times series plots<span></span></a></li>
</ul></li>
<li><a href="descriptive.html#election" id="toc-election"><span class="toc-section-number">2.6</span> 2000 US Presidential Election<span></span></a></li>
<li><a href="descriptive.html#graphs2" id="toc-graphs2"><span class="toc-section-number">2.7</span> Graphs (2 variables)<span></span></a>
<ul>
<li><a href="descriptive.html#scatter-plots" id="toc-scatter-plots"><span class="toc-section-number">2.7.1</span> Scatter plots<span></span></a></li>
</ul></li>
<li><a href="descriptive.html#transformation" id="toc-transformation"><span class="toc-section-number">2.8</span> Transformation of data<span></span></a>
<ul>
<li><a href="descriptive.html#transsymmetry" id="toc-transsymmetry"><span class="toc-section-number">2.8.1</span> Transformation to approximate symmetry<span></span></a></li>
<li><a href="descriptive.html#straighten" id="toc-straighten"><span class="toc-section-number">2.8.2</span> Straightening scatter plots<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="probability.html#probability" id="toc-probability"><span class="toc-section-number">3</span> Probability<span></span></a>
<ul>
<li><a href="probability.html#sids" id="toc-sids"><span class="toc-section-number">3.1</span> Misleading statistical evidence in cot death trials<span></span></a></li>
<li><a href="probability.html#relative-frequency-definition-of-probability" id="toc-relative-frequency-definition-of-probability"><span class="toc-section-number">3.2</span> Relative frequency definition of probability<span></span></a></li>
<li><a href="probability.html#basic-properties-of-probability" id="toc-basic-properties-of-probability"><span class="toc-section-number">3.3</span> Basic properties of probability<span></span></a></li>
<li><a href="probability.html#conditional-probability" id="toc-conditional-probability"><span class="toc-section-number">3.4</span> Conditional probability<span></span></a></li>
<li><a href="probability.html#addition-rule-of-probability" id="toc-addition-rule-of-probability"><span class="toc-section-number">3.5</span> Addition rule of probability<span></span></a>
<ul>
<li><a href="probability.html#mutually-exclusive-events" id="toc-mutually-exclusive-events"><span class="toc-section-number">3.5.1</span> Mutually exclusive events<span></span></a></li>
</ul></li>
<li><a href="probability.html#multrule" id="toc-multrule"><span class="toc-section-number">3.6</span> Multiplication rule of probability<span></span></a></li>
<li><a href="probability.html#indepevents" id="toc-indepevents"><span class="toc-section-number">3.7</span> Independence of events<span></span></a>
<ul>
<li><a href="probability.html#bloodindep" id="toc-bloodindep"><span class="toc-section-number">3.7.1</span> An example of independence<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="more-probability.html#more-probability" id="toc-more-probability"><span class="toc-section-number">4</span> More Probability<span></span></a>
<ul>
<li><a href="more-probability.html#law-of-total-probability" id="toc-law-of-total-probability"><span class="toc-section-number">4.1</span> Law of total probability<span></span></a></li>
<li><a href="more-probability.html#bayes-theorem" id="toc-bayes-theorem"><span class="toc-section-number">4.2</span> Bayes’ theorem<span></span></a></li>
<li><a href="more-probability.html#dna-identification-evidence" id="toc-dna-identification-evidence"><span class="toc-section-number">4.3</span> DNA identification evidence<span></span></a></li>
</ul></li>
<li><a href="rvs.html#rvs" id="toc-rvs"><span class="toc-section-number">5</span> Random variables<span></span></a>
<ul>
<li><a href="rvs.html#discrete" id="toc-discrete"><span class="toc-section-number">5.1</span> Discrete random variables<span></span></a></li>
<li><a href="rvs.html#continuous" id="toc-continuous"><span class="toc-section-number">5.2</span> Continuous random variables<span></span></a></li>
<li><a href="rvs.html#expectation" id="toc-expectation"><span class="toc-section-number">5.3</span> Expectation<span></span></a>
<ul>
<li><a href="rvs.html#expectation-of-a-discrete-random-variable" id="toc-expectation-of-a-discrete-random-variable"><span class="toc-section-number">5.3.1</span> Expectation of a discrete random variable<span></span></a></li>
<li><a href="rvs.html#expectation-of-a-continuous-random-variable" id="toc-expectation-of-a-continuous-random-variable"><span class="toc-section-number">5.3.2</span> Expectation of a continuous random variable<span></span></a></li>
<li><a href="rvs.html#properties-of-mathrmex" id="toc-properties-of-mathrmex"><span class="toc-section-number">5.3.3</span> Properties of <span class="math inline">\(\mathrm{E}(X)\)</span><span></span></a></li>
<li><a href="rvs.html#EgX" id="toc-EgX"><span class="toc-section-number">5.3.4</span> The expectation of <span class="math inline">\(g(X)\)</span><span></span></a></li>
</ul></li>
<li><a href="rvs.html#variance" id="toc-variance"><span class="toc-section-number">5.4</span> Variance<span></span></a>
<ul>
<li><a href="rvs.html#variance-of-a-discrete-random-variable" id="toc-variance-of-a-discrete-random-variable"><span class="toc-section-number">5.4.1</span> Variance of a discrete random variable<span></span></a></li>
<li><a href="rvs.html#variance-of-a-continuous-random-variable" id="toc-variance-of-a-continuous-random-variable"><span class="toc-section-number">5.4.2</span> Variance of a continuous random variable<span></span></a></li>
<li><a href="rvs.html#variance-and-standard-deviation" id="toc-variance-and-standard-deviation"><span class="toc-section-number">5.4.3</span> Variance and standard deviation<span></span></a></li>
<li><a href="rvs.html#properties-of-mathrmvarx" id="toc-properties-of-mathrmvarx"><span class="toc-section-number">5.4.4</span> Properties of <span class="math inline">\(\mathrm{var}(X)\)</span><span></span></a></li>
</ul></li>
<li><a href="rvs.html#locations" id="toc-locations"><span class="toc-section-number">5.5</span> Other measures of location<span></span></a>
<ul>
<li><a href="rvs.html#the-median-of-a-random-variable" id="toc-the-median-of-a-random-variable"><span class="toc-section-number">5.5.1</span> The median of a random variable<span></span></a></li>
<li><a href="rvs.html#the-mode-of-a-random-variable" id="toc-the-mode-of-a-random-variable"><span class="toc-section-number">5.5.2</span> The mode of a random variable<span></span></a></li>
</ul></li>
<li><a href="rvs.html#quantiles" id="toc-quantiles"><span class="toc-section-number">5.6</span> Quantiles<span></span></a></li>
<li><a href="rvs.html#measures-of-shape" id="toc-measures-of-shape"><span class="toc-section-number">5.7</span> Measures of shape<span></span></a></li>
</ul></li>
<li><a href="simple.html#simple" id="toc-simple"><span class="toc-section-number">6</span> Simple distributions<span></span></a>
<ul>
<li><a href="simple.html#australian-births-data" id="toc-australian-births-data"><span class="toc-section-number">6.1</span> Australian births data<span></span></a></li>
<li><a href="simple.html#the-bernoulli-distribution" id="toc-the-bernoulli-distribution"><span class="toc-section-number">6.2</span> The Bernoulli distribution<span></span></a>
<ul>
<li><a href="simple.html#summary-of-the-bernoullip-distribution" id="toc-summary-of-the-bernoullip-distribution"><span class="toc-section-number">6.2.1</span> Summary of the Bernoulli(<span class="math inline">\(p\)</span>) distribution<span></span></a></li>
</ul></li>
<li><a href="simple.html#binomial" id="toc-binomial"><span class="toc-section-number">6.3</span> The binomial distribution<span></span></a>
<ul>
<li><a href="simple.html#binominf" id="toc-binominf"><span class="toc-section-number">6.3.1</span> A brief look at statistical inference about <span class="math inline">\(p\)</span><span></span></a></li>
<li><a href="simple.html#summary-of-the-binomialnp-distribution" id="toc-summary-of-the-binomialnp-distribution"><span class="toc-section-number">6.3.2</span> Summary of the binomial(<span class="math inline">\(n,p\)</span>) distribution<span></span></a></li>
</ul></li>
<li><a href="simple.html#the-geometric-distribution" id="toc-the-geometric-distribution"><span class="toc-section-number">6.4</span> The geometric distribution<span></span></a>
<ul>
<li><a href="simple.html#summary-of-the-geometricp-distribution" id="toc-summary-of-the-geometricp-distribution"><span class="toc-section-number">6.4.1</span> Summary of the geometric(<span class="math inline">\(p\)</span>) distribution<span></span></a></li>
</ul></li>
<li><a href="simple.html#Poisson" id="toc-Poisson"><span class="toc-section-number">6.5</span> The Poisson distribution<span></span></a>
<ul>
<li><a href="simple.html#summary-of-the-poissonmu-distribution" id="toc-summary-of-the-poissonmu-distribution"><span class="toc-section-number">6.5.1</span> Summary of the Poisson(<span class="math inline">\(\mu\)</span>) distribution<span></span></a></li>
</ul></li>
<li><a href="simple.html#summary-of-these-discrete-distributions" id="toc-summary-of-these-discrete-distributions"><span class="toc-section-number">6.6</span> Summary of these discrete distributions<span></span></a></li>
<li><a href="simple.html#uniform" id="toc-uniform"><span class="toc-section-number">6.7</span> The uniform distribution<span></span></a>
<ul>
<li><a href="simple.html#summary-of-the-uniformab-distribution" id="toc-summary-of-the-uniformab-distribution"><span class="toc-section-number">6.7.1</span> Summary of the uniform(<span class="math inline">\(a,b\)</span>) distribution<span></span></a></li>
</ul></li>
<li><a href="simple.html#exponential" id="toc-exponential"><span class="toc-section-number">6.8</span> The exponential distribution<span></span></a>
<ul>
<li><a href="simple.html#summary-of-the-exponentiallambda-distribution" id="toc-summary-of-the-exponentiallambda-distribution"><span class="toc-section-number">6.8.1</span> Summary of the exponential(<span class="math inline">\(\lambda\)</span>) distribution<span></span></a></li>
</ul></li>
<li><a href="simple.html#normal" id="toc-normal"><span class="toc-section-number">6.9</span> The normal distribution<span></span></a>
<ul>
<li><a href="simple.html#summary-of-the-mboxnmusigma2-distribution" id="toc-summary-of-the-mboxnmusigma2-distribution"><span class="toc-section-number">6.9.1</span> Summary of the <span class="math inline">\(\mbox{N}(\mu,\sigma^2)\)</span> distribution<span></span></a></li>
<li><a href="simple.html#the-standard-normal-disribution" id="toc-the-standard-normal-disribution"><span class="toc-section-number">6.9.2</span> The standard normal disribution<span></span></a></li>
<li><a href="simple.html#evaluating-the-normal-c.d.f.-and-quantiles" id="toc-evaluating-the-normal-c.d.f.-and-quantiles"><span class="toc-section-number">6.9.3</span> Evaluating the normal c.d.f. and quantiles<span></span></a></li>
<li><a href="simple.html#interpretation-of-sigma" id="toc-interpretation-of-sigma"><span class="toc-section-number">6.9.4</span> Interpretation of <span class="math inline">\(\sigma\)</span><span></span></a></li>
</ul></li>
<li><a href="simple.html#summary-of-these-continuous-distributions" id="toc-summary-of-these-continuous-distributions"><span class="toc-section-number">6.10</span> Summary of these continuous distributions<span></span></a></li>
<li><a href="simple.html#qq" id="toc-qq"><span class="toc-section-number">6.11</span> QQ plots<span></span></a>
<ul>
<li><a href="simple.html#normal-qq-plots" id="toc-normal-qq-plots"><span class="toc-section-number">6.11.1</span> Normal QQ plots<span></span></a></li>
<li><a href="simple.html#uniform-qq-plots" id="toc-uniform-qq-plots"><span class="toc-section-number">6.11.2</span> Uniform QQ plots<span></span></a></li>
<li><a href="simple.html#exponential-qq-plots" id="toc-exponential-qq-plots"><span class="toc-section-number">6.11.3</span> Exponential QQ plots<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="inference.html#inference" id="toc-inference"><span class="toc-section-number">7</span> Statistical Inference<span></span></a>
<ul>
<li><a href="inference.html#the-story-so-far" id="toc-the-story-so-far"><span class="toc-section-number">7.1</span> The story so far<span></span></a></li>
<li><a href="inference.html#sample-and-populations" id="toc-sample-and-populations"><span class="toc-section-number">7.2</span> Sample and populations<span></span></a></li>
<li><a href="inference.html#probmodels" id="toc-probmodels"><span class="toc-section-number">7.3</span> Probability models<span></span></a></li>
<li><a href="inference.html#fitting-models" id="toc-fitting-models"><span class="toc-section-number">7.4</span> Fitting models<span></span></a></li>
<li><a href="inference.html#uncertainty-in-estimation" id="toc-uncertainty-in-estimation"><span class="toc-section-number">7.5</span> Uncertainty in estimation<span></span></a>
<ul>
<li><a href="inference.html#simulation-coin-tossing-example" id="toc-simulation-coin-tossing-example"><span class="toc-section-number">7.5.1</span> Simulation: coin-tossing example<span></span></a></li>
<li><a href="inference.html#simnorm" id="toc-simnorm"><span class="toc-section-number">7.5.2</span> Simulation: estimating the parameters of a normal distribution<span></span></a></li>
<li><a href="inference.html#simexp" id="toc-simexp"><span class="toc-section-number">7.5.3</span> Simulation: estimating the parameters of an exponential distribution<span></span></a></li>
<li><a href="inference.html#central-limit-theorem" id="toc-central-limit-theorem"><span class="toc-section-number">7.5.4</span> Central Limit Theorem<span></span></a></li>
</ul></li>
<li><a href="inference.html#good" id="toc-good"><span class="toc-section-number">7.6</span> Properties of estimators<span></span></a>
<ul>
<li><a href="inference.html#bias" id="toc-bias"><span class="toc-section-number">7.6.1</span> Bias<span></span></a></li>
<li><a href="inference.html#varianceofestimator" id="toc-varianceofestimator"><span class="toc-section-number">7.6.2</span> Variance<span></span></a></li>
<li><a href="inference.html#mean-squared-error-mse" id="toc-mean-squared-error-mse"><span class="toc-section-number">7.6.3</span> Mean squared error (MSE)<span></span></a></li>
<li><a href="inference.html#standard-error" id="toc-standard-error"><span class="toc-section-number">7.6.4</span> Standard error<span></span></a></li>
<li><a href="inference.html#consistency" id="toc-consistency"><span class="toc-section-number">7.6.5</span> Consistency<span></span></a></li>
</ul></li>
<li><a href="inference.html#assessing-goodness-of-fit" id="toc-assessing-goodness-of-fit"><span class="toc-section-number">7.7</span> Assessing goodness-of-fit<span></span></a>
<ul>
<li><a href="inference.html#residuals" id="toc-residuals"><span class="toc-section-number">7.7.1</span> Residuals<span></span></a></li>
<li><a href="inference.html#standardised-residuals" id="toc-standardised-residuals"><span class="toc-section-number">7.7.2</span> Standardised residuals<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="contingency.html#contingency" id="toc-contingency"><span class="toc-section-number">8</span> Contingency tables<span></span></a>
<ul>
<li><a href="contingency.html#way2" id="toc-way2"><span class="toc-section-number">8.1</span> 2-way contingency tables<span></span></a>
<ul>
<li><a href="contingency.html#indep" id="toc-indep"><span class="toc-section-number">8.1.1</span> Independence<span></span></a></li>
<li><a href="contingency.html#compprob" id="toc-compprob"><span class="toc-section-number">8.1.2</span> Comparing probabilities<span></span></a></li>
<li><a href="contingency.html#measures" id="toc-measures"><span class="toc-section-number">8.1.3</span> Measures of association<span></span></a></li>
</ul></li>
<li><a href="contingency.html#way3" id="toc-way3"><span class="toc-section-number">8.2</span> 3-way contingency tables<span></span></a>
<ul>
<li><a href="contingency.html#mutual-independence" id="toc-mutual-independence"><span class="toc-section-number">8.2.1</span> Mutual independence<span></span></a></li>
<li><a href="contingency.html#marginal-independence" id="toc-marginal-independence"><span class="toc-section-number">8.2.2</span> Marginal independence<span></span></a></li>
<li><a href="contingency.html#conditional-independence" id="toc-conditional-independence"><span class="toc-section-number">8.2.3</span> Conditional independence<span></span></a></li>
<li><a href="contingency.html#confounding-variables" id="toc-confounding-variables"><span class="toc-section-number">8.2.4</span> Confounding variables<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="linreg.html#linreg" id="toc-linreg"><span class="toc-section-number">9</span> Linear regression<span></span></a>
<ul>
<li><a href="linreg.html#simple-linear-regression" id="toc-simple-linear-regression"><span class="toc-section-number">9.1</span> Simple linear regression<span></span></a>
<ul>
<li><a href="linreg.html#simple-linear-regression-model" id="toc-simple-linear-regression-model"><span class="toc-section-number">9.1.1</span> Simple linear regression model<span></span></a></li>
<li><a href="linreg.html#least-squares-estimation-of-alpha-and-beta" id="toc-least-squares-estimation-of-alpha-and-beta"><span class="toc-section-number">9.1.2</span> Least squares estimation of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span><span></span></a></li>
<li><a href="linreg.html#least-squares-fitting-to-hubbles-data" id="toc-least-squares-fitting-to-hubbles-data"><span class="toc-section-number">9.1.3</span> Least squares fitting to Hubble’s data<span></span></a></li>
<li><a href="linreg.html#normal-linear-regression-model" id="toc-normal-linear-regression-model"><span class="toc-section-number">9.1.4</span> Normal linear regression model<span></span></a></li>
<li><a href="linreg.html#lmsummary" id="toc-lmsummary"><span class="toc-section-number">9.1.5</span> Summary of the assumptions of a (normal) linear regression model<span></span></a></li>
</ul></li>
<li><a href="linreg.html#looking" id="toc-looking"><span class="toc-section-number">9.2</span> Looking at scatter plots<span></span></a></li>
<li><a href="linreg.html#model-checking" id="toc-model-checking"><span class="toc-section-number">9.3</span> Model checking<span></span></a>
<ul>
<li><a href="linreg.html#departures-from-assumptions" id="toc-departures-from-assumptions"><span class="toc-section-number">9.3.1</span> Departures from assumptions<span></span></a></li>
<li><a href="linreg.html#outliers" id="toc-outliers"><span class="toc-section-number">9.3.2</span> Outliers and influential observations<span></span></a></li>
</ul></li>
<li><a href="linreg.html#linregtrans" id="toc-linregtrans"><span class="toc-section-number">9.4</span> Use of transformations<span></span></a>
<ul>
<li><a href="linreg.html#interpretation-after-transformation" id="toc-interpretation-after-transformation"><span class="toc-section-number">9.4.1</span> Interpretation after transformation<span></span></a></li>
</ul></li>
<li><a href="linreg.html#over-fitting" id="toc-over-fitting"><span class="toc-section-number">9.5</span> Over-fitting<span></span></a></li>
</ul></li>
<li><a href="correlationchapter.html#correlationchapter" id="toc-correlationchapter"><span class="toc-section-number">10</span> Correlation<span></span></a>
<ul>
<li><a href="correlationchapter.html#correlation-a-measure-of-linear-association" id="toc-correlation-a-measure-of-linear-association"><span class="toc-section-number">10.1</span> Correlation: a measure of linear association<span></span></a></li>
<li><a href="correlationchapter.html#covariance-and-correlation" id="toc-covariance-and-correlation"><span class="toc-section-number">10.2</span> Covariance and correlation<span></span></a>
<ul>
<li><a href="correlationchapter.html#estimation" id="toc-estimation"><span class="toc-section-number">10.2.1</span> Estimation<span></span></a></li>
<li><a href="correlationchapter.html#links-between-regression-and-correlation" id="toc-links-between-regression-and-correlation"><span class="toc-section-number">10.2.2</span> Links between regression and correlation<span></span></a></li>
</ul></li>
<li><a href="correlationchapter.html#use-and-misuse-of-correlation" id="toc-use-and-misuse-of-correlation"><span class="toc-section-number">10.3</span> Use and misuse of correlation<span></span></a>
<ul>
<li><a href="correlationchapter.html#do-not-use-correlation-for-regression-sampling-schemes" id="toc-do-not-use-correlation-for-regression-sampling-schemes"><span class="toc-section-number">10.3.1</span> Do not use correlation for regression sampling schemes<span></span></a></li>
<li><a href="correlationchapter.html#correxamples" id="toc-correxamples"><span class="toc-section-number">10.3.2</span> Examples of correlations of different strengths<span></span></a></li>
<li><a href="correlationchapter.html#beware-missing-data-codes" id="toc-beware-missing-data-codes"><span class="toc-section-number">10.3.3</span> Beware missing data codes<span></span></a></li>
<li><a href="correlationchapter.html#more-guessing-sample-correlations" id="toc-more-guessing-sample-correlations"><span class="toc-section-number">10.3.4</span> More guessing sample correlations<span></span></a></li>
<li><a href="correlationchapter.html#summary" id="toc-summary"><span class="toc-section-number">10.3.5</span> Summary<span></span></a></li>
<li><a href="correlationchapter.html#anscombes-datasets" id="toc-anscombes-datasets"><span class="toc-section-number">10.3.6</span> Anscombe’s datasets<span></span></a></li>
<li><a href="correlationchapter.html#we-must-interpret-correlation-with-care." id="toc-we-must-interpret-correlation-with-care."><span class="toc-section-number">10.3.7</span> We must interpret correlation with care.<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="a-general-strategy-for-statistical-modelling.html#a-general-strategy-for-statistical-modelling" id="toc-a-general-strategy-for-statistical-modelling"><span class="toc-section-number">11</span> A general strategy for statistical modelling<span></span></a></li>
<li><a href="references.html#references" id="toc-references">References<span></span></a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">STAT0002 Introduction to Probability and Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="rvs" class="section level1 hasAnchor" number="5">
<h1><span class="header-section-number">Chapter 5</span> Random variables<a href="rvs.html#rvs" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><strong>Example</strong>. We return to the space shuttle example.</p>
<p>Consider what happens to the O-rings on a particular test flight, at a particular temperature. A given O-ring either is damaged (shows signs of thermal distress) or it is not damaged. Let <span class="math inline">\(D\)</span> denote the event that an O-ring is damaged and <span class="math inline">\(\bar{D}\)</span> the event that it is not damaged. If we consider all 6 O-rings, there are many possible outcomes in the sample space, <span class="math inline">\(2^6=64\)</span>, in fact:
<span class="math display">\[ S= \{DDDDDD\}, \{DDDDD\bar{D}\}, \ldots, \{D\bar{D}\bar{D}\bar{D}\bar{D}\bar{D}\},
\{\bar{D}\bar{D}\bar{D}\bar{D}\bar{D}\bar{D}\}. \]</span>
Suppose that we are not interested in which particular O-rings were damaged, just the total number <span class="math inline">\(N\)</span> of damaged O-rings. The possible values for <span class="math inline">\(N\)</span> are 0,1,2,3,4,5,6.</p>
<p>Each outcome in <span class="math inline">\(S\)</span> gives a value for <span class="math inline">\(N\)</span> in {0,1,2,3,4,5,6}:</p>
<p><span class="math inline">\(\{DDDDDD\}\)</span> gives <span class="math inline">\(N=6\)</span>,</p>
<p><span class="math inline">\(\{DDDDD\bar{D}\}\)</span> gives <span class="math inline">\(N=5\)</span>,</p>
<p><span class="math inline">\(\{DDDD\bar{D}D\}\)</span> gives <span class="math inline">\(N=5\)</span>,</p>
<p><span class="math inline">\(\vdots\)</span></p>
<p><span class="math inline">\(\{\bar{D}\bar{D}\bar{D}\bar{D}\bar{D}\bar{D}\}\)</span> gives <span class="math inline">\(N=0\)</span>.</p>
<p>By defining <span class="math inline">\(N\)</span> to be the total number of damaged O-rings, we have moved from considering outcomes to considering a variable with a numerical value. <span class="math inline">\(N\)</span> is a real-valued function on the sample space <span class="math inline">\(S\)</span>, that is, <span class="math inline">\(N\)</span> maps each outcome in <span class="math inline">\(S\)</span> to a real number. <span class="math inline">\(N\)</span> is a rule that assigns a real number to every outcome <span class="math inline">\(s\)</span> in <span class="math inline">\(S\)</span>. Since the outcomes in <span class="math inline">\(S\)</span> are random the variable <span class="math inline">\(N\)</span> is also random, and we can assign probabilities to its possible values, that is, <span class="math inline">\(P(N=0), P(N=1)\)</span> and so on.</p>
<p><span class="math inline">\(N\)</span> is a <strong>random variable</strong>. In fact, if we assume that O-rings are damaged independently of each other and each O-ring has the same probability <span class="math inline">\(p\)</span> of being damaged, <span class="math inline">\(N\)</span> is a random variable with a special name. It is a binomial random variable with parameters 6 and <span class="math inline">\(p\)</span>. We will consider binomial random variables in more detail in Section <a href="simple.html#binomial">6.3</a>.</p>
<p><strong>Notation</strong>. We denote random variables by upper case letters, for example, <span class="math inline">\(N, X, Y, Z\)</span>. Once we have observed the value of a random variable it is no longer random: it is equal to a particular value. To make this clear we denote sample values of r.v.s. by lower case letters, for example, <span class="math inline">\(n, x, y, z\)</span> and write <span class="math inline">\(N=n, X=x\)</span> and so on. Thus, <span class="math inline">\(P(X=x)\)</span> is the probability that the random variable <span class="math inline">\(X\)</span> has the value <span class="math inline">\(x\)</span>.</p>
<div id="discrete" class="section level2 hasAnchor" number="5.1">
<h2><span class="header-section-number">5.1</span> Discrete random variables<a href="rvs.html#discrete" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Definition</strong>. A discrete random variable is a random variable that can take only a finite, or countably infinite, number of values.</p>
<p>An example of a countably infinite set of values is {0,1,2,3, …}. The random variable <span class="math inline">\(N\)</span> in the space shuttle example takes a finite number of values: 0,1,2,3,4,5,6. Therefore <span class="math inline">\(N\)</span> is a discrete random variable.</p>
<p><strong>Definition</strong>. Let <span class="math inline">\(X\)</span> be a discrete random variable. The <strong>probability mass function (p.m.f.)</strong> <span class="math inline">\(p_X(x)\)</span>, or simply <span class="math inline">\(p(x)\)</span>, of <span class="math inline">\(X\)</span> is
<span class="math display">\[ p_X(x) = P(X=x), \qquad \mbox{for $x$ in the support of $X$}.  \]</span></p>
<p>The p.m.f. of <span class="math inline">\(X\)</span> tells us the probability with which <span class="math inline">\(X\)</span> takes any particular value <span class="math inline">\(x\)</span>. The <strong>support</strong> of <span class="math inline">\(X\)</span> is the set of values that it is possible for <span class="math inline">\(X\)</span> to take. It is very important to write this down every time you write down a p.m.f.. A discrete random variable is completely specified by its probability mass function.</p>
<p><strong>Properties of p.m.f.s</strong></p>
<p>Let <span class="math inline">\(X\)</span> take values <span class="math inline">\(x_1, x_2,\ldots.\)</span> Then</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(p_X(x_i) \geq 0\)</span>, for all <span class="math inline">\(i\)</span>,</li>
<li><span class="math inline">\(\displaystyle\sum_i p_X(x_i) = 1\)</span>.</li>
</ol>
<p>Note: 1. is true because the <span class="math inline">\(p_X(x_i)\)</span>s are probabilities; 2. is true because summing over the <span class="math inline">\(x_i\)</span>s is equivalent to summing over the sample space of outcomes.</p>
<p><strong>Definition</strong>. The cumulative distribution function (c.d.f.) of a random variable <span class="math inline">\(X\)</span> is
<span class="math display">\[ F_X(x) = P(X \leq x), \qquad \mbox{for} -\infty &lt; x &lt; \infty. \]</span></p>
<p><strong>Relationship between the c.d.f. and p.m.f. of a discrete random variable</strong>. For a discrete random variable:
<span class="math display">\[ F_X(x) = P(X \leq x) = \sum_{x_i \leq x} P(X = x_i). \]</span>
Therefore, assuming for the moment that the random variable takes only integer values,
<span class="math display">\[ P(X=x) = P(X \leq x) - P(X \leq x-1) = F_X(x) - F_X(x-1) \]</span>
for any integer <span class="math inline">\(x\)</span></p>
</div>
<div id="continuous" class="section level2 hasAnchor" number="5.2">
<h2><span class="header-section-number">5.2</span> Continuous random variables<a href="rvs.html#continuous" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Example</strong>. We return to the Oxford birth times example.</p>
<p>The top plot in Figure <a href="rvs.html#fig:oxcontvar">5.1</a> shows a histogram of the 95 birth times. The variable of interest in this example is a time. Time is a continuous variable: in principle, the times in this dataset could take any positive real value, uncountably many values. In practice, these times have been recorded discretely, in units of 1/10 of an hour or 1/4 of an hour.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:oxcontvar"></span>
<img src="images/ox_cont_var.png" alt="Top: histogram of the Oxford birth durations. Second from top: histogram of 1,000 values simulated from a distribution fitted to the data. Second from bottom: similarly for 10,000 simulated values. Bottom: p.d.f. of the distribution fitted to the Oxford birth times data." width="75%" />
<p class="caption">
Figure 5.1: Top: histogram of the Oxford birth durations. Second from top: histogram of 1,000 values simulated from a distribution fitted to the data. Second from bottom: similarly for 10,000 simulated values. Bottom: p.d.f. of the distribution fitted to the Oxford birth times data.
</p>
</div>
<p>Suppose that we continue to collect data on birth duration from this hospital, and, as new observations arrive, we add them to the top histogram in Figure <a href="rvs.html#fig:oxcontvar">5.1</a>. We imagine that the times are recorded continuously. As the number of observations <span class="math inline">\(n\)</span> increases we decrease the bin width of the histogram. As <span class="math inline">\(n\)</span> increases to infinity the bin width shrinks to zero and the histogram tends to a smooth continuous curve.</p>
<p>This is shown in the bottom 3 plots in Figure <a href="rvs.html#fig:oxcontvar">5.1</a>. The extra data are not real. They are data I have simulated, using a computer, to have a distribution with a similar shape to the histogram of the real data.</p>
<p>Let <span class="math inline">\(T\)</span> denote the time, in hours, that a woman arriving at the hospital takes to give birth. The smooth continuous curve at the bottom of Figure <a href="rvs.html#fig:oxcontvar">5.1</a> is called the <strong>probability density function (p.d.f.)</strong> <span class="math inline">\(f_T(t)\)</span> of the random variable <span class="math inline">\(T\)</span>. Since the total area of the rectangles in a histogram is equal to 1, the area <span class="math inline">\(\int_{-\infty}^{\infty} f_T(t) \, \mathrm{d}t\)</span> under the p.d.f. <span class="math inline">\(f_T(t)\)</span> is equal to 1.</p>
<p><strong>Definition</strong>. A <strong>probability density function (p.d.f.)</strong> is a function <span class="math inline">\(f_{X}(x)\)</span>, or simply <span class="math inline">\(f(x)\)</span>, such that</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(f_X(x) \geq 0\)</span>, for <span class="math inline">\(-\infty &lt; x &lt; \infty\)</span>;</li>
<li><span class="math inline">\(\displaystyle\int_{-\infty}^{\infty} f_X(x) \, \mathrm{d}x = 1\)</span>.</li>
</ol>
<p>Therefore, p.d.f.s are always non-negative and integrate to 1. The support of a continuous random variable is the set of values for which the p.d.f. is positive. Suppose that we wish to find <span class="math inline">\(P(4 &lt; T \leq 12)\)</span>. To find the proportion of times between 4 and 12 using a histogram, we sum the areas of all bins between 4 and 12, that is, we find the area shaded in the histogram in Figure <a href="rvs.html#fig:oxshady">5.2</a>. To do this using the p.d.f. we do effectively the same thing: we find the area under the p.d.f. <span class="math inline">\(f_T(t)\)</span> between 4 and 12. Since <span class="math inline">\(f_T(t)\)</span> is a smooth continuous curve, (that is, the bin widths are zero) we integrate <span class="math inline">\(f_T(t)\)</span> between 4 and 12.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:oxshady"></span>
<img src="images/ox_shady.png" alt="Top: histogram of the Oxford birth durations. Bottom: p.d.f. of the distribution fitted to the Oxford birth duration data." width="75%" />
<p class="caption">
Figure 5.2: Top: histogram of the Oxford birth durations. Bottom: p.d.f. of the distribution fitted to the Oxford birth duration data.
</p>
</div>
<p>Therefore
<span class="math display">\[ P(4 &lt; T \leq 12) = \displaystyle\int_4^{12} f_T(t) \,\mathrm{d}t = F_T(12)-F_T(4). \]</span></p>
<p>More generally,
<span class="math display">\[ P(a &lt; T \leq b) = \displaystyle\int_a^b f_T(t) \,\mathrm{d}t = F_T(b)-F_T(a). \]</span></p>
<p><strong>Definition</strong>. A random variable <span class="math inline">\(X\)</span> is a <strong>continuous random variable</strong> if there exists a p.d.f. <span class="math inline">\(f_X(x)\)</span> such that
<span class="math display">\[
P(a &lt; X \leq b) = \int_{a}^{b} f_X(x) \,\mathrm{d}x,
\]</span>
for all <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> such that <span class="math inline">\(a &lt; b\)</span>.</p>
<p>Figure <a href="rvs.html#fig:pdfshady">5.3</a> illustrates the properties of a p.d.f..</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pdfshady"></span>
<img src="images/pdf_shady.png" alt="Properties of a p.d.f.. The areas that correspond to the probability that a random variable takes a value in a given interval are shaded." width="75%" />
<p class="caption">
Figure 5.3: Properties of a p.d.f.. The areas that correspond to the probability that a random variable takes a value in a given interval are shaded.
</p>
</div>
<p>Notes</p>
<ul>
<li>It is very important to appreciate that <span class="math inline">\(f_X(x)\)</span> is <strong>not</strong> a probability: it does <strong>not</strong> give <span class="math inline">\(P(X=x)\)</span>. In fact <span class="math inline">\(P(X=x)=0\)</span>: the probability that a continuous random variable <span class="math inline">\(X\)</span> takes the value <span class="math inline">\(x\)</span> is zero.</li>
<li>Indeed, it is possible for a p.d.f. to be greater than 1. Consider a continuous random variable <span class="math inline">\(X\)</span> with p.d.f.
<span class="math display">\[ f_X(x) = \left\{ \begin{array}{ll} 2\,(1-x) &amp; \,0 \leq x \leq 1, \\ 0 &amp; \,\mbox{otherwise}.\end{array}\right. \]</span>
For this random variable <span class="math inline">\(f_X(x)&gt;1\)</span> for any <span class="math inline">\(x \in [0, 1/2)\)</span> .</li>
<li>Since <span class="math inline">\(P(X=x)=0\)</span>
<span class="math display">\[ P(a &lt; X \leq b) = P(a \leq X \leq b) = P(a \leq X &lt; b) = P(a &lt; X &lt; b). \]</span></li>
<li><span class="math inline">\(f_X(x)\)</span> is a probability <strong>density</strong>. The probability that <span class="math inline">\(X\)</span> lies in a very small interval of length <span class="math inline">\(\delta\)</span> near <span class="math inline">\(x\)</span> is approximately <span class="math inline">\(f_X(x) \delta\)</span>. For the p.d.f. at the bottom of figure <a href="rvs.html#fig:oxcontvar">5.1</a>, <span class="math inline">\(f_T(6) &gt; f_T(12)\)</span>, indicating that a randomly chosen woman is more likely to spend approximately 6 hours giving birth than approximately
12 hours.</li>
</ul>
<p><strong>Relationship between the c.d.f. and p.d.f. of a continuous random variable</strong>. For a continuous random variable
<span class="math display">\[ F_X(x) = P(X \leq x) = \int_{-\infty}^x f_X(u) \,\mathrm{d}u. \]</span>
Therefore,
<span class="math display">\[ f_X(x) = \frac{\mathrm{d}}{\mathrm{d}x} F_X(x). \]</span></p>
</div>
<div id="expectation" class="section level2 hasAnchor" number="5.3">
<h2><span class="header-section-number">5.3</span> Expectation<a href="rvs.html#expectation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The expectation of a random variable is a measure of the location of its distribution.</p>
<div id="expectation-of-a-discrete-random-variable" class="section level3 hasAnchor" number="5.3.1">
<h3><span class="header-section-number">5.3.1</span> Expectation of a discrete random variable<a href="rvs.html#expectation-of-a-discrete-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Example</strong>. We return to the space shuttle example.</p>
<p>Again we consider test flights conducted at a particular temperature, say 53<span class="math inline">\(^\circ\)</span>F. Suppose that NASA are able to conduct a very large number <span class="math inline">\(n\)</span> of test flights at 53<span class="math inline">\(^\circ\)</span>F, producing a sample <span class="math inline">\(x_1,\ldots,x_n\)</span> of numbers of damaged O-rings.</p>
<p>Let <span class="math inline">\(n(x)\)</span> be the number of test flights on which <span class="math inline">\(x\)</span> of the 6 O-rings were damaged. We can write the sample mean <span class="math inline">\(\bar{x}\)</span> of <span class="math inline">\(x_1,\ldots,x_n\)</span> as
<span class="math display">\[\begin{eqnarray*}
\bar{x} &amp;=&amp; \frac{0 \times n(0) + 1 \times n(1) + \cdots + 6 \times n(6)}{n}, \\
&amp;=&amp; \sum_{x=0}^6 x\,\frac{n(x)}{n}.
\end{eqnarray*}\]</span>
As the sample size <span class="math inline">\(n\)</span> increases to infinity, the sample proportion <span class="math inline">\(n(x)/n\)</span> tends to <span class="math inline">\(P(X=x)\)</span>, for <span class="math inline">\(x=0,1,\ldots,6\)</span>. Therefore, in the limit as <span class="math inline">\(n \rightarrow \infty\)</span>, <span class="math inline">\(\bar{x}\)</span> tends to
<span class="math display" id="eq:shuttlemean">\[\begin{eqnarray}
\sum_{x=0}^6 x\,P(X=x).
\tag{5.1}
\end{eqnarray}\]</span>
This is known as the mean of the probability distribution of <span class="math inline">\(X\)</span>. It is a measure of the location of the distribution.</p>
<p>The quantity in equation <a href="rvs.html#eq:shuttlemean">(5.1)</a> is the value of the sample mean <span class="math inline">\(\bar{x}\)</span> that we would expect to get from a very large sample. Therefore it is often called the <strong>expectation</strong> or <strong>expected value</strong> of the random variable <span class="math inline">\(X\)</span> and it is denoted <span class="math inline">\(\mathrm{E}(X)\)</span>.</p>
<p><strong>Definition</strong>. The <strong>expectation</strong> (or <strong>expected value</strong> or <strong>mean</strong>) <span class="math inline">\(\mathrm{E}(X)\)</span> of a discrete random variable <span class="math inline">\(X\)</span> is given by
<span class="math display" id="eq:discmean">\[\begin{eqnarray}
\mathrm{E}(X) &amp;=&amp; \sum_x x\,P(X=x).
\tag{5.2}
\end{eqnarray}\]</span>
This is a weighted average of the values that <span class="math inline">\(X\)</span> can take, each value being weighted by <span class="math inline">\(P(X=x)\)</span>.</p>
<p>Note:</p>
<ul>
<li>We often write <span class="math inline">\(\mu\)</span> or <span class="math inline">\(\mu_X\)</span> for <span class="math inline">\(\mathrm{E}(X)\)</span>.</li>
<li>Units: the units of <span class="math inline">\(\mathrm{E}(X)\)</span> are the same as those of <span class="math inline">\(X\)</span>. For example, if <span class="math inline">\(X\)</span> is measured in hours then <span class="math inline">\(\mathrm{E}(X)\)</span> is measured in hours.</li>
<li><span class="math inline">\(\mathrm{E}(X)\)</span> exists only if <span class="math inline">\(\sum_x |x|\,P(X=x) &lt; \infty\)</span>. If the number of values <span class="math inline">\(X\)</span> can take is finite then <span class="math inline">\(\mathrm{E}(X)\)</span> will always exist.</li>
</ul>
</div>
<div id="expectation-of-a-continuous-random-variable" class="section level3 hasAnchor" number="5.3.2">
<h3><span class="header-section-number">5.3.2</span> Expectation of a continuous random variable<a href="rvs.html#expectation-of-a-continuous-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can define the expectation of a continuous random variable in a similar way to a discrete random variable, replacing summation with integration.</p>
<p><strong>Definition</strong>.
The expectation <span class="math inline">\(\mathrm{E}(X)\)</span> of a continuous random variable <span class="math inline">\(X\)</span> is given by
<span class="math display" id="eq:contmean">\[\begin{eqnarray}
\mathrm{E}(X) = \int_{-\infty}^{\infty} x\,f_X(x) \,\mathrm{d}x.
\tag{5.3}
\end{eqnarray}\]</span>
Note:</p>
<ul>
<li>Like the discrete case, this is a weighted average of the values that <span class="math inline">\(X\)</span> can take, but now each value is weighted by the
p.d.f. <span class="math inline">\(f_X(x)\)</span>.</li>
<li>The range of integration in equation <a href="rvs.html#eq:contmean">(5.3)</a> is over the whole real line but, in practice, integration will be over the range of possible values of <span class="math inline">\(X\)</span>.</li>
<li><span class="math inline">\(\mathrm{E}(X)\)</span> exists only if <span class="math inline">\(\int_{-\infty}^{\infty} |x|\,f_X(x) \,\mathrm{d}x &lt; \infty\)</span>.</li>
</ul>
</div>
<div id="properties-of-mathrmex" class="section level3 hasAnchor" number="5.3.3">
<h3><span class="header-section-number">5.3.3</span> Properties of <span class="math inline">\(\mathrm{E}(X)\)</span><a href="rvs.html#properties-of-mathrmex" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are constants then
<span class="math display">\[ \mathrm{E}(a\,X+b) = a\,\mathrm{E}(X)+b. \]</span>
This makes sense. If we multiply all observations by <span class="math inline">\(a\)</span> their mean will also be multiplied by <span class="math inline">\(a\)</span>. If we add <span class="math inline">\(b\)</span> to all observations their mean will be increased by <span class="math inline">\(b\)</span>, that is, the distribution of <span class="math inline">\(X\)</span> shifts up by <span class="math inline">\(b\)</span>.</p>
<ul>
<li>If <span class="math inline">\(X \geq 0\)</span> then <span class="math inline">\(\mathrm{E}(X) \geq 0\)</span>.</li>
<li>If <span class="math inline">\(X\)</span> is a constant <span class="math inline">\(c\)</span>, that is, <span class="math inline">\(P(X=c)=1\)</span> then <span class="math inline">\(\mathrm{E}(X)=c\)</span>.</li>
<li>It can be shown that
<span class="math display">\[ \mathrm{E}(X_1 + X_2 + \cdots + X_n) = \mathrm{E}(X_1) + \mathrm{E}(X_2) + \cdots + \mathrm{E}(X_n). \]</span></li>
</ul>
</div>
<div id="EgX" class="section level3 hasAnchor" number="5.3.4">
<h3><span class="header-section-number">5.3.4</span> The expectation of <span class="math inline">\(g(X)\)</span><a href="rvs.html#EgX" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose that <span class="math inline">\(Y=g(X)\)</span> is a function of of <span class="math inline">\(X\)</span>, such as <span class="math inline">\(aX+b\)</span>, <span class="math inline">\(X^2\)</span> or <span class="math inline">\(\log X\)</span>. Then <span class="math inline">\(Y\)</span> is also a random variable. If we find the p.m.f (if <span class="math inline">\(Y\)</span> is discrete) or p.d.f. (if <span class="math inline">\(Y\)</span> is continuous) of <span class="math inline">\(Y\)</span> then we can find the expectation of <span class="math inline">\(Y\)</span> using equation <a href="rvs.html#eq:discmean">(5.2)</a> or <a href="rvs.html#eq:contmean">(5.3)</a> as appropriate.</p>
<p><span class="math display" id="eq:expfn">\[\begin{equation}
\mathrm{E}(Y) = \mathrm{E}[g(X)] =
\begin{cases}
\displaystyle\sum_x g(x)\,P(X=x) &amp; \text{if } X \text{ is discrete}, \\
\int_{-\infty}^{\infty} g(x)\,f_X(x) \,\mathrm{d}x &amp; \text{if } X \text{ is continuous}.
\end{cases}
\tag{5.4}
\end{equation}\]</span></p>
<p>Note: for a non-linear function <span class="math inline">\(g(X)\)</span>, it is usually the case that
<span class="math display">\[ \mathrm{E}[g(X)] \neq g[\mathrm{E}(X)] \]</span>
although there are exceptions.</p>
</div>
</div>
<div id="variance" class="section level2 hasAnchor" number="5.4">
<h2><span class="header-section-number">5.4</span> Variance<a href="rvs.html#variance" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The variance of a random variable is a measure of the spread of its distribution.</p>
<div id="variance-of-a-discrete-random-variable" class="section level3 hasAnchor" number="5.4.1">
<h3><span class="header-section-number">5.4.1</span> Variance of a discrete random variable<a href="rvs.html#variance-of-a-discrete-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Example</strong>. We return the space shuttle example.</p>
<p>As before we let <span class="math inline">\(n(x)\)</span> be the number of test flights on which <span class="math inline">\(x\)</span> of the 6 O-rings were damaged. We saw in Section <a href="descriptive.html#meanstdev">2.3.2</a> that a measure of the spread of a sample <span class="math inline">\(x_1,\ldots,x_n\)</span> is the sample variance <span class="math inline">\(s_X^2\)</span> which, in this example, can be written as</p>
<p><span class="math display">\[\begin{eqnarray*}
s_X^2 &amp;=&amp; \frac{1}{n-1}\,\left\{
(0-\bar{x})^2\,n(0)+(1-\bar{x})^2\,n(1)+\cdots+(6-\bar{x})^2\,n(6) \right\},
\\
      &amp;=&amp; \sum_{x=0}^6 (x-\bar{x})^2\,\frac{n(x)}{n-1}.
\end{eqnarray*}\]</span>
As the sample size <span class="math inline">\(n\)</span> increases to infinity, <span class="math inline">\(\frac{n(x)}{n-1}\)</span> tends to <span class="math inline">\(P(X=x)\)</span>, for <span class="math inline">\(x=0,1,\ldots,6\)</span> and <span class="math inline">\(\bar{x}\)</span> tends to <span class="math inline">\(\mu\)</span>=<span class="math inline">\(\mathrm{E}(X)\)</span>.</p>
<p>Therefore, as <span class="math inline">\(n \rightarrow \infty\)</span>, <span class="math inline">\(s_X^2\)</span> tends to</p>
<p><span class="math display" id="eq:shuttlevar">\[\begin{equation}
\sum_{x=0}^6 (x-\mu)^2\,P(X=x).
\tag{5.5}
\end{equation}\]</span></p>
<p>This is known as the variance of the probability distribution of <span class="math inline">\(X\)</span>. It is a measure of the spread of the distribution. The quantity in equation <a href="rvs.html#eq:shuttlevar">(5.5)</a> is the value of the sample variance <span class="math inline">\(s_X^2\)</span> that we would expect to get from a very large sample.</p>
<p><strong>Definition</strong>. The variance <span class="math inline">\(\mathrm{var}(X)\)</span> of a discrete random variable <span class="math inline">\(X\)</span> with mean <span class="math inline">\(\mathrm{E}(X)=\mu\)</span> is given by</p>
<p><span class="math display" id="eq:varidisc">\[\begin{equation}
\mathrm{var}(X) = \sum_x\,(x-\mu)^2\,P(X=x).
\tag{5.6}
\end{equation}\]</span></p>
<p>This is a weighted average of the squared differences between the values that <span class="math inline">\(X\)</span> can take and its mean <span class="math inline">\(\mu\)</span>, each value being weighted by <span class="math inline">\(P(X=x)\)</span>.</p>
<p>A variance can be infinite. If the number of values that <span class="math inline">\(X\)</span> can take is finite then <span class="math inline">\(\mathrm{var}(X)\)</span> will always be finite.</p>
</div>
<div id="variance-of-a-continuous-random-variable" class="section level3 hasAnchor" number="5.4.2">
<h3><span class="header-section-number">5.4.2</span> Variance of a continuous random variable<a href="rvs.html#variance-of-a-continuous-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We can define the variance of a continuous random variable in a similar way to a discrete random variable, replacing summation with integration.</p>
<p><strong>Definition</strong>. The variance <span class="math inline">\(\mathrm{var}(X)\)</span> of a continuous random variable <span class="math inline">\(X\)</span> with mean <span class="math inline">\(\mathrm{E}(X)=\mu\)</span> is given by</p>
<p><span class="math display" id="eq:varicont">\[\begin{equation}
\mathrm{var}(X) = \int_{-\infty}^{\infty} (x-\mu)^2 f_X(x) \,\mathrm{d}x.
\tag{5.7}
\end{equation}\]</span></p>
</div>
<div id="variance-and-standard-deviation" class="section level3 hasAnchor" number="5.4.3">
<h3><span class="header-section-number">5.4.3</span> Variance and standard deviation<a href="rvs.html#variance-and-standard-deviation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Definition</strong>. Let <span class="math inline">\(X\)</span> be a random variable with <span class="math inline">\(\mathrm{E}(X)=\mu\)</span>. The variance <span class="math inline">\(\mathrm{var}(X)\)</span> is given by
<span class="math display">\[ \mathrm{var}(X) = \mathrm{E}\left[(X-\mu)^2\right]. \]</span>
This follows from equations <a href="rvs.html#eq:varidisc">(5.6)</a> and <a href="rvs.html#eq:varicont">(5.7)</a> and the expression in equation <a href="rvs.html#eq:expfn">(5.4)</a> for the expectation of a function <span class="math inline">\(g(X)\)</span> of a random variable <span class="math inline">\(X\)</span>.</p>
<p>There is an alternative way to calculate <span class="math inline">\(\mathrm{var}(X)\)</span>:
<span class="math display">\[ \mathrm{var}(X) = \mathrm{E}\left(X^2\right) - [\mathrm{E}(X)]^2. \]</span></p>
<p><strong>Exercise</strong>. Prove this.</p>
<p><strong>Definition</strong>. The standard deviation sd<span class="math inline">\((X)\)</span> of <span class="math inline">\(X\)</span> is given by sd(<span class="math inline">\(X\)</span>)=<span class="math inline">\(+\sqrt{\mathrm{var}(X)}\)</span>.</p>
<p>Notes on <span class="math inline">\(\mathrm{var}(X)\)</span> and sd(<span class="math inline">\(X\)</span>):</p>
<ul>
<li><span class="math inline">\(\mathrm{var}(X) \geq 0\)</span> and <span class="math inline">\(\mathrm{sd}(X) \geq 0\)</span>. Variances and standard deviations cannot be negative.<br />
</li>
<li>The units of <span class="math inline">\(\mathrm{var}(X)\)</span> are the square of those of <span class="math inline">\(X\)</span>. For example, if <span class="math inline">\(X\)</span> is measured in hours then <span class="math inline">\(\mathrm{var}(X)\)</span> is measured in hours<span class="math inline">\(^2\)</span> (and sd(<span class="math inline">\(X\)</span>) is measured in hours). The units of <span class="math inline">\(\mathrm{sd}(X)\)</span> are the same as those of <span class="math inline">\(X\)</span>.</li>
<li>We often write <span class="math inline">\(\sigma^2\)</span> or <span class="math inline">\(\sigma_X^2\)</span> for <span class="math inline">\(\mathrm{var}(X)\)</span> and
<span class="math inline">\(\sigma\)</span> or <span class="math inline">\(\sigma_X\)</span> for <span class="math inline">\(\mathrm{sd}(X)\)</span>.</li>
<li><span class="math inline">\(\mathrm{var}(X)\)</span> exists only if <span class="math inline">\(\mu\)</span> exists.</li>
</ul>
</div>
<div id="properties-of-mathrmvarx" class="section level3 hasAnchor" number="5.4.4">
<h3><span class="header-section-number">5.4.4</span> Properties of <span class="math inline">\(\mathrm{var}(X)\)</span><a href="rvs.html#properties-of-mathrmvarx" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>If <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are constants then
<span class="math display">\[ \mathrm{var}(a\,X+b) = a^2\,\mathrm{var}(X). \]</span>
This makes sense. If we multiply all observations by <span class="math inline">\(a\)</span> their variance, which is measured square units, will be multiplied by <span class="math inline">\(a^2\)</span>. If we add <span class="math inline">\(b\)</span> to all observations their variance will be unchanged because the distribution simply shifts up by <span class="math inline">\(b\)</span> and its spread is unaffected.</li>
<li>If <span class="math inline">\(X\)</span> is a constant <span class="math inline">\(c\)</span>, that is, <span class="math inline">\(P(X=c)=1\)</span> then <span class="math inline">\(\mathrm{var}(X)=0\)</span>: the distribution of <span class="math inline">\(X\)</span> has zero spread.</li>
<li>It can also be shown that <strong>if the random variables <span class="math inline">\(X_1, X_2, \ldots X_n\)</span> are independent</strong> then</li>
</ul>
<p><span class="math display" id="eq:varsum">\[\begin{equation}
\mathrm{var}(X_1 + X_2 + \cdots + X_n) = \mathrm{var}(X_1) + \mathrm{var}(X_2) + \cdots + \mathrm{var}(X_n).
\tag{5.8}
\end{equation}\]</span></p>
<p><strong>Note</strong>. Independence is sufficient for this result to hold but it is not necessary. Taking <span class="math inline">\(n=2\)</span> as an example, in generality we have
<span class="math display">\[ \mathrm{var}(X_1 + X_2) = \mathrm{var}(X_1) + \mathrm{var}(X_2) + 2\,\mathrm{cov}(X_1,X_2), \]</span>
where <span class="math inline">\(\mathrm{cov}(X_1,X_2)\)</span> is the <strong>covariance</strong> between the random variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>. Covariance is a measure of the strength of <strong>linear</strong> association. If <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent (have no association of any kind) then <span class="math inline">\(\mathrm{cov}(X_1,X_2)=0\)</span>, because they have no linear association. However, it is possible for <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> to be dependent but <span class="math inline">\(\mathrm{cov}(X_1,X_2)=0\)</span>, because, although they have some kind of association, they have no <strong>linear</strong> association. Thus, independence is a stronger requirement than zero covariance.</p>
<p>Returing to general <span class="math inline">\(n\)</span> we have
<span class="math display">\[ \mathrm{var}(X_1 + X_2 + \cdots + X_n) = \mathrm{var}(X_1) + \mathrm{var}(X_2) + \cdots + \mathrm{var}(X_n) + 2 \mathop{\sum\sum}_{i &lt; j} \mathrm{cov}(X_i,X_j). \]</span>
If <span class="math inline">\(\mathrm{cov}(X_i,X_j)=0\)</span> for all <span class="math inline">\(i &lt; j\)</span> then equation <a href="rvs.html#eq:varsum">(5.8)</a> holds. We will study covariance, and its standardised form <strong>correlation</strong>, in Chapter <a href="correlationchapter.html#correlationchapter">10</a>.</p>
</div>
</div>
<div id="locations" class="section level2 hasAnchor" number="5.5">
<h2><span class="header-section-number">5.5</span> Other measures of location<a href="rvs.html#locations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="the-median-of-a-random-variable" class="section level3 hasAnchor" number="5.5.1">
<h3><span class="header-section-number">5.5.1</span> The median of a random variable<a href="rvs.html#the-median-of-a-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Recall that the sample median of a set of observations is the middle observation when the observations are arranged in order of size. We define the median of a random variable <span class="math inline">\(X\)</span> as a value, median(<span class="math inline">\(X\)</span>), such that</p>
<p><span class="math display">\[ P(X &lt; \mathrm{median}(X)) \leq \frac12 \leq P(X \leq \mathrm{median}(X)). \]</span></p>
<p>In other words, <span class="math inline">\(\mathrm{median}(X)\)</span> is a value where a plot of the c.d.f. <span class="math inline">\(F_X(x)=P(X \leq x)\)</span> hits <span class="math inline">\(1/2\)</span>.</p>
<p>For a continuous random variable <span class="math inline">\(X\)</span> we have
<span class="math display">\[  F_X(\mathrm{median}(X)) = P(X \leq \mathrm{median}(X)) =\frac12. \]</span>
and a median will divide the distribution into two parts, each with probability 1/2:
<span class="math display">\[ P(X &lt; \mathrm{median}(X)) = P(X &gt; \mathrm{median}(X)) = \frac12. \]</span></p>
<p>This will not necessarily hold for a discrete distribution. For example, suppose that
<span class="math display">\[ P(X=0)=\frac16, \qquad  P(X=1)=\frac12, \qquad P(X=2)=\frac13. \]</span></p>
<p>Then
<span class="math display">\[\begin{eqnarray*}
F_X(x) = P(X \leq x) = \left\{\begin{array}{ll}
0 &amp; \mbox{for } x &lt;0, \\
\frac16 &amp; \mbox{for } 0 \leq x &lt; 1, \\
\frac23 &amp; \mbox{for } 1 \leq x &lt; 2, \\
1 &amp; \mbox{for } x \geq 2,
\end{array}\right.
\end{eqnarray*}\]</span>
Therefore, <span class="math inline">\(\mathrm{median}(X) = 1\)</span>. However, <span class="math inline">\(P(X&lt;1)=\frac16\)</span> and <span class="math inline">\(P(X&gt;1)=\frac13\)</span>.</p>
</div>
<div id="the-mode-of-a-random-variable" class="section level3 hasAnchor" number="5.5.2">
<h3><span class="header-section-number">5.5.2</span> The mode of a random variable<a href="rvs.html#the-mode-of-a-random-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Recall that the sample mode of categorical or discrete data is the value (or values) which occurs most often. We define the mode, mode(<span class="math inline">\(X\)</span>), of a random variable as follows.</p>
<p>For a discrete random variable <span class="math inline">\(X\)</span>, the mode is the value which has the highest probability of occurring: <span class="math inline">\(P(X=\mathrm{mode}(X))\)</span> will be larger than for any other value <span class="math inline">\(X\)</span> can have. In other words, <span class="math inline">\(\mathrm{mode}(X)\)</span> is the value at which the p.m.f. is maximised.</p>
<p>For a continuous random variable <span class="math inline">\(X\)</span>, the mode is the value at which the p.d.f. is maximised. <strong>If the maximum occurs at a turning point of <span class="math inline">\(f_X(x)\)</span></strong> then it can be found by solving the equation
<span class="math display">\[ \frac{\mathrm{d}}{\mathrm{d}x} f_X(x)  = 0, \]</span>
and checking that you have indeed found a maximum.</p>
</div>
</div>
<div id="quantiles" class="section level2 hasAnchor" number="5.6">
<h2><span class="header-section-number">5.6</span> Quantiles<a href="rvs.html#quantiles" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To keep things simple we consider a <strong>continuous</strong> random variable <span class="math inline">\(X\)</span>. For <span class="math inline">\(0 &lt; p &lt; 1\)</span>, a <span class="math inline">\(100p\%\)</span> quantile of <span class="math inline">\(X\)</span> is defined to be a value <span class="math inline">\(x_p\)</span> such that
<span class="math display">\[ F_X(x_p)=P(X \leq x_p) = p. \]</span>
Another way to express this is to say that <span class="math inline">\(x_p\)</span> is <span class="math inline">\(F_X^{-1}(p)\)</span>, where <span class="math inline">\(F_X^{-1}\)</span> is the inverse c.d.f. of <span class="math inline">\(X\)</span>. The inverse c.d.f. <span class="math inline">\(F_X^{-1}\)</span> is also called the quantile function <span class="math inline">\(Q\)</span> of <span class="math inline">\(X\)</span>, so we could write <span class="math inline">\(x_p = Q(p)\)</span>.</p>
<p>Thus, <span class="math inline">\(x_{1/4}=F_X^{-1}(1/4)\)</span> is the lower quartile of <span class="math inline">\(X\)</span>, <span class="math inline">\(x_{1/2}=F_X^{-1}(1/2)\)</span> is the median of <span class="math inline">\(X\)</span> and <span class="math inline">\(x_{3/4}=F_X^{-1}(3/4)\)</span> is the upper quartile of <span class="math inline">\(X\)</span>.</p>
<p>The inter-quartile range is <span class="math inline">\(x_{3/4}-x_{1/4}=F_X^{-1}(3/4)-F_X^{-1}(1/4)\)</span>, which is a measure of spread.</p>
</div>
<div id="measures-of-shape" class="section level2 hasAnchor" number="5.7">
<h2><span class="header-section-number">5.7</span> Measures of shape<a href="rvs.html#measures-of-shape" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The <strong>moment coefficient of skewness</strong> of a random variable <span class="math inline">\(X\)</span> with mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span> is given by
<span class="math display">\[ \mathrm{E}\left[\left(\frac{X - \mu}{\sigma}\right)^3\right] = \displaystyle\frac{\mathrm{E}\left[\left(X- \mu\right)^3\right]}{\sigma^3}, \]</span>
provided that <span class="math inline">\(\mathrm{E}[\left(X- \mu\right)^3]\)</span> exists.</p>
<p>The <strong>quartile skewness</strong> of a random variable <span class="math inline">\(X\)</span> with c.d.f <span class="math inline">\(F_X(x)\)</span> is given by
<span class="math display">\[ \frac{(x_{3/4}-x_{1/2}) - (x_{1/2}-x_{1/4})}{x_{3/4}-x_{1/4}} = \frac{[F^{-1}_X(3/4) - F^{-1}_X(1/2)] - [F^{-1}_X(1/2) - F^{-1}_X(1/4)]}{F^{-1}_X(3/4) - F^{-1}_X(1/4)}. \]</span></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="more-probability.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="simple.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["stat0002book.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
