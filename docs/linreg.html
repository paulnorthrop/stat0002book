<!DOCTYPE html>
<html lang="en-gb" xml:lang="en-gb">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Linear regression | STAT0002 Introduction to Probability and Statistics</title>
  <meta name="description" content="Produces STAT0002 notes in an accessible format" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Linear regression | STAT0002 Introduction to Probability and Statistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Produces STAT0002 notes in an accessible format" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Linear regression | STAT0002 Introduction to Probability and Statistics" />
  
  <meta name="twitter:description" content="Produces STAT0002 notes in an accessible format" />
  

<meta name="author" content="Dr Paul Northrop" />


<meta name="date" content="2021-03-29" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="contingency.html"/>
<link rel="next" href="correlationchapter.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">STAT0002 2020-21</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>The purpose of these notes</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#real"><i class="fa fa-check"></i><b>1.1</b> Real statistical investigations</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#shuttle"><i class="fa fa-check"></i><b>1.2</b> Challenger Space Shuttle Catastrophe</a><ul>
<li class="chapter" data-level="1.2.1" data-path="introduction.html"><a href="introduction.html#uncertainty"><i class="fa fa-check"></i><b>1.2.1</b> Uncertainty</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#a-very-brief-introduction-to-stochastic-simulation"><i class="fa fa-check"></i><b>1.3</b> A very brief introduction to stochastic simulation</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="descriptive.html"><a href="descriptive.html"><i class="fa fa-check"></i><b>2</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="2.1" data-path="descriptive.html"><a href="descriptive.html#types-of-data"><i class="fa fa-check"></i><b>2.1</b> Types of data</a><ul>
<li class="chapter" data-level="2.1.1" data-path="descriptive.html"><a href="descriptive.html#qualitative-or-categorical-data"><i class="fa fa-check"></i><b>2.1.1</b> Qualitative or categorical data</a></li>
<li class="chapter" data-level="2.1.2" data-path="descriptive.html"><a href="descriptive.html#quantitative-or-numerical-data"><i class="fa fa-check"></i><b>2.1.2</b> Quantitative or numerical data</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="descriptive.html"><a href="descriptive.html#describing-distributions"><i class="fa fa-check"></i><b>2.2</b> Describing distributions</a><ul>
<li class="chapter" data-level="" data-path="descriptive.html"><a href="descriptive.html#example-oxford-births-data"><i class="fa fa-check"></i>Example: Oxford births data</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="descriptive.html"><a href="descriptive.html#summary-statistics"><i class="fa fa-check"></i><b>2.3</b> Summary Statistics</a><ul>
<li class="chapter" data-level="2.3.1" data-path="descriptive.html"><a href="descriptive.html#fivenumber"><i class="fa fa-check"></i><b>2.3.1</b> Five number summary</a></li>
<li class="chapter" data-level="2.3.2" data-path="descriptive.html"><a href="descriptive.html#meanstdev"><i class="fa fa-check"></i><b>2.3.2</b> Mean and standard deviation</a></li>
<li class="chapter" data-level="2.3.3" data-path="descriptive.html"><a href="descriptive.html#mode"><i class="fa fa-check"></i><b>2.3.3</b> Mode</a></li>
<li class="chapter" data-level="2.3.4" data-path="descriptive.html"><a href="descriptive.html#symmetry"><i class="fa fa-check"></i><b>2.3.4</b> Symmetry</a></li>
<li class="chapter" data-level="2.3.5" data-path="descriptive.html"><a href="descriptive.html#corr1"><i class="fa fa-check"></i><b>2.3.5</b> Correlation</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="descriptive.html"><a href="descriptive.html#tables"><i class="fa fa-check"></i><b>2.4</b> Tables</a><ul>
<li class="chapter" data-level="2.4.1" data-path="descriptive.html"><a href="descriptive.html#frequency-distribution"><i class="fa fa-check"></i><b>2.4.1</b> Frequency distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="descriptive.html"><a href="descriptive.html#graphs"><i class="fa fa-check"></i><b>2.5</b> Graphs (1 variable)</a><ul>
<li class="chapter" data-level="2.5.1" data-path="descriptive.html"><a href="descriptive.html#histogram"><i class="fa fa-check"></i><b>2.5.1</b> Histograms</a></li>
<li class="chapter" data-level="2.5.2" data-path="descriptive.html"><a href="descriptive.html#stem"><i class="fa fa-check"></i><b>2.5.2</b> Stem-and-leaf plots</a></li>
<li class="chapter" data-level="2.5.3" data-path="descriptive.html"><a href="descriptive.html#dotplots"><i class="fa fa-check"></i><b>2.5.3</b> Dotplots</a></li>
<li class="chapter" data-level="2.5.4" data-path="descriptive.html"><a href="descriptive.html#boxplots"><i class="fa fa-check"></i><b>2.5.4</b> Boxplots</a></li>
<li class="chapter" data-level="2.5.5" data-path="descriptive.html"><a href="descriptive.html#barplots"><i class="fa fa-check"></i><b>2.5.5</b> Barplots</a></li>
<li class="chapter" data-level="2.5.6" data-path="descriptive.html"><a href="descriptive.html#times-series-plots"><i class="fa fa-check"></i><b>2.5.6</b> Times series plots</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="descriptive.html"><a href="descriptive.html#election"><i class="fa fa-check"></i><b>2.6</b> 2000 US Presidential Election</a></li>
<li class="chapter" data-level="2.7" data-path="descriptive.html"><a href="descriptive.html#graphs2"><i class="fa fa-check"></i><b>2.7</b> Graphs (2 variables)</a><ul>
<li class="chapter" data-level="2.7.1" data-path="descriptive.html"><a href="descriptive.html#scatter-plots"><i class="fa fa-check"></i><b>2.7.1</b> Scatter plots</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="descriptive.html"><a href="descriptive.html#transformation"><i class="fa fa-check"></i><b>2.8</b> Transformation of data</a><ul>
<li class="chapter" data-level="2.8.1" data-path="descriptive.html"><a href="descriptive.html#transsymmetry"><i class="fa fa-check"></i><b>2.8.1</b> Transformation to approximate symmetry</a></li>
<li class="chapter" data-level="2.8.2" data-path="descriptive.html"><a href="descriptive.html#straighten"><i class="fa fa-check"></i><b>2.8.2</b> Straightening scatter plots</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>3</b> Probability</a><ul>
<li class="chapter" data-level="3.1" data-path="probability.html"><a href="probability.html#sids"><i class="fa fa-check"></i><b>3.1</b> Misleading statistical evidence in cot death trials</a></li>
<li class="chapter" data-level="3.2" data-path="probability.html"><a href="probability.html#relative-frequency-definition-of-probability"><i class="fa fa-check"></i><b>3.2</b> Relative frequency definition of probability</a></li>
<li class="chapter" data-level="3.3" data-path="probability.html"><a href="probability.html#basic-properties-of-probability"><i class="fa fa-check"></i><b>3.3</b> Basic properties of probability</a></li>
<li class="chapter" data-level="3.4" data-path="probability.html"><a href="probability.html#conditional-probability"><i class="fa fa-check"></i><b>3.4</b> Conditional probability</a></li>
<li class="chapter" data-level="3.5" data-path="probability.html"><a href="probability.html#addition-rule-of-probability"><i class="fa fa-check"></i><b>3.5</b> Addition rule of probability</a><ul>
<li class="chapter" data-level="3.5.1" data-path="probability.html"><a href="probability.html#mutually-exclusive-events"><i class="fa fa-check"></i><b>3.5.1</b> Mutually exclusive events</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="probability.html"><a href="probability.html#multrule"><i class="fa fa-check"></i><b>3.6</b> Multiplication rule of probability</a></li>
<li class="chapter" data-level="3.7" data-path="probability.html"><a href="probability.html#indepevents"><i class="fa fa-check"></i><b>3.7</b> Independence of events</a><ul>
<li class="chapter" data-level="3.7.1" data-path="probability.html"><a href="probability.html#bloodindep"><i class="fa fa-check"></i><b>3.7.1</b> An example of independence</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="probability.html"><a href="probability.html#law-of-total-probability"><i class="fa fa-check"></i><b>3.8</b> Law of total probability</a></li>
<li class="chapter" data-level="3.9" data-path="probability.html"><a href="probability.html#bayes-theorem"><i class="fa fa-check"></i><b>3.9</b> Bayes’ theorem</a></li>
<li class="chapter" data-level="3.10" data-path="probability.html"><a href="probability.html#dna-identification-evidence"><i class="fa fa-check"></i><b>3.10</b> DNA identification evidence</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="rvs.html"><a href="rvs.html"><i class="fa fa-check"></i><b>4</b> Random variables</a><ul>
<li class="chapter" data-level="4.1" data-path="rvs.html"><a href="rvs.html#discrete"><i class="fa fa-check"></i><b>4.1</b> Discrete random variables</a></li>
<li class="chapter" data-level="4.2" data-path="rvs.html"><a href="rvs.html#continuous"><i class="fa fa-check"></i><b>4.2</b> Continuous random variables</a></li>
<li class="chapter" data-level="4.3" data-path="rvs.html"><a href="rvs.html#expectation"><i class="fa fa-check"></i><b>4.3</b> Expectation</a><ul>
<li class="chapter" data-level="4.3.1" data-path="rvs.html"><a href="rvs.html#expectation-of-a-discrete-random-variable"><i class="fa fa-check"></i><b>4.3.1</b> Expectation of a discrete random variable</a></li>
<li class="chapter" data-level="4.3.2" data-path="rvs.html"><a href="rvs.html#expectation-of-a-continuous-random-variable"><i class="fa fa-check"></i><b>4.3.2</b> Expectation of a continuous random variable</a></li>
<li class="chapter" data-level="4.3.3" data-path="rvs.html"><a href="rvs.html#properties-of-mathrmex"><i class="fa fa-check"></i><b>4.3.3</b> Properties of <span class="math inline">\(\mathrm{E}(X)\)</span></a></li>
<li class="chapter" data-level="4.3.4" data-path="rvs.html"><a href="rvs.html#EgX"><i class="fa fa-check"></i><b>4.3.4</b> The expectation of <span class="math inline">\(g(X)\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="rvs.html"><a href="rvs.html#variance"><i class="fa fa-check"></i><b>4.4</b> Variance</a><ul>
<li class="chapter" data-level="4.4.1" data-path="rvs.html"><a href="rvs.html#variance-of-a-discrete-random-variable"><i class="fa fa-check"></i><b>4.4.1</b> Variance of a discrete random variable</a></li>
<li class="chapter" data-level="4.4.2" data-path="rvs.html"><a href="rvs.html#variance-of-a-continuous-random-variable"><i class="fa fa-check"></i><b>4.4.2</b> Variance of a continuous random variable</a></li>
<li class="chapter" data-level="4.4.3" data-path="rvs.html"><a href="rvs.html#variance-and-standard-deviation"><i class="fa fa-check"></i><b>4.4.3</b> Variance and standard deviation</a></li>
<li class="chapter" data-level="4.4.4" data-path="rvs.html"><a href="rvs.html#properties-of-mathrmvarx"><i class="fa fa-check"></i><b>4.4.4</b> Properties of <span class="math inline">\(\mathrm{var}(X)\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="rvs.html"><a href="rvs.html#locations"><i class="fa fa-check"></i><b>4.5</b> Other measures of location</a><ul>
<li class="chapter" data-level="4.5.1" data-path="rvs.html"><a href="rvs.html#the-median-of-a-random-variable"><i class="fa fa-check"></i><b>4.5.1</b> The median of a random variable</a></li>
<li class="chapter" data-level="4.5.2" data-path="rvs.html"><a href="rvs.html#the-mode-of-a-random-variable"><i class="fa fa-check"></i><b>4.5.2</b> The mode of a random variable</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="rvs.html"><a href="rvs.html#quantiles"><i class="fa fa-check"></i><b>4.6</b> Quantiles</a></li>
<li class="chapter" data-level="4.7" data-path="rvs.html"><a href="rvs.html#measures-of-shape"><i class="fa fa-check"></i><b>4.7</b> Measures of shape</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="simple.html"><a href="simple.html"><i class="fa fa-check"></i><b>5</b> Simple distributions</a><ul>
<li class="chapter" data-level="5.1" data-path="simple.html"><a href="simple.html#australian-births-data"><i class="fa fa-check"></i><b>5.1</b> Australian births data</a></li>
<li class="chapter" data-level="5.2" data-path="simple.html"><a href="simple.html#the-bernoulli-distribution"><i class="fa fa-check"></i><b>5.2</b> The Bernoulli distribution</a><ul>
<li class="chapter" data-level="5.2.1" data-path="simple.html"><a href="simple.html#summary-of-the-bernoullip-distribution"><i class="fa fa-check"></i><b>5.2.1</b> Summary of the Bernoulli(<span class="math inline">\(p\)</span>) distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="simple.html"><a href="simple.html#binomial"><i class="fa fa-check"></i><b>5.3</b> The binomial distribution</a><ul>
<li class="chapter" data-level="5.3.1" data-path="simple.html"><a href="simple.html#binominf"><i class="fa fa-check"></i><b>5.3.1</b> A brief look at statistical inference about <span class="math inline">\(p\)</span></a></li>
<li class="chapter" data-level="5.3.2" data-path="simple.html"><a href="simple.html#summary-of-the-binomialnp-distribution"><i class="fa fa-check"></i><b>5.3.2</b> Summary of the binomial(<span class="math inline">\(n,p\)</span>) distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="simple.html"><a href="simple.html#the-geometric-distribution"><i class="fa fa-check"></i><b>5.4</b> The geometric distribution</a><ul>
<li class="chapter" data-level="5.4.1" data-path="simple.html"><a href="simple.html#summary-of-the-geometricp-distribution"><i class="fa fa-check"></i><b>5.4.1</b> Summary of the geometric(<span class="math inline">\(p\)</span>) distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="simple.html"><a href="simple.html#Poisson"><i class="fa fa-check"></i><b>5.5</b> The Poisson distribution</a><ul>
<li class="chapter" data-level="5.5.1" data-path="simple.html"><a href="simple.html#summary-of-the-poissonmu-distribution"><i class="fa fa-check"></i><b>5.5.1</b> Summary of the Poisson(<span class="math inline">\(\mu\)</span>) distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="simple.html"><a href="simple.html#summary-of-these-discrete-distributions"><i class="fa fa-check"></i><b>5.6</b> Summary of these discrete distributions</a></li>
<li class="chapter" data-level="5.7" data-path="simple.html"><a href="simple.html#uniform"><i class="fa fa-check"></i><b>5.7</b> The uniform distribution</a><ul>
<li class="chapter" data-level="5.7.1" data-path="simple.html"><a href="simple.html#summary-of-the-uniformab-distribution"><i class="fa fa-check"></i><b>5.7.1</b> Summary of the uniform(<span class="math inline">\(a,b\)</span>) distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="simple.html"><a href="simple.html#exponential"><i class="fa fa-check"></i><b>5.8</b> The exponential distribution</a><ul>
<li class="chapter" data-level="5.8.1" data-path="simple.html"><a href="simple.html#summary-of-the-exponentiallambda-distribution"><i class="fa fa-check"></i><b>5.8.1</b> Summary of the exponential(<span class="math inline">\(\lambda\)</span>) distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="simple.html"><a href="simple.html#normal"><i class="fa fa-check"></i><b>5.9</b> The normal distribution</a><ul>
<li class="chapter" data-level="5.9.1" data-path="simple.html"><a href="simple.html#summary-of-the-mboxnmusigma2-distribution"><i class="fa fa-check"></i><b>5.9.1</b> Summary of the <span class="math inline">\(\mbox{N}(\mu,\sigma^2)\)</span> distribution</a></li>
<li class="chapter" data-level="5.9.2" data-path="simple.html"><a href="simple.html#the-standard-normal-disribution"><i class="fa fa-check"></i><b>5.9.2</b> The standard normal disribution</a></li>
<li class="chapter" data-level="5.9.3" data-path="simple.html"><a href="simple.html#evaluating-the-normal-c.d.f.-and-quantiles"><i class="fa fa-check"></i><b>5.9.3</b> Evaluating the normal c.d.f. and quantiles</a></li>
<li class="chapter" data-level="5.9.4" data-path="simple.html"><a href="simple.html#interpretation-of-sigma"><i class="fa fa-check"></i><b>5.9.4</b> Interpretation of <span class="math inline">\(\sigma\)</span></a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="simple.html"><a href="simple.html#summary-of-these-continuous-distributions"><i class="fa fa-check"></i><b>5.10</b> Summary of these continuous distributions</a></li>
<li class="chapter" data-level="5.11" data-path="simple.html"><a href="simple.html#qq"><i class="fa fa-check"></i><b>5.11</b> QQ plots</a><ul>
<li class="chapter" data-level="5.11.1" data-path="simple.html"><a href="simple.html#normal-qq-plots"><i class="fa fa-check"></i><b>5.11.1</b> Normal QQ plots</a></li>
<li class="chapter" data-level="5.11.2" data-path="simple.html"><a href="simple.html#uniform-qq-plots"><i class="fa fa-check"></i><b>5.11.2</b> Uniform QQ plots</a></li>
<li class="chapter" data-level="5.11.3" data-path="simple.html"><a href="simple.html#exponential-qq-plots"><i class="fa fa-check"></i><b>5.11.3</b> Exponential QQ plots</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>6</b> Statistical Inference</a><ul>
<li class="chapter" data-level="6.1" data-path="inference.html"><a href="inference.html#the-story-so-far"><i class="fa fa-check"></i><b>6.1</b> The story so far</a></li>
<li class="chapter" data-level="6.2" data-path="inference.html"><a href="inference.html#sample-and-populations"><i class="fa fa-check"></i><b>6.2</b> Sample and populations</a></li>
<li class="chapter" data-level="6.3" data-path="inference.html"><a href="inference.html#probmodels"><i class="fa fa-check"></i><b>6.3</b> Probability models</a></li>
<li class="chapter" data-level="6.4" data-path="inference.html"><a href="inference.html#fitting-models"><i class="fa fa-check"></i><b>6.4</b> Fitting models</a></li>
<li class="chapter" data-level="6.5" data-path="inference.html"><a href="inference.html#uncertainty-in-estimation"><i class="fa fa-check"></i><b>6.5</b> Uncertainty in estimation</a><ul>
<li class="chapter" data-level="6.5.1" data-path="inference.html"><a href="inference.html#simulation-coin-tossing-example"><i class="fa fa-check"></i><b>6.5.1</b> Simulation: coin-tossing example</a></li>
<li class="chapter" data-level="6.5.2" data-path="inference.html"><a href="inference.html#simnorm"><i class="fa fa-check"></i><b>6.5.2</b> Simulation: estimating the parameters of a normal distribution</a></li>
<li class="chapter" data-level="6.5.3" data-path="inference.html"><a href="inference.html#simexp"><i class="fa fa-check"></i><b>6.5.3</b> Simulation: estimating the parameters of an exponential distribution</a></li>
<li class="chapter" data-level="6.5.4" data-path="inference.html"><a href="inference.html#central-limit-theorem"><i class="fa fa-check"></i><b>6.5.4</b> Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="inference.html"><a href="inference.html#good"><i class="fa fa-check"></i><b>6.6</b> Properties of estimators</a><ul>
<li class="chapter" data-level="6.6.1" data-path="inference.html"><a href="inference.html#bias"><i class="fa fa-check"></i><b>6.6.1</b> Bias</a></li>
<li class="chapter" data-level="6.6.2" data-path="inference.html"><a href="inference.html#varianceofestimator"><i class="fa fa-check"></i><b>6.6.2</b> Variance</a></li>
<li class="chapter" data-level="6.6.3" data-path="inference.html"><a href="inference.html#mean-squared-error-mse"><i class="fa fa-check"></i><b>6.6.3</b> Mean squared error (MSE)</a></li>
<li class="chapter" data-level="6.6.4" data-path="inference.html"><a href="inference.html#standard-error"><i class="fa fa-check"></i><b>6.6.4</b> Standard error</a></li>
<li class="chapter" data-level="6.6.5" data-path="inference.html"><a href="inference.html#consistency"><i class="fa fa-check"></i><b>6.6.5</b> Consistency</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="inference.html"><a href="inference.html#assessing-goodness-of-fit"><i class="fa fa-check"></i><b>6.7</b> Assessing goodness-of-fit</a><ul>
<li class="chapter" data-level="6.7.1" data-path="inference.html"><a href="inference.html#residuals"><i class="fa fa-check"></i><b>6.7.1</b> Residuals</a></li>
<li class="chapter" data-level="6.7.2" data-path="inference.html"><a href="inference.html#standardised-residuals"><i class="fa fa-check"></i><b>6.7.2</b> Standardised residuals</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="contingency.html"><a href="contingency.html"><i class="fa fa-check"></i><b>7</b> Contingency tables</a><ul>
<li class="chapter" data-level="7.1" data-path="contingency.html"><a href="contingency.html#way2"><i class="fa fa-check"></i><b>7.1</b> 2-way contingency tables</a><ul>
<li class="chapter" data-level="7.1.1" data-path="contingency.html"><a href="contingency.html#indep"><i class="fa fa-check"></i><b>7.1.1</b> Independence</a></li>
<li class="chapter" data-level="7.1.2" data-path="contingency.html"><a href="contingency.html#compprob"><i class="fa fa-check"></i><b>7.1.2</b> Comparing probabilities</a></li>
<li class="chapter" data-level="7.1.3" data-path="contingency.html"><a href="contingency.html#measures"><i class="fa fa-check"></i><b>7.1.3</b> Measures of association</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="contingency.html"><a href="contingency.html#way3"><i class="fa fa-check"></i><b>7.2</b> 3-way contingency tables</a><ul>
<li class="chapter" data-level="7.2.1" data-path="contingency.html"><a href="contingency.html#mutual-independence"><i class="fa fa-check"></i><b>7.2.1</b> Mutual independence</a></li>
<li class="chapter" data-level="7.2.2" data-path="contingency.html"><a href="contingency.html#marginal-independence"><i class="fa fa-check"></i><b>7.2.2</b> Marginal independence</a></li>
<li class="chapter" data-level="7.2.3" data-path="contingency.html"><a href="contingency.html#conditional-independence"><i class="fa fa-check"></i><b>7.2.3</b> Conditional independence</a></li>
<li class="chapter" data-level="7.2.4" data-path="contingency.html"><a href="contingency.html#confounding-variables"><i class="fa fa-check"></i><b>7.2.4</b> Confounding variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="linreg.html"><a href="linreg.html"><i class="fa fa-check"></i><b>8</b> Linear regression</a><ul>
<li class="chapter" data-level="8.1" data-path="linreg.html"><a href="linreg.html#simple-linear-regression"><i class="fa fa-check"></i><b>8.1</b> Simple linear regression</a><ul>
<li class="chapter" data-level="8.1.1" data-path="linreg.html"><a href="linreg.html#simple-linear-regression-model"><i class="fa fa-check"></i><b>8.1.1</b> Simple linear regression model</a></li>
<li class="chapter" data-level="8.1.2" data-path="linreg.html"><a href="linreg.html#least-squares-estimation-of-alpha-and-beta"><i class="fa fa-check"></i><b>8.1.2</b> Least squares estimation of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span></a></li>
<li class="chapter" data-level="8.1.3" data-path="linreg.html"><a href="linreg.html#least-squares-fitting-to-hubbles-data"><i class="fa fa-check"></i><b>8.1.3</b> Least squares fitting to Hubble’s data</a></li>
<li class="chapter" data-level="8.1.4" data-path="linreg.html"><a href="linreg.html#normal-linear-regression-model"><i class="fa fa-check"></i><b>8.1.4</b> Normal linear regression model</a></li>
<li class="chapter" data-level="8.1.5" data-path="linreg.html"><a href="linreg.html#lmsummary"><i class="fa fa-check"></i><b>8.1.5</b> Summary of the assumptions of a (normal) linear regression model</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="linreg.html"><a href="linreg.html#looking"><i class="fa fa-check"></i><b>8.2</b> Looking at scatter plots</a></li>
<li class="chapter" data-level="8.3" data-path="linreg.html"><a href="linreg.html#model-checking"><i class="fa fa-check"></i><b>8.3</b> Model checking</a><ul>
<li class="chapter" data-level="8.3.1" data-path="linreg.html"><a href="linreg.html#departures-from-assumptions"><i class="fa fa-check"></i><b>8.3.1</b> Departures from assumptions</a></li>
<li class="chapter" data-level="8.3.2" data-path="linreg.html"><a href="linreg.html#outliers"><i class="fa fa-check"></i><b>8.3.2</b> Outliers and influential observations</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="linreg.html"><a href="linreg.html#linregtrans"><i class="fa fa-check"></i><b>8.4</b> Use of transformations</a><ul>
<li class="chapter" data-level="8.4.1" data-path="linreg.html"><a href="linreg.html#interpretation-after-transformation"><i class="fa fa-check"></i><b>8.4.1</b> Interpretation after transformation</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="linreg.html"><a href="linreg.html#over-fitting"><i class="fa fa-check"></i><b>8.5</b> Over-fitting</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="correlationchapter.html"><a href="correlationchapter.html"><i class="fa fa-check"></i><b>9</b> Correlation</a><ul>
<li class="chapter" data-level="9.1" data-path="correlationchapter.html"><a href="correlationchapter.html#correlation-a-measure-of-linear-association"><i class="fa fa-check"></i><b>9.1</b> Correlation: a measure of linear association</a></li>
<li class="chapter" data-level="9.2" data-path="correlationchapter.html"><a href="correlationchapter.html#covariance-and-correlation"><i class="fa fa-check"></i><b>9.2</b> Covariance and correlation</a><ul>
<li class="chapter" data-level="9.2.1" data-path="correlationchapter.html"><a href="correlationchapter.html#estimation"><i class="fa fa-check"></i><b>9.2.1</b> Estimation</a></li>
<li class="chapter" data-level="9.2.2" data-path="correlationchapter.html"><a href="correlationchapter.html#links-between-regression-and-correlation"><i class="fa fa-check"></i><b>9.2.2</b> Links between regression and correlation</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="correlationchapter.html"><a href="correlationchapter.html#use-and-misuse-of-correlation"><i class="fa fa-check"></i><b>9.3</b> Use and misuse of correlation</a><ul>
<li class="chapter" data-level="9.3.1" data-path="correlationchapter.html"><a href="correlationchapter.html#do-not-use-correlation-for-regression-sampling-schemes"><i class="fa fa-check"></i><b>9.3.1</b> Do not use correlation for regression sampling schemes</a></li>
<li class="chapter" data-level="9.3.2" data-path="correlationchapter.html"><a href="correlationchapter.html#correxamples"><i class="fa fa-check"></i><b>9.3.2</b> Examples of correlations of different strengths</a></li>
<li class="chapter" data-level="9.3.3" data-path="correlationchapter.html"><a href="correlationchapter.html#beware-missing-data-codes"><i class="fa fa-check"></i><b>9.3.3</b> Beware missing data codes</a></li>
<li class="chapter" data-level="9.3.4" data-path="correlationchapter.html"><a href="correlationchapter.html#more-guessing-sample-correlations"><i class="fa fa-check"></i><b>9.3.4</b> More guessing sample correlations</a></li>
<li class="chapter" data-level="9.3.5" data-path="correlationchapter.html"><a href="correlationchapter.html#summary"><i class="fa fa-check"></i><b>9.3.5</b> Summary</a></li>
<li class="chapter" data-level="9.3.6" data-path="correlationchapter.html"><a href="correlationchapter.html#anscombes-datasets"><i class="fa fa-check"></i><b>9.3.6</b> Anscombe’s datasets</a></li>
<li class="chapter" data-level="9.3.7" data-path="correlationchapter.html"><a href="correlationchapter.html#we-must-interpret-correlation-with-care."><i class="fa fa-check"></i><b>9.3.7</b> We must interpret correlation with care.</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="a-general-strategy-for-statistical-modelling.html"><a href="a-general-strategy-for-statistical-modelling.html"><i class="fa fa-check"></i><b>10</b> A general strategy for statistical modelling</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">STAT0002 Introduction to Probability and Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linreg" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Linear regression</h1>
<p>In this chapter, and Chapter <a href="correlationchapter.html#correlationchapter">9</a>, we examine the relationship between 2 continuous variables. First we consider <strong>regression</strong> problems, where the distribution of a <strong>response variable</strong> <span class="math inline">\(Y\)</span> is thought to be dependent on the value of an <strong>explanatory variable</strong> <span class="math inline">\(X\)</span>. Possible aims are (a) to understand the relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>, or (b) to predict <span class="math inline">\(Y\)</span> from the value of <span class="math inline">\(X\)</span>. We examine the conditional distribution of the random variable <span class="math inline">\(Y\)</span> given that <span class="math inline">\(X=x\)</span>, that is, <span class="math inline">\(Y \mid X=x\)</span>. In particular, we study a <strong>simple linear regression model</strong> in which the conditional mean <span class="math inline">\(\mbox{E}(Y \mid X=x)\)</span> of <span class="math inline">\(Y\)</span> given that <span class="math inline">\(X=x\)</span> is assumed to be a linear function of <span class="math inline">\(x\)</span> and the conditional variance <span class="math inline">\(\mbox{var}(Y \mid X=x)\)</span> of <span class="math inline">\(Y\)</span> is assumed to be constant. That is,
<span class="math display">\[ \mbox{E}(Y \mid X=x) = \alpha+\beta\,x \qquad \mbox{and} \qquad \mbox{var}(Y \mid X=x)=\sigma^2, \]</span>
for some constants <span class="math inline">\(\alpha, \beta\)</span> and <span class="math inline">\(\sigma^2\)</span>.</p>
<p>In many cases it is clear which variable should be the response variable <span class="math inline">\(Y\)</span> and which should be the explanatory variable <span class="math inline">\(X\)</span>. For example,</p>
<ul>
<li>If changes in <span class="math inline">\(x\)</span> cause changes in <span class="math inline">\(Y\)</span>, so that the direction of dependence is clear. For example, <span class="math inline">\(x\)</span>=river depth influencing <span class="math inline">\(Y\)</span>=flow rate.</li>
<li>If the values of <span class="math inline">\(X\)</span> are controlled by an experimenter and then the value of <span class="math inline">\(Y\)</span> is observed. For example, <span class="math inline">\(x\)</span>=dosage of drug and <span class="math inline">\(Y\)</span>=reduction in blood pressure. This is sometimes called <strong>regression sampling</strong>.</li>
<li>If we wish to predict <span class="math inline">\(Y\)</span> using <span class="math inline">\(x\)</span>. For example, <span class="math inline">\(x\)</span>=share value today and <span class="math inline">\(Y\)</span>=share value tomorrow.</li>
</ul>
<p>In a related, but different, problem the 2 random variables <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> are treated symmetrically. The question is how these random variables are associated. A measure of strength of <strong>linear</strong> association between 2 variables is given by a <strong>correlation coefficient</strong> (see Chapter <a href="correlationchapter.html#correlationchapter">9</a>).</p>
<p>Regression answers the question “How does the conditional distribution of the random variable <span class="math inline">\(Y\)</span> depend on the value <span class="math inline">\(x\)</span> of <span class="math inline">\(X\)</span>?”. Correlation answers the question “How strong is any <strong>linear</strong> association between the random variables <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>?”. In a regression problem we assume that the <span class="math inline">\(Y\)</span> values are random variables (that is, subject to random variability) but the <span class="math inline">\(x\)</span> values are not. When using a correlation coefficient we assume that both <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are random variables.</p>
<div id="simple-linear-regression" class="section level2">
<h2><span class="header-section-number">8.1</span> Simple linear regression</h2>
<p>We use a small set of data, and some physical theory, to estimate the age of the Universe! In 1929 the famous astronomer Edwin Hubble published a paper (<span class="citation">Hubble (<a href="#ref-Hubble1929" role="doc-biblioref">1929</a>)</span>) reporting a relationship he had observed (using a telescope) between the distance of a nebula (a star) from the Earth and the velocity (the <strong>recession velocity</strong>) with which it was moving away from the Earth. Hubble’s data are given in Table <a href="linreg.html#fig:tablehubble">8.1</a>. A scatter plot of distance against velocity is given in Figure <a href="linreg.html#fig:hubblescatter">8.2</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:tablehubble"></span>
<img src="images/hubbletable.PNG" alt="Hubble's data. 1 MPc = 1 megaparsec = $3.086 \times 10^{19}$ km. A megaparsec is a long distance: the distance from the Earth to the Sun is 'only' $1.5 \times 10^8$ km." width="75%" />
<p class="caption">
Figure 8.1: Hubble’s data. 1 MPc = 1 megaparsec = <span class="math inline">\(3.086 \times 10^{19}\)</span> km. A megaparsec is a long distance: the distance from the Earth to the Sun is ‘only’ <span class="math inline">\(1.5 \times 10^8\)</span> km.
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:hubblescatter"></span>
<img src="images/hubble_scatter.png" alt="Scatter plot of distance against recession velocity." width="75%" />
<p class="caption">
Figure 8.2: Scatter plot of distance against recession velocity.
</p>
</div>
<p>It appears that distance and velocity are <strong>positively associated</strong>: the values of distance tend to be larger for nebulae with large velocities than for nebulae with smaller velocities. Also, this relationship appears to be approximately linear, at least over the range of velocities available.</p>
<p>Scientists then wondered how the positive linear association between distance and velocity could have arisen. The result was ‘Big Bang’ theory. This theory proposes that the Universe started with a Big Bang at a single point in space a very long time ago, scattering material around the surface of an ever-expanding sphere. If Big Bang theory is correct then the relationship between distance (<span class="math inline">\(Y\)</span>) and recession velocity (<span class="math inline">\(X\)</span>) should be of the form
<span class="math display">\[ Y = T X, \]</span>
where <span class="math inline">\(T\)</span> is the age of the Universe when the observations were made. This is called Hubble’s Law. In other words, distance, <span class="math inline">\(Y\)</span>, should depend linearly on velocity, <span class="math inline">\(X\)</span>. <span class="math inline">\(H=1/T\)</span> is called <strong>Hubble’s constant</strong>.</p>
<p>The points in Figure <a href="linreg.html#fig:hubblescatter">8.2</a> do not lie exactly on a straight line, partly because the values of distance are not exact: they include measurement error. Also, there may have been astronomical events since the Big Bang which have weakened further the supposed linear relationships between distance and velocity. If we look at nebulae with the same value, <span class="math inline">\(x\)</span>, of velocity the measured value of distance, <span class="math inline">\(Y\)</span>, varies from one nebulae to another. For example, the 4 nebulae with velocities of 500 km/sec have have distances 0.9, 1.1, 1.4 and 2.0 MPc. So, for a given value of velocity there is variability in their distances from the Earth. Therefore, <span class="math inline">\(Y \mid X=x\)</span> is a random variable, with conditional mean <span class="math inline">\(\mbox{E}(Y \mid X=x)\)</span> and conditional variance <span class="math inline">\(\mbox{var}(Y \mid X=x)\)</span>.</p>
<p>In Figure <a href="linreg.html#fig:hubblescatter">8.2</a> it looks possible that there is a straight line relationship between <span class="math inline">\(\mbox{E}(Y \mid X=x)\)</span> and <span class="math inline">\(x\)</span>. Therefore we consider fitting a simple linear regression model of <span class="math inline">\(Y\)</span> on <span class="math inline">\(x\)</span>. You could think of this as a way to draw a ‘line of best fit’ through the points in Figure <a href="linreg.html#fig:hubblescatter">8.2</a>.</p>
<div id="simple-linear-regression-model" class="section level3">
<h3><span class="header-section-number">8.1.1</span> Simple linear regression model</h3>
<p>We assume that
<span class="math display" id="eq:regeqn">\[\begin{equation}
Y_i = \alpha + \beta\,x_i +  \epsilon_i, \qquad i=1,\ldots,n, 
\tag{8.1}
\end{equation}\]</span>
where <span class="math inline">\(\epsilon_i, i=1, \ldots, n\)</span> are error terms, representing random ‘noise’. The <span class="math inline">\(\alpha + \beta\,x_i\)</span> part of the model is the <strong>systematic</strong> part. The <span class="math inline">\(\epsilon_i\)</span> is the <strong>random</strong> part of the model. It is assumed that, for <span class="math inline">\(i = 1, ..., n\)</span>,
<span class="math display">\[  \mbox{E}( \epsilon_i)=0, \qquad \mbox{and} \qquad  \mbox{var}( \epsilon_i)=\sigma^2, \]</span>
and that <span class="math inline">\(\epsilon_1, \ldots, \epsilon_n\)</span> are <strong>uncorrelated</strong>. We will study <strong>correlation</strong> in the next section. It is a measure of the degree of <strong>linear</strong> association between two random variables. Uncorrelated random variables have no linear association.</p>
<p>Another way to write down this model is, for <span class="math inline">\(i=1,\ldots,n\)</span>,
<span class="math display">\[  \mbox{E}(Y_i \mid X=x_i) = \alpha+\beta\,x_i, \qquad\quad  (\mbox{straight line relationship}), \]</span>
and
<span class="math display">\[ \qquad  \mbox{var}(Y_i \mid X=x_i)=\sigma^2, \qquad\quad (\mbox{constant variance}), \]</span>
where, given the values <span class="math inline">\(x_1,\ldots,x_n\)</span>, the random variables <span class="math inline">\(Y_1, \ldots, Y_n\)</span> are uncorrelated.</p>
<p>Figure <a href="linreg.html#fig:regschematic">8.3</a> shows how the conditional distribution
of <span class="math inline">\(Y\)</span> is assumed to vary with the value of <span class="math inline">\(x\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:regschematic"></span>
<img src="images/regschematic.png" alt="Conditional distribution of $Y$ given $X=x$ for a linear regression model." width="75%" />
<p class="caption">
Figure 8.3: Conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X=x\)</span> for a linear regression model.
</p>
</div>
<p><strong>Interpretation of parameters</strong></p>
<ul>
<li>Intercept: <span class="math inline">\(\alpha\)</span>. The expected value (mean) of <span class="math inline">\(Y\)</span> when <span class="math inline">\(X=0\)</span>, that is, <span class="math inline">\(\mbox{E}(Y~|~X=0)\)</span>.</li>
<li>Gradient or slope: <span class="math inline">\(\beta\)</span>. The amount by which the mean of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X=x\)</span>, <span class="math inline">\(\mbox{E}(Y~|~X=x)\)</span>,
increases when <span class="math inline">\(x\)</span> is increased by 1 unit. That is,
<span class="math display">\[ \beta =  \mbox{E}(Y~|~X=x+1)- \mbox{E}(Y~|~X=x). \]</span></li>
<li>Error variance: <span class="math inline">\(\sigma^2\)</span>. The variability of the response about the linear regression line (in the vertical direction).</li>
</ul>
</div>
<div id="least-squares-estimation-of-alpha-and-beta" class="section level3">
<h3><span class="header-section-number">8.1.2</span> Least squares estimation of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span></h3>
<p>Suppose that we have paired data <span class="math inline">\((x_1,y_1), \ldots, (x_n,y_n)\)</span>. How can we fit a simple linear regression model to these data? Initially, our aim is to use estimators <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(\hat{\beta}\)</span> of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> to produce an estimated regression line
<span class="math display">\[ y= \hat{\alpha}+\hat{\beta}\,x. \]</span>
There are many possible estimators of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> that could be used. A standard approach, which produces estimators with some nice properties is <strong>least squares estimation</strong>. Firstly, we rearrange equation <a href="linreg.html#eq:regeqn">(8.1)</a> to define <strong>residuals</strong>
<span class="math display">\[ R_i = Y_i-(\hat{\alpha}+\hat{\beta}\,x_i) = Y_i-\hat{Y}_i, \qquad i=1,\ldots,n, \]</span>
the differences between the observed values <span class="math inline">\(Y_i, i = 1, \ldots, n\)</span> and the <strong>fitted values</strong> <span class="math inline">\(\hat{Y}_i=\hat{\alpha}+\hat{\beta}\,x_i\)</span>, <span class="math inline">\(i = 1, \ldots, n\)</span> given by the estimated regression line.</p>
<p>The least squares estimators have the property that they minimise the sum of squared residuals:
<span class="math display">\[ \sum_{i=1}^n \left(Y_i-\hat{\alpha}-\hat{\beta}\,x_i\right)^2. \]</span></p>
<p>It is possible to do this by hand to give
<span class="math display">\[\begin{equation}
\hat{\beta}=\frac{\displaystyle\sum_{i=1}^n \left(x_i- \overline{x}\right)\,\left(Y_i- \overline{Y}\right)}
{\displaystyle\sum_{i=1}^n \left(x_i- \overline{x}\right)^2} = \frac{C_{xY}}{C_{xx}} \qquad
\mbox{and} \qquad  
\hat{\alpha}=  \overline{Y} - \hat{\beta}\, \overline{x}, 
\end{equation}\]</span>
where <span class="math inline">\(\overline{Y} = (1/n)\sum_{i=1}^n Y_i\)</span> and <span class="math inline">\(\overline{x} = (1/n)\sum_{i=1}^n x_i\)</span>. Note: <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(\hat{\beta}\)</span> are each linear combinations of<span class="math inline">\(Y_1,\ldots,Y_n\)</span>.</p>
<p>For a given set of data the minimised sum of squared residuals is called the <strong>residual sum of squares (RSS)</strong>, that is,
<span class="math display">\[ RSS = \sum_{i=1}^n \left(Y_i-\hat{\alpha}-\hat{\beta}\,x_i\right)^2 = \sum_{i=1}^n r_i^{\,\,2} 
= \sum_{i=1}^n \left(Y_i-\hat{Y}_i\right)^2. \]</span></p>
<p><strong>Estimating <span class="math inline">\(\sigma^2\)</span></strong>. There is one remaining parameter to estimate; the error variance <span class="math inline">\(\sigma^2\)</span>. The usual estimator is
<span class="math display">\[\begin{equation}
\hat{\sigma}^2 = \frac{RSS}{n-2}.
\end{equation}\]</span>
An estimate of <span class="math inline">\(\sigma^2\)</span> is important because it quantifies how much variability there is about the assumed straight line relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(x\)</span>.</p>
<p><strong>Properties of estimators.</strong> It can be shown that
<span class="math display">\[  \mbox{E}(\hat{\alpha})=\alpha, \quad  \mbox{E}(\hat{\beta})=\beta, \quad  \mbox{E}(\hat{\sigma}^2)=\sigma^2, \]</span>
that is, these estimators are unbiased for the parameters they are intended to estimate. It can also be shown that the least squares estimators <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(\hat{\beta}\)</span> have the smallest possible variances of all unbiased estimators of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> which are linear combinations of the response <span class="math inline">\(Y_1,\ldots,Y_n\)</span>.</p>
<p><strong>Coefficient of determination.</strong> We may wish to quantify how much of the variability in the responses <span class="math inline">\(Y_1,\ldots,Y_n\)</span> is explained by the values <span class="math inline">\(x_1,\ldots,x_n\)</span> of the explanatory variable. To do this we can compare the variance of the residuals <span class="math inline">\(\{r_i\}\)</span> with the variance of the original observations <span class="math inline">\(\{Y_i\}\)</span>, producing the <strong>coefficient of determination</strong>, <span class="math inline">\(R^2\)</span>, given by
<span class="math display">\[ R^2 = 1- \frac{RSS}{\displaystyle\sum_{i=1}^n \left(Y_i-\bar{Y}\right)^2} 
= 1-\frac{\mbox{variability in $Y$ not explained by $x$}}
{\mbox{total variability of $Y$ about $\bar{Y}$}}, \]</span>
where <span class="math inline">\(0 \leq R^2 \leq 1\)</span>: <span class="math inline">\(R^2=1\)</span> indicates a perfect fit; <span class="math inline">\(R^2=0\)</span> indicates that none of the variability in <span class="math inline">\(Y_1,\ldots,Y_n\)</span> is explained by <span class="math inline">\(x_1,\ldots,x_n\)</span>, producing a horizontal regression line (<span class="math inline">\(\hat{\beta}=0\)</span>). The value of <span class="math inline">\(R^2\)</span>, perhaps expressed as a percentage, is often quoted when a simple linear regression model is fitted. This gives an estimate of the percentage of variability in <span class="math inline">\(Y\)</span> that is explained by <span class="math inline">\(x\)</span>.</p>
</div>
<div id="least-squares-fitting-to-hubbles-data" class="section level3">
<h3><span class="header-section-number">8.1.3</span> Least squares fitting to Hubble’s data</h3>
<p>Figures <a href="linreg.html#fig:hubblefitflat">8.4</a>, <a href="linreg.html#fig:hubblefitorigin">8.5</a> and <a href="linreg.html#fig:hubblefit">8.6</a> show least squares regression lines under 3 different models:</p>
<ul>
<li>Model 1. <span class="math inline">\(Y\)</span> does not depend on <span class="math inline">\(X\)</span>, so that</li>
</ul>
<p><span class="math display">\[ Y_i = \alpha_1 + \epsilon_i, \qquad i=1,\ldots, n. \]</span></p>
<ul>
<li>Model 2. <span class="math inline">\(Y\)</span> depends on <span class="math inline">\(X\)</span> according to Hubble’s law, so that</li>
</ul>
<p><span class="math display">\[ Y_i = \beta_2\,x_i + \epsilon_i, \qquad i=1,\ldots, n, \]</span></p>
<p>where <span class="math inline">\(\beta_2=T\)</span> is the age of the Universe.</p>
<ul>
<li>Model 3. <span class="math inline">\(Y\)</span> depends on <span class="math inline">\(X\)</span> according to the full linear regression model</li>
</ul>
<p><span class="math display">\[ Y_i = \alpha_3+\beta_3\,x_i + \epsilon_i, \qquad i=1,\ldots, n. \]</span></p>
<div class="figure" style="text-align: center"><span id="fig:hubblefitflat"></span>
<img src="images/hubble_fit_flat.png" alt="Scatter plot of distance against recession velocity, with least squares fit of a horizontal line." width="75%" />
<p class="caption">
Figure 8.4: Scatter plot of distance against recession velocity, with least squares fit of a horizontal line.
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:hubblefitorigin"></span>
<img src="images/hubble_fit_origin.png" alt="Scatter plot of distance against recession velocity, with least squares fit of a line through the origin." width="75%" />
<p class="caption">
Figure 8.5: Scatter plot of distance against recession velocity, with least squares fit of a line through the origin.
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:hubblefit"></span>
<img src="images/hubble_fit.png" alt="Scatter plot of distance against recession velocity, with least squares fit of an unconstrained line." width="75%" />
<p class="caption">
Figure 8.6: Scatter plot of distance against recession velocity, with least squares fit of an unconstrained line.
</p>
</div>
<p>These figures also shows the sizes of the residuals and the residual sums of squares <span class="math inline">\(RSS\)</span>. From the plots, and the relative sizes of <span class="math inline">\(RSS\)</span>, it seems clear that velocity <span class="math inline">\(x\)</span> explains some of the variability in the values of distance <span class="math inline">\(Y\)</span>. The residual sum of squares, <span class="math inline">\(RSS_3\)</span>, of Model 3 is smaller than the residual sum of squares, <span class="math inline">\(RSS_2\)</span>, of Model 2. It is impossible that <span class="math inline">\(RSS_3 &gt; RSS_2\)</span>.</p>
<p>A key question is whether <span class="math inline">\(RSS_3\)</span> is so much smaller than <span class="math inline">\(RSS_2\)</span> that we would choose Model 3 over Model 2, which is a question that is considered in STAT0003. Model 2 is an example of <strong>regression through the origin</strong>, where it is assumed that the intercept equals 0. We should only fit this kind of model if we have a good reason to. Here Hubble’s Law gives us a good reason. Note that for a regression through the origin, we use <span class="math inline">\(\hat{\sigma}^2 = RSS/(n-1)\)</span>.</p>
<p>Since we want to estimate the age of the Universe, we use the estimated regression line for Model 2:
<span class="math display">\[ y = 0.00192\,x.  \]</span>
Therefore, the estimated age of the Universe is given by
<span class="math display">\[ \hat{T}=0.00192 \mbox{ Mpc/km/sec}. \]</span>
To clarify the units of <span class="math inline">\(\hat{T}\)</span> we need to convert MPcs to kms by multiplying by <span class="math inline">\(3.086 \times 10^{19}\)</span>. This gives <span class="math inline">\(\hat{T}\)</span> in seconds. We convert to years by dividing by <span class="math inline">\(60 \times 60 \times 24 \times 365.25\)</span>:
<span class="math display">\[ \hat{T}
= 0.00192 \times \frac{3.086 \times 10^{19}}{60 \times 60 \times 24 \times 365.25} 
\approx 2 \mbox{ billion years}. \]</span></p>
<p><strong>Update</strong></p>
<p>Since Hubble’s work, physicists have obtained more data in order to obtain better estimates of distances of nebulae from the Earth and hence the age of the Universe. Figure <a href="linreg.html#fig:hubblescatternew">8.7</a> shows an example of these data (<span class="citation">Freedman et al. (<a href="#ref-Freedman2001" role="doc-biblioref">2001</a>)</span>). Using more powerful telescopes, it has been possible to estimate the distances of nebulae which are further from the Earth. Having a wider range of distances gives a better (smaller variance) estimator of the age of the Universe.</p>
<div class="figure" style="text-align: center"><span id="fig:hubblescatternew"></span>
<img src="images/hubble_scatter_new.png" alt="Scatter plots of new 'Hubble' data with fitted regression through the origin." width="75%" />
<p class="caption">
Figure 8.7: Scatter plots of new ‘Hubble’ data with fitted regression through the origin.
</p>
</div>
<p>Using these data we obtain <span class="math inline">\(\hat{T}=0.0123\)</span> which gives
<span class="math display">\[ 
\hat{T} = 
0.0123 \times \frac{3.086 \times 10^{19}}{60 \times 60 \times 24 \times 365.25} \approx 12 \mbox{ billion years}. \]</span>
This estimate agrees more closely with current scientific understanding of the age of the Universe than Hubble original estimate.</p>
</div>
<div id="normal-linear-regression-model" class="section level3">
<h3><span class="header-section-number">8.1.4</span> Normal linear regression model</h3>
<p>It is common to make the extra assumption that the errors are normally distributed:
<span class="math display">\[ \epsilon_i \sim N(0,\sigma^2), \qquad i=1,\ldots,n. \]</span>
In other words
<span class="math display">\[ Y_i \mid X=x_i \sim N(\alpha+\beta\,x_i,\sigma^2), \qquad i=1,\ldots,n. \]</span>
This assumption can be used to (a) decide whether the explanatory variable <span class="math inline">\(x\)</span> is needed in the model, and (b) produce interval estimates of <span class="math inline">\(\alpha, \beta, \sigma^2\)</span> and for predictions made from the model. These tasks are beyond the scope of STAT0002, but are covered in STAT0003.</p>
</div>
<div id="lmsummary" class="section level3">
<h3><span class="header-section-number">8.1.5</span> Summary of the assumptions of a (normal) linear regression model</h3>
<ol style="list-style-type: decimal">
<li><strong>Linearity</strong>: the conditional mean of <span class="math inline">\(Y\)</span> given <span class="math inline">\(x\)</span> is a linear function of <span class="math inline">\(x\)</span>.</li>
<li><strong>Constant error variance</strong>: the variability of <span class="math inline">\(Y\)</span> is the same for all values of <span class="math inline">\(x\)</span>.</li>
<li><strong>Uncorrelatedness of errors</strong>: the errors are not linearly associated.</li>
<li><strong>Normality of errors</strong>: for a given value of <span class="math inline">\(x\)</span>, <span class="math inline">\(Y\)</span> has a normal distribution.</li>
</ol>
<p><strong>Uncorrelatedness</strong> and <strong>independence</strong> are related concepts.<br />
If two random variables are independent then they are uncorrelated, that is, independence implies lack of correlation. However, the reverse is not true: two random variables can be uncorrelated but <strong>not</strong> independent (see Section <a href="correlationchapter.html#correxamples">9.3.2</a>, that is, lack of correlation does <strong>not</strong> imply independence. The only exception to this is the (multivariate) normal distribution: for example, if two (jointly) normal random variables are uncorrelated then they are independent. This explains why it is common for an alternative assumption 3. to be used:</p>
<ol start="3" style="list-style-type: decimal">
<li><strong>Independence of errors</strong>: knowledge that one response <span class="math inline">\(Y_i\)</span> is larger than expected based on the model does not give us information about whether a different response <span class="math inline">\(Y_j\)</span> is larger (or smaller) than expected.</li>
</ol>
<p>Notice that, even in the normal linear regression model, we have not made any assumption about the distribution of the <span class="math inline">\(x\)</span>s. In some studies the values of <span class="math inline">\(x\)</span> are chosen by an experimenter, that is, they are not random at all.</p>
</div>
</div>
<div id="looking" class="section level2">
<h2><span class="header-section-number">8.2</span> Looking at scatter plots</h2>
<p>Before fitting a linear regression model we should look at a scatter plot of <span class="math inline">\(Y\)</span> against <span class="math inline">\(x\)</span> to see whether the assumptions of the linear regression model appear to be reasonable. Some questions:</p>
<ol style="list-style-type: lower-roman">
<li><p>Is the relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(x\)</span> approximately linear?</p></li>
<li><p>Suppose that we imagine, or draw, a straight line (or a curve if the relationship is non-linear) through the data. Is the variability of <span class="math inline">\(Y\)</span> (the vertical spread) of points about this straight line approximately constant for all values of <span class="math inline">\(x\)</span>?</p></li>
<li><p>Are there any points which do not appear to fit in with the general pattern of the rest of the data?</p></li>
</ol>
<p>Figure <a href="linreg.html#fig:lmexplplots">8.8</a> gives some example scatter plots.</p>
<div class="figure" style="text-align: center"><span id="fig:lmexplplots"></span>
<img src="images/lm_expl_plots.png" alt="Some scatter plots.(a) (approximately) linear relationship, (approximately) constant spread about line; (b) non-linear relationship, constant spread about curve;(c) non-linear relationship, increasing spread about curve;(d) quadratic relationship, constant spread about curve; (e) linear relationship, increasing spread about line;(f) linear relationship, non-constant spread about line." width="95%" />
<p class="caption">
Figure 8.8: Some scatter plots.(a) (approximately) linear relationship, (approximately) constant spread about line; (b) non-linear relationship, constant spread about curve;(c) non-linear relationship, increasing spread about curve;(d) quadratic relationship, constant spread about curve; (e) linear relationship, increasing spread about line;(f) linear relationship, non-constant spread about line.
</p>
</div>
<p>We now consider what course of action to take based on each of these plots.</p>
<ol style="list-style-type: lower-alpha">
<li>The assumptions of linearity and constant error variance appear to be reasonable. Therefore, we fit a simple linear regression model to these data.</li>
<li>The assumption of linearity is not reasonable, but
<ol style="list-style-type: lower-roman">
<li>the relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(x\)</span> is monotonic (increasing), and</li>
<li>the variability in <span class="math inline">\(Y\)</span> is approximately constant for all values of <span class="math inline">\(x\)</span>.</li>
</ol></li>
</ol>
<p>We could try transforming <span class="math inline">\(x\)</span> to straighten the scatter plot, because transforming <span class="math inline">\(x\)</span> will not affect the vertical spread of the points. The plot is like the plot in the top left of Figure <a href="descriptive.html#fig:tukey">2.27</a>, so we could try <span class="math inline">\(\log x\)</span> or <span class="math inline">\(\sqrt{x}\)</span>. (If we transform <span class="math inline">\(Y\)</span> then (b)(ii) would probably no longer be true.)</p>
<ol start="3" style="list-style-type: lower-alpha">
<li>The assumption of linearity is not reasonable, but
<ol style="list-style-type: lower-roman">
<li>the relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(x\)</span> is monotonic (increasing), and</li>
<li>the variability in <span class="math inline">\(Y\)</span> increases as <span class="math inline">\(x\)</span> increases (or, equivalently, as the mean of <span class="math inline">\(Y\)</span> increases).</li>
</ol></li>
</ol>
<p>We could try transforming <span class="math inline">\(Y\)</span> to straighten the scatter plot. Transforming <span class="math inline">\(Y\)</span> will affect the vertical spread of the points. We may be able to find a transformation of <span class="math inline">\(Y\)</span> which both straightens the plot and makes the variability in <span class="math inline">\(Y\)</span> approximately constant for all values of <span class="math inline">\(x\)</span>. The plot is like the plot in the bottom right of Figure <a href="descriptive.html#fig:tukey">2.27</a>, so we could try <span class="math inline">\(\log Y\)</span> or <span class="math inline">\(\sqrt{Y}\)</span>. If not, we may need to transform both <span class="math inline">\(Y\)</span> and <span class="math inline">\(x\)</span>. (If we transform <span class="math inline">\(x\)</span> only then the variability of <span class="math inline">\(Y\)</span> will still increase as <span class="math inline">\(x\)</span> increases.)</p>
<ol start="4" style="list-style-type: lower-alpha">
<li>The assumption of linearity is not reasonable, and
<ol style="list-style-type: lower-roman">
<li>the relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(x\)</span> is not monotonic (it is quadratic), and</li>
<li>the variability in <span class="math inline">\(Y\)</span> is approximately constant for all values of <span class="math inline">\(x\)</span>.</li>
</ol></li>
</ol>
<p>Fit a regression model of the form
<span class="math display">\[ Y_i = \alpha+\beta_1\,x_i+\beta_2\,x_i^2 + \epsilon_i, \qquad i=1,\ldots, n. \]</span>
This is beyond the scope of STAT0002. You will study this in STAT0006.</p>
<ol start="5" style="list-style-type: lower-alpha">
<li><p>The assumption of linearity is reasonable but the variability in <span class="math inline">\(Y\)</span> increases as <span class="math inline">\(x\)</span> increases. A transformation of <span class="math inline">\(Y\)</span> may be able to make the variability of <span class="math inline">\(Y\)</span> approximately constant, but it would produce a non-linear relationship. (Transforming both <span class="math inline">\(Y\)</span> and <span class="math inline">\(x\)</span> might work.) However, even though the assumption of constant error variance is not reasonable, the least squares estimates of a simple linear regression are sensible (unbiased). However, it is better to use something called <strong>weighted least squares</strong> estimation (this is beyond the scope
of STAT0002).</p></li>
<li><p>The assumption of linearity is reasonable but the variability in <span class="math inline">\(Y\)</span> is small for extreme (small or large) values of <span class="math inline">\(x\)</span> and large for middling values of <span class="math inline">\(x\)</span>. The comments made in (e) also apply here.</p></li>
</ol>
<p>If we use transformation to make the assumptions of the simple linear regression model reasonable then, ideally, the transformation should make sense in terms of the subject matter of the data, and be easy to interpret. When interpreting the results of the modelling we need to take into account any transformation used. We will consider some examples where transformation is needed in Section <a href="linreg.html#linregtrans">8.4</a>. In the next section we consider how to check the assumptions of a model <strong>after</strong> it has been fitted.</p>
</div>
<div id="model-checking" class="section level2">
<h2><span class="header-section-number">8.3</span> Model checking</h2>
<p>We consider how to use residuals to check the various assumptions of a simple linear regression model. The residual <span class="math inline">\(r_i\)</span> is a measure of how closely a model agrees with the observation <span class="math inline">\(y_i\)</span>. Recall that we are checking for both isolated lack-of-fit (a few unusual observations) and systematic lack-of-fit (where some more general behaviour of the data is different from the model).</p>
<p>It is best to use graphs to examine the residuals. For many of these plots we can simply use the residuals
<span class="math display">\[ r_i = y_i-\hat{y}_i, \qquad i=1,\ldots,n. \]</span>
However, it can be helpful to standardise residuals so that they have approximately
a variance of 1, if the model is true. One possibility is to divide by the estimate of <span class="math inline">\(\sigma\)</span>, that is, <span class="math inline">\(r_i/\hat{\sigma}\)</span>.
However, different <span class="math inline">\(r_i\)</span>s usually have slightly different variances. To take account of this we can use <strong>standardised residuals</strong>
<span class="math display">\[ r^S_i = \frac{r_i}{\hat{\sigma}\,\sqrt{1-h_{ii}}}, \]</span>
where <span class="math inline">\(h_{ii}\)</span> is a number between 0 and 1 whose calculation is something that you do not need to know for STAT0002.</p>
<p>We consider each assumption of the model and the plot(s) which can be used to check for systematic departures from the assumption.</p>
<ul>
<li><strong>Linearity</strong>. The relationship between <span class="math inline">\(\mbox{E}(Y~|~X=x)\)</span> is linear in <span class="math inline">\(x\)</span>.</li>
</ul>
<p>The initial plot of <span class="math inline">\(Y\)</span> against <span class="math inline">\(x\)</span> gives us a good idea about this. We also plot (standardised) residuals against <span class="math inline">\(x\)</span> (or the fitted values). If the assumption of linearity is valid then we expect a random scatter of points with no obvious
pattern. Patterns in the residuals may indicate a non-linear relationship between
<span class="math inline">\(Y\)</span> and <span class="math inline">\(x\)</span>.</p>
<ul>
<li><strong>Constant error variance.</strong> The errors have the same variance, <span class="math inline">\(\sigma^2\)</span>.</li>
</ul>
<p>We plot (standardised) residuals against the fitted values. If the assumption of constant variance is valid then we expect a random scatter of points with no obvious pattern. A common departure is that the variability of the residuals increases/decreases with the fitted values. This suggests that the error variance increases/decreases with <span class="math inline">\(Y\)</span>.</p>
<ul>
<li><strong>Independent errors.</strong> The errors <span class="math inline">\(\{\epsilon_i\}\)</span> should be independent.</li>
</ul>
<p>This is an important assumption. However, it can be difficult to check, because there are many ways in which the response data could be dependent. We list some common forms of dependence below.</p>
<ul>
<li><p><strong>Serial</strong> dependence (dependence in time). Suppose that if observation <span class="math inline">\(Y_i\)</span> is larger (smaller) than expected under the model, giving a positive (negative) residual, then observation <span class="math inline">\(Y_{i+1}\)</span> will tend also to be larger (smaller) than expected. In this case the successive residuals will be positively associated: the value of one residual affects the value of the next residual. In this case, if the residuals are plotted in time order, there will tend to be more groups of positive residuals and groups of negative residuals than there would be if the residuals were independent. Similarly, if the residuals are negatively associated the residuals will tend alternate from positive to negative and back again repeatedly. If the data were collected in time order (and we are given the order) we should produce a scatter plot of <span class="math inline">\(r^S_{i+1}\)</span> against <span class="math inline">\(r^S_i\)</span>, that is, each residual against the previous residual, to check whether the size of one residual affects the size of the next residual.</p></li>
<li><p><strong>Spatial</strong> dependence. This is similar to serial dependence except that the dependence is over space rather than in time. It arises in situations where data are collected at different locations in space. It is common that observations from two nearby locations tend to be more similar to each other than observations from two distant locations. Suppose that we have time series of observations at two locations. We should produce a scatter plot of the residuals at one location against the (corresponding) residuals at the other location.</p></li>
<li><p><strong>Cluster</strong> dependence. This may occur when the data have been collected in groups. For example, suppose that we collect data on students from UCL. Two students from the same department may tend to be more similar to each other in their responses than two students from different departments. In such cases we could colour or label the points in the residual plots to reflect the department of each student, and look for patterns.</p></li>
</ul>
<p>Otherwise, it is important to understand how the data were generated, and what they mean, in order to assess whether (given the values of <span class="math inline">\(x_1,\ldots x_n\)</span>) the responses, <span class="math inline">\(Y_1,\ldots,Y_n\)</span>, can be assumed to be independent.</p>
<ul>
<li><strong>Normality</strong>. If the errors <span class="math inline">\(\{\epsilon_i\}\)</span> are assumed to have a normal distribution then the standardised residuals should have approximately a N(0,1) distribution. We can look at a histogram or a normal QQ plot of residuals.</li>
</ul>
<p>In addition we should check these plots for any points which do not fit in with the pattern of the other residuals.</p>
<p>Figure <a href="linreg.html#fig:resplots">8.9</a> shows the plots of residuals against fitted values after fitting a linear regression model to the scatter plots in Figure <a href="linreg.html#fig:lmexplplots">8.8</a>. (Note that we shouldn’t really fit a linear model to the data in plots (b), (c) and (d). We do this here just to show what the residual plots look like.) You should be able to see how the residual plots relate to the original scatter plots.</p>
<div class="figure" style="text-align: center"><span id="fig:resplots"></span>
<img src="images/lm_res_plots.png" alt="Patterns to look out for in residual plots: (a) random scatter (satisfactory); (b) non-linear, constant spread about curve; (c) non-linear, increasing spread about curve; (d) quadratic, constant spread about curve; (e) scatter about line residual=0, increasing spread about line (triangle/funnel); (f) scatter about line residual=0, non-constant spread about line (diamond)." width="95%" />
<p class="caption">
Figure 8.9: Patterns to look out for in residual plots: (a) random scatter (satisfactory); (b) non-linear, constant spread about curve; (c) non-linear, increasing spread about curve; (d) quadratic, constant spread about curve; (e) scatter about line residual=0, increasing spread about line (triangle/funnel); (f) scatter about line residual=0, non-constant spread about line (diamond).
</p>
</div>
<p>Plot (a) is an example of a residual plot for a model that fits well: a random scatter of points about the horizontal line residual=0, with approximately constant vertical spread for all values on the <span class="math inline">\(x\)</span>-axis. In Section <a href="linreg.html#looking">8.2</a> we explained, for the other plots, what the patterns in these plots implied about the relationship between the response and the explanatory variable, and we considered actions we could take in these cases.</p>
<p>Note that:</p>
<ul>
<li>We must take into account the spread of the points on the <span class="math inline">\(x\)</span>-axes of these plots. For example, a triangular pattern can be produced just because there are more large fitted values than small fitted values (see Figure <a href="linreg.html#fig:unevenx">8.10</a>).</li>
<li>The human eye is very good at picking out patterns in plot, even when these patterns do not really exist!</li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:unevenx"></span>
<img src="images/uneven.png" alt="Left: scatter plot of $y$ against $x$.  The true relationship is linear with constant error variance.  Right: a triangular residual plot results from uneven distribution of $x$ values." width="95%" />
<p class="caption">
Figure 8.10: Left: scatter plot of <span class="math inline">\(y\)</span> against <span class="math inline">\(x\)</span>. The true relationship is linear with constant error variance. Right: a triangular residual plot results from uneven distribution of <span class="math inline">\(x\)</span> values.
</p>
</div>
<p>Figure <a href="linreg.html#fig:nonnormal">8.11</a> shows a scatter plot where the relationship between <span class="math inline">\(\mbox{E}(Y \mid X=x)\)</span> and <span class="math inline">\(x\)</span> is linear and the variability in <span class="math inline">\(Y\)</span> about this line is approximately constant. However, the distribution of <span class="math inline">\(Y\)</span> about the line is skewed. We can see this from the scatter plot and, more clearly, in the residual plots in Figure <a href="linreg.html#fig:nonnormalres">8.12</a>. Although transformation of <span class="math inline">\(Y\)</span> could make the error distribution more symmetric, it will mess up the linearity. Therefore, the action to take is to fit the linear regression model, and report the fact that the error distribution is skewed.</p>
<div class="figure" style="text-align: center"><span id="fig:nonnormal"></span>
<img src="images/non_normal.png" alt="Scatter plot of $y$ against $x$.  The true relationship is linear but the error distribution skewed (with constant variance)." width="75%" />
<p class="caption">
Figure 8.11: Scatter plot of <span class="math inline">\(y\)</span> against <span class="math inline">\(x\)</span>. The true relationship is linear but the error distribution skewed (with constant variance).
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:nonnormalres"></span>
<img src="images/non_normal_res.png" alt="Residual plots from the linear model fitted to the data in Figure \@ref(fig:nonnormal)." width="95%" />
<p class="caption">
Figure 8.12: Residual plots from the linear model fitted to the data in Figure <a href="linreg.html#fig:nonnormal">8.11</a>.
</p>
</div>
<p>Transformation of <span class="math inline">\(Y\)</span> could make the error distribution more symmetric, but it will mess up the linearity. Therefore, we fit the linear regression model, and report
the fact that the error distribution is skewed.</p>
<p><strong>The Hubble example (continued)</strong>. We return to Hubble’s data. Figure <a href="linreg.html#fig:hubblecheck">8.14</a> shows some residual plots for Model 2 and Model 3.</p>
<div class="figure" style="text-align: center"><span id="fig:hubblecheckorigin"></span>
<img src="images/hubble_check_origin.png" alt="Checking the linear regression model fitted to Hubble's data. Model 2, regression through the origin." width="95%" />
<p class="caption">
Figure 8.13: Checking the linear regression model fitted to Hubble’s data. Model 2, regression through the origin.
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:hubblecheck"></span>
<img src="images/hubble_check.png" alt="Checking the linear regression model fitted to Hubble's data. Model 3." width="95%" />
<p class="caption">
Figure 8.14: Checking the linear regression model fitted to Hubble’s data. Model 3.
</p>
</div>
<p>There is no obvious lack-of-fit for Model 3. For Model 2 the main effect of forcing the regression line through the origin is to produce weak linear patterns in the plots of residuals against fitted values and against velocity.</p>
<div id="departures-from-assumptions" class="section level3">
<h3><span class="header-section-number">8.3.1</span> Departures from assumptions</h3>
<p>As we have seen in Section <a href="linreg.html#lmsummary">8.1.5</a> the main assumptions underlying a (normal) linear regression model are linearity, constant error variance, independence of errors and normality of errors. Since all models are wrong (see Section <a href="inference.html#probmodels">6.3</a> these assumptions cannot be exactly true. However, we hope that they are approximately true, or at least close enough to being true that the results from the model are useful.</p>
<ul>
<li>If one, or more, of these assumptions is not even approximately true, should we be worried? What should we do about it?</li>
<li>Are these assumptions of equal importance? Or are particular assumptions more important than others, because even a small departure from the assumption results in our inferences being very incorrect?</li>
</ul>
<p>We consider departures from each assumption, and what to do about them, in turn.
For the moment we assume that there is a departure from only one of the assumptions.</p>
<p><strong>Linearity</strong></p>
<p>Departures.</p>
<ol style="list-style-type: lower-alpha">
<li>Systematic departure. A straight line is not appropriate generally: curved relationship or two separate straight lines, e.g. one for men, one for women.</li>
<li>Isolated departure. A straight line is appropriate for most of the data but there are a small number of outliers.</li>
</ol>
<p>Consequences.</p>
<ul>
<li>Bias: systematic over/under estimation and prediction of <span class="math inline">\(\mbox{E}(Y \mid X=x)\)</span>.}</li>
<li>Misleading inferences: confidence intervals and standard errors (see STAT0003)
do not reflect the true uncertainty, hypothesis tests of regression parameters
(see STAT0003) do not have the correct properties.</li>
</ul>
<p>Action.</p>
<ol style="list-style-type: lower-alpha">
<li>Systematic departure. Curved relationship: transform <span class="math inline">\(Y\)</span> and/or <span class="math inline">\(x\)</span> to achieve approximate linearity; or fit a non-linear regression model (beyond scope of STAT0002). 2 straight lines: fit two separate regression lines, e.g. one for males and one for females.</li>
<li>Isolated departure. Check outliers (see Section <a href="linreg.html#outliers">8.3.2</a>). If outliers are kept, use a statistical method that is designed to be resistant to the presence of outliers (beyond scope of STAT0002).</li>
</ol>
<p><strong>Constant error variance</strong></p>
<p>Departures: for example, plots (e) and (f) in Figure <a href="linreg.html#fig:lmexplplots">8.8</a>.</p>
<p>Consequences.</p>
<ul>
<li>The least squares estimators are still unbiased for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>.</li>
<li>Misleading inferences (as in linearity above).</li>
</ul>
<p>Action: transform <span class="math inline">\(Y\)</span> to achieve approximate constancy of error variance; or
use weighted least squares (beyond scope of STAT0002).</p>
<p><strong>Independence of errors</strong></p>
<p>Departures: serial, spatial or cluster dependence.</p>
<p>Consequences:</p>
<ul>
<li>The least squares estimators are still unbiased for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>.</li>
<li>Seriously misleading inferences (as above but more affected). For example, if there is strong dependence between the errors then the amount of information in the data to estimate the model parameters is much different than if the errors are independent. Not taking this into account means that uncertainty in the estimates is misrepresented.</li>
</ul>
<p>Action: more complicated modelling to take into account the form of the dependence (beyond scope of STAT0002).</p>
<p><strong>Normality of errors</strong></p>
<p>Departures: an error distribution which is not normal, for example, skewed (see Figure <a href="linreg.html#fig:nonnormal">8.11</a>, light-tailed or heavy-tailed.</p>
<p>Consequences:</p>
<ul>
<li>The least squares estimators are still unbiased for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>.</li>
<li>Inferences: confidence intervals, hypothesis tests etc. are OK unless the non-normality is extreme or the sample size is small. This is because the sampling distributions of <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(\hat{\beta}\)</span> can be approximately normal
even when the errors are not normal.</li>
<li>However, prediction intervals for new observations (see STAT0003) are affected because they are based directly on the normality of the errors.</li>
</ul>
<p>Action:</p>
<ul>
<li>Fit the model; report the non-normality of the error distribution.</li>
<li>There may be a good reason to use a model with a different response distribution for <span class="math inline">\(Y\)</span>, for example, if the <span class="math inline">\(Y\)</span>s are counts we could use a Poisson distribution (beyond the scope of STAT0002).</li>
</ul>
<p>The order of importance of the assumptions for a linear model (most important first) is: linearity and independence, constant error variance, normality of errors. We may find that there are clear departures from more than one assumption, for example, the relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(x\)</span> is non-linear and the error variance is not constant. If we are lucky we might find a transformation to improve both of these departures.</p>
</div>
<div id="outliers" class="section level3">
<h3><span class="header-section-number">8.3.2</span> Outliers and influential observations</h3>
<p>We have already considered the possibility of outliers. We shouldn’t remove an observation just because it appears to be an outlier. However, we should consider whether the conclusions of our analysis are not influenced strongly by outliers. An observation which influences strongly the conclusions of a statistical analysis is called an <strong>influential observation</strong>. We can assess whether an observation (or group of observations) is influential is to carry out the analysis with and without the observation, and see whether the conclusions change by a practically relevant amount. We will see that in a linear regression analysis some types of outlier are influential, whereas others are not.</p>
<p>Figure <a href="linreg.html#fig:lminfluence">8.15</a> shows some examples where there are one or two outlying observations. The greater the influence of the outlier(s) the greater the change in the fitted regression line when the they are removed. Generally speaking, an observation with an outlying (small or large) value of <span class="math inline">\(x\)</span> has a greater potential (the technical term is <strong>leverage</strong>) to affect the slope of the fitted regression line than an observation which has an average <span class="math inline">\(x\)</span> value. This can be seen by comparing plots (a) and (b).</p>
<div class="figure" style="text-align: center"><span id="fig:lminfluence"></span>
<img src="images/lm_influence.png" alt="Outliers and their influence on the fitted linear regression line. The fitted line for the complete data (---------) and when the outlier(s) are removed (- - - - - -) is drawn on the plots. (a) outlying $x$ value with moderate influence on the fitted regression line; (b) average $x$ value with small influence; (c) 2 outlying $x$s with large influence; (d) outlying $x$ with large influence." width="95%" />
<p class="caption">
Figure 8.15: Outliers and their influence on the fitted linear regression line. The fitted line for the complete data (———) and when the outlier(s) are removed (- - - - - -) is drawn on the plots. (a) outlying <span class="math inline">\(x\)</span> value with moderate influence on the fitted regression line; (b) average <span class="math inline">\(x\)</span> value with small influence; (c) 2 outlying <span class="math inline">\(x\)</span>s with large influence; (d) outlying <span class="math inline">\(x\)</span> with large influence.
</p>
</div>
<p><strong>How to deal with outliers and influential observations</strong></p>
<p>Before carrying out a statistical analysis the data should be checked for unusual or unexpected observations and, if possible, any errors should be corrected. Suppose that as a result of statistical analyses an observation, or group of observations, are thought to be outlying and/or influential. Then the following procedure should be followed.</p>
<ol style="list-style-type: decimal">
<li><p>Do the conclusions change by a practically important amount when the observation is deleted?</p>
<ol style="list-style-type: lower-alpha">
<li>If ``No’’: Leave the observation in the dataset;</li>
<li>If ``Yes’’: goto 2.</li>
</ol></li>
<li><p>Investigate the observation.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Is the observation clearly incorrect?</p>
<ol style="list-style-type: lower-roman">
<li>If ``Yes’’: if the error cannot be corrected then either use a resistant statistical method or delete the case.</li>
<li>If ``No’’: goto 3.</li>
</ol></li>
<li><p>Is there good reason to believe that the observation does not come from the population of interest?</p>
<ol style="list-style-type: lower-roman">
<li>If ``Yes’’: either use a resistant statistical method or delete the case.</li>
<li>If ``No’’: goto 3.</li>
</ol></li>
</ol></li>
<li><p>(For linear regression.) Does the observation have an explanatory variable (<span class="math inline">\(x\)</span>) value which is much smaller or much larger than those of the other observations?</p>
<ol style="list-style-type: lower-alpha">
<li>If ``Yes’’: delete the observation; refit the model; report conclusions for the reduced range of explanatory variables.</li>
<li>If ``No’’: more data, or more information about the influential observation, are needed. Perhaps report 2 separate conclusions: one with the influential observation, one without.</li>
</ol></li>
</ol>
<p>There must be a good reason for deleting an observation.</p>
</div>
</div>
<div id="linregtrans" class="section level2">
<h2><span class="header-section-number">8.4</span> Use of transformations</h2>
<p>In Section <a href="descriptive.html#transformation">2.8</a> we found that taking a log transformation of rainfall amounts in the cloud-seeding dataset had two effects: (a) it made the values in the two groups more symmetrically distributed, and (b) made the respective sample variances more similar. In Section <a href="linreg.html#looking">8.2</a> we saw examples where there were clear departures from the assumptions of a linear regression model (see Section <a href="linreg.html#lmsummary">8.1.5</a>), but where a transformation of <span class="math inline">\(Y\)</span> and/or <span class="math inline">\(x\)</span> might be able to improve this.</p>
<p>Suppose that, either before fitting a linear regression model or afterwards (checking the model using residual plots), we find that one, or more, of the assumptions of linearity, constant variance or normality appear not to be valid.We may be able to find transformations of the response variable <span class="math inline">\(Y\)</span> and/or the explanatory variable <span class="math inline">\(x\)</span> for which these assumptions are more valid.However, it may not be possible to find transformations for which all the assumptions are approximately valid.</p>
<p>To illustrate this we return to the 2000 US Presidential Election example. We saw that some of the relationships between Pat Buchanan’s vote <span class="math inline">\(Y\)</span> and the explanatory variables were distinctly non-linear. We also concluded that the vote Buchanan received in Palm Beach County was an outlier, explainable by the format of the ‘Butterfly’ ballot paper used in Palm Beach. Therefore, we build a linear regression model for Buchanan’s vote based on the election data in Florida, excluding Palm Beach County. We use only one explanatory variable: Total Population, denoted by <span class="math inline">\(x\)</span>.</p>
<p>We consider 4 different models:</p>
<p><span class="math display">\[\begin{eqnarray*}
\mbox{E}(Y~|~X=x) &amp;=&amp; \alpha_1+\beta_1\,x, \qquad \quad \,\,\,\,\mbox{(model 1)}, \\
\mbox{E}(Y~|~X=x) &amp;=&amp; \alpha_2+\beta_2\,\log x, \qquad \mbox{(model 2)}, \\
\mbox{E}(\log Y~|~X=x) &amp;=&amp; \alpha_3+\beta_3\,\log x, \qquad \mbox{(model 3)}, \\
\mbox{E}(\sqrt{Y}~|~X=x) &amp;=&amp; \alpha_4+\beta_4\,\log x, \qquad \mbox{(model 4)}.
\end{eqnarray*}\]</span></p>
<p>Figure <a href="linreg.html#fig:electionLS">8.16</a> shows scatter plots of the variables involved for each of these models, with least squares regression lines.</p>
<div class="figure" style="text-align: center"><span id="fig:electionLS"></span>
<img src="images/election_LS.png" alt="Least squares regression lines added to plots of the percentage of the vote for Buchanan against total population. Various transformations have been used." width="95%" />
<p class="caption">
Figure 8.16: Least squares regression lines added to plots of the percentage of the vote for Buchanan against total population. Various transformations have been used.
</p>
</div>
<p>Figures <a href="linreg.html#fig:electioncheck1">8.17</a>, <a href="linreg.html#fig:electioncheck2">8.18</a>, <a href="linreg.html#fig:electioncheck3">8.19</a> and <a href="linreg.html#fig:electioncheck4">8.20</a> show residual plots for each of these models.</p>
<div class="figure" style="text-align: center"><span id="fig:electioncheck1"></span>
<img src="images/election_check1.png" alt="Model checking for model 1." width="95%" />
<p class="caption">
Figure 8.17: Model checking for model 1.
</p>
</div>
<p>For Model 1 the plots of residuals against fitted values and residuals against Total Population indicate non-linearity and non-constant error variance. Given that the fit is so bad in these respects, it is pointless examining the histogram and normal QQ plot of the residuals. However, these plots indicate that the residuals look very unlike a sample from a normal distribution.</p>
<div class="figure" style="text-align: center"><span id="fig:electioncheck2"></span>
<img src="images/election_check2.png" alt="Model checking for model 2." width="95%" />
<p class="caption">
Figure 8.18: Model checking for model 2.
</p>
</div>
<p>The general patterns in the plots for Model 2 are similar to those for Model 1, but the fit is improved compared to Model 1.</p>
<div class="figure" style="text-align: center"><span id="fig:electioncheck3"></span>
<img src="images/election_check3.png" alt="Model checking for model 3." width="95%" />
<p class="caption">
Figure 8.19: Model checking for model 3.
</p>
</div>
<p>For Model 3 the plots of residuals against fitted values and against Total Population show a random scatter of points about the residual = 0 horizontal line. This is what we expect to see when the assumptions of linearity and constancy of error variance hold. In addition, the histogram and normal QQ plot of the residuals suggest that the residuals look like a sample from a normal distribution. This suggests that it would be reasonable to assume that the errors in the model are normally distributed.</p>
<div class="figure" style="text-align: center"><span id="fig:electioncheck4"></span>
<img src="images/election_check4.png" alt="Model checking for model 4." width="95%" />
<p class="caption">
Figure 8.20: Model checking for model 4.
</p>
</div>
<p>For Model 4 there is a slight suggestion in the plots of residuals against fitted values and residuals against Total Population that the error variance increases with the value of <span class="math inline">\(Y\)</span>, and hence decreases with the value of Total Population.</p>
<p>In summary, transformations may be used to</p>
<ul>
<li>make the relationship between two random variables closer to being linear;</li>
<li>make the variance of a random variable <span class="math inline">\(Y\)</span> closer to being constant in different groups, or for different values of another variable <span class="math inline">\(x\)</span>;</li>
<li>make the distribution of a random variable more symmetric;</li>
<li>make the distribution of a random variable (or an error term in a linear regression model) closer to being normal.</li>
</ul>
<p>However, it may not be possible for a single transformation to achieve all the properties we want. For example, suppose that the relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(x\)</span> is non-linear and that the variance of <span class="math inline">\(Y\)</span> depends on the value of <span class="math inline">\(x\)</span>. We may find that</p>
<ul>
<li>the relationship between <span class="math inline">\(\sqrt{Y}\)</span> and <span class="math inline">\(x\)</span> is approximately linear, and</li>
<li>the variance of <span class="math inline">\(\log Y\)</span> is approximately constant for all values of <span class="math inline">\(x\)</span>.</li>
</ul>
<p>If we wish to fit a linear regression model, after transforming <span class="math inline">\(Y\)</span>, a compromise is necessary.</p>
<div id="interpretation-after-transformation" class="section level3">
<h3><span class="header-section-number">8.4.1</span> Interpretation after transformation</h3>
<p>If we use a transformation we should be careful to interpret the results carefully in light of the transformation used. Often we will want to report the results on the original scale of the data, rather than the transformed scale. We illustrate this using the 2000 US Presidential Election example, using Model 3:
<span class="math display">\[ \mbox{E}(\log Y \mid X=x) = \alpha+\beta\,\log x, \]</span>
where <span class="math inline">\(Y\)</span> is the percentage of the vote obtained by Pat Buchanan and <span class="math inline">\(\log\)</span> denotes the natural log, that is, log to the base <span class="math inline">\(e\)</span>. Again, we fit the model after excluding the observation from Palm Beach County. The least squares regression line is shown in the bottom left of Figure <a href="linreg.html#fig:electionLS">8.16</a>. The equation of this line is</p>
<p><span class="math display">\[ \widehat{\mbox{E}}(\log Y \mid X=x) = 0.4 - 0.3\,\log x, \]</span></p>
<p>that is, <span class="math inline">\(\hat{\alpha} = 0.4\)</span> and <span class="math inline">\(\hat{\beta} = -0.3\)</span>.</p>
<p>This provides an estimate for the way that the mean of <span class="math inline">\(\log Y\)</span> depends on <span class="math inline">\(x\)</span>. However, we may well be interested in <span class="math inline">\(Y\)</span>, not <span class="math inline">\(\log Y\)</span>. Recall from Section <a href="rvs.html#EgX">4.3.4</a> that, some special cases aside, <span class="math inline">\(g[\mbox{E}(Y)] \neq \mbox{E}[g(Y)].\)</span> In the current context,</p>
<p><span class="math display">\[ \log \mbox{E}(Y \mid X=x) \neq \mbox{E}(\log Y \mid X=x) \]</span>
or
<span class="math display">\[ \mbox{E}(Y \mid X=x) \neq \exp \left\{ \mbox{E}(\log Y \mid X=x) \right\}. \]</span>
Therefore, if we exponentiate the regression line then we do not obtain <span class="math inline">\(\mbox{E}(Y \mid X=x)\)</span>, at least not exactly.</p>
<p>We will consider two ways to tackle this problem.</p>
<p><strong>Finding <span class="math inline">\(\mbox{median}(Y \mid X = x).\)</span></strong> We assume that the error distribution is symmetric about 0, which implies that</p>
<p><span class="math display">\[ \mbox{E}(\log Y \mid X=x) = \mbox{median}(\log Y \mid X=x). \]</span></p>
<p>Consider a continuous random variable <span class="math inline">\(W\)</span>. If <span class="math inline">\(P(\log W \leq m) = 1/2\)</span> then <span class="math inline">\(P(W \leq e^m) = 1/2\)</span>. If <span class="math inline">\(m\)</span> is the median of <span class="math inline">\(\log W\)</span> then <span class="math inline">\(e^m\)</span> is the median of <span class="math inline">\(W\)</span>. Therefore,</p>
<p><span class="math display">\[ \mbox{median}(Y \mid X=x) = \exp \left\{ \mbox{median}(\log Y \mid X=x) \right\} = \exp \{ \alpha + \beta \, \log x\} = e^\alpha x^\beta. \]</span></p>
<p>This leads to the following estimate for the conditional median of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X = x\)</span>:</p>
<p><span class="math display">\[ \widehat{\mbox{median}}(Y \mid X=x) = e^{0.4 - 0.3\,\log x} = e^{0.4}\,x^{-0.3} = 1.5\,x^{-0.3}. \]</span></p>
<p>Based on this equation we estimate that if the population <span class="math inline">\(x\)</span> doubles then the median of the percentage of the vote that Buchanan obtains is multiplied by <span class="math inline">\(2^{-0.3}=0.8\)</span>. Doubling the population is estimated to reduce the median of Buchanan’s vote by 20%, for example 5% would reduce to 4%. This provides an interpretation for the estimate <span class="math inline">\(\hat{\beta} = -0.3\)</span>. What about <span class="math inline">\(\hat{\alpha}=0.4\)</span>, or rather <span class="math inline">\(e^{0.4}=1.5\)</span>? When <span class="math inline">\(x=1\)</span>,
<span class="math display">\[ \widehat{\mbox{median}}(Y \mid X=x) = 1.5. \]</span>
We estimate that in populations of size 1000 the median of Buchanan’s vote is 1.5% of the vote. However, The smallest population in the data is 6289 people, so we are extrapolating when we make this interpretation. It would be better to work with population recorded in units of 100,000. Then the intercept relates to a realistic population.</p>
<p><strong>Finding <span class="math inline">\(\mbox{E}(Y \mid X = x).\)</span></strong> We assume that the error distribution is normal. Suppose that <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> are Buchanan’s percentage of the vote and the population size in an arbitrarily-chosen county. Given that <span class="math inline">\(X=x\)</span> we have
<span class="math display">\[ \log Y = \alpha + \beta \log x + \epsilon, \]</span>
where <span class="math inline">\(\epsilon \sim N(0,\sigma^2)\)</span>. Therefore,
<span class="math display">\[ Y = e^\alpha x^\beta e^\epsilon. \]</span>
It can be shown that <span class="math inline">\(\mbox{E}\left(e^\epsilon\right)=e^{\sigma^2/2}\)</span>. Taking expectations gives</p>
<p><span class="math display">\[ \mbox{E}(Y \mid X=x) = \mbox{E}( e^\alpha x^\beta e^\epsilon)
= e^\alpha x^\beta \mbox{E}\left( e^\epsilon \right)
= e^\alpha \, x^\beta \, e^{\sigma^2/2}. \]</span></p>
<p>Therefore,</p>
<p><span class="math display">\[ \mbox{E}(Y \mid X = x) = e^{\sigma^2/2} \, \mbox{median}(Y \mid X = x). \]</span></p>
<p>Therefore, if the error variance <span class="math inline">\(\sigma^2\)</span> is small then <span class="math inline">\(\mbox{E}(Y \mid X = x) \approx \mbox{median}(Y \mid X = x).\)</span></p>
</div>
</div>
<div id="over-fitting" class="section level2">
<h2><span class="header-section-number">8.5</span> Over-fitting</h2>
<p>We fitted a straight line model,
<span class="math display">\[ \mbox{E}(Y \mid X=x) = \alpha+\beta\,x, \]</span>
to Hubble’s data. Even if this model is true, we can (usually) obtain a better fit (smaller RSS) by fitting a quadratic model,
<span class="math display">\[\mbox{E}(Y \mid X=x) = \alpha+\beta\,x+\gamma\,x^2.\]</span></p>
<p>Extending this idea means that we can usually get closer and closer fits to the data by adding to the model a term in <span class="math inline">\(x^3\)</span>, a term in <span class="math inline">\(x^4\)</span>, and so on. Fitting a model with more than one explanatory variable is beyond the scope of STAT0002. However, in Figure <a href="linreg.html#fig:hubblecurves">8.21</a> we show fits to Hubble’s data of polynomials of degree 2, 3, 4 and 8.</p>
<div class="figure" style="text-align: center"><span id="fig:hubblecurves"></span>
<img src="images/hubble_curves.png" alt="Fits of polynomial of degrees 2, 3, 4 and 8 to Hubble's data." width="95%" />
<p class="caption">
Figure 8.21: Fits of polynomial of degrees 2, 3, 4 and 8 to Hubble’s data.
</p>
</div>
<p>As the degree of the polynomial increases the <span class="math inline">\(RSS\)</span> decreases. Fitting a polynomial of degree 8 gives us a better fit (as judged by the values of RSS) than the linear fit. However, there are several reasons why using a high degree polynomial is a bad idea.</p>
<ul>
<li><strong>Lack of sense</strong>. Fitting a very wiggly curve to these data doesn’t make sense: Hubble’s theory says that distance should be a linear function of velocity.</li>
<li><strong>Lack of understanding</strong>. Fitting a model which is too complicated doesn’t help our understanding the process which produced the data.</li>
<li><strong>Poor prediction</strong>. A fitted curve that is too complicated will tend to predict new values poorly, because the curve adapts too closely to the random wiggles of the data, rather than getting the general relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(x\)</span> approximately correct. A model that fits existing data well doesn’t necessarily predict new observations well.</li>
</ul>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Freedman2001">
<p>Freedman, Wendy L., Barry F. Madore, Brad K. Gibson, Laura Ferrarese, Daniel D. Kelson, Shoko Sakai, Jeremy R. Mould, et al. 2001. “Final Results from theHubble Space TelescopeKey Project to Measure the Hubble Constant.” <em>The Astrophysical Journal</em> 553 (1): 47–72. <a href="https://doi.org/10.1086/320638">https://doi.org/10.1086/320638</a>.</p>
</div>
<div id="ref-Hubble1929">
<p>Hubble, Edwin. 1929. “A Relation Between Distance and Radial Velocity Among Extra-Galactic Nebulae.” <em>Proceedings of the National Academy of Sciences</em> 15 (3): 168–73. <a href="https://doi.org/10.1073/pnas.15.3.168">https://doi.org/10.1073/pnas.15.3.168</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="contingency.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="correlationchapter.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["stat0002book.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
