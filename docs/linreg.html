<!DOCTYPE html>
<html lang="en-gb" xml:lang="en-gb">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Linear regression | STAT0002 Introduction to Probability and Statistics</title>
  <meta name="description" content="Produces STAT0002 notes in an accessible format" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Linear regression | STAT0002 Introduction to Probability and Statistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Produces STAT0002 notes in an accessible format" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Linear regression | STAT0002 Introduction to Probability and Statistics" />
  
  <meta name="twitter:description" content="Produces STAT0002 notes in an accessible format" />
  

<meta name="author" content="Dr Paul Northrop" />


<meta name="date" content="2020-12-04" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="contingency.html"/>
<link rel="next" href="correlationchapter.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">STAT0002 2020-21</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>The purpose of these notes</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#real"><i class="fa fa-check"></i><b>1.1</b> Real statistical investigations</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#shuttle"><i class="fa fa-check"></i><b>1.2</b> Challenger Space Shuttle Catastrophe</a><ul>
<li class="chapter" data-level="1.2.1" data-path="introduction.html"><a href="introduction.html#uncertainty"><i class="fa fa-check"></i><b>1.2.1</b> Uncertainty</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#a-very-brief-introduction-to-stochastic-simulation"><i class="fa fa-check"></i><b>1.3</b> A very brief introduction to stochastic simulation</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="descriptive.html"><a href="descriptive.html"><i class="fa fa-check"></i><b>2</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="2.1" data-path="descriptive.html"><a href="descriptive.html#types-of-data"><i class="fa fa-check"></i><b>2.1</b> Types of data</a><ul>
<li class="chapter" data-level="2.1.1" data-path="descriptive.html"><a href="descriptive.html#qualitative-or-categorical-data"><i class="fa fa-check"></i><b>2.1.1</b> Qualitative or categorical data</a></li>
<li class="chapter" data-level="2.1.2" data-path="descriptive.html"><a href="descriptive.html#quantitative-or-numerical-data"><i class="fa fa-check"></i><b>2.1.2</b> Quantitative or numerical data</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="descriptive.html"><a href="descriptive.html#describing-distributions"><i class="fa fa-check"></i><b>2.2</b> Describing distributions</a><ul>
<li class="chapter" data-level="" data-path="descriptive.html"><a href="descriptive.html#example-oxford-births-data"><i class="fa fa-check"></i>Example: Oxford births data</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="descriptive.html"><a href="descriptive.html#summary-statistics"><i class="fa fa-check"></i><b>2.3</b> Summary Statistics</a><ul>
<li class="chapter" data-level="2.3.1" data-path="descriptive.html"><a href="descriptive.html#fivenumber"><i class="fa fa-check"></i><b>2.3.1</b> Five number summary</a></li>
<li class="chapter" data-level="2.3.2" data-path="descriptive.html"><a href="descriptive.html#meanstdev"><i class="fa fa-check"></i><b>2.3.2</b> Mean and standard deviation</a></li>
<li class="chapter" data-level="2.3.3" data-path="descriptive.html"><a href="descriptive.html#mode"><i class="fa fa-check"></i><b>2.3.3</b> Mode</a></li>
<li class="chapter" data-level="2.3.4" data-path="descriptive.html"><a href="descriptive.html#symmetry"><i class="fa fa-check"></i><b>2.3.4</b> Symmetry</a></li>
<li class="chapter" data-level="2.3.5" data-path="descriptive.html"><a href="descriptive.html#correlation"><i class="fa fa-check"></i><b>2.3.5</b> Correlation</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="descriptive.html"><a href="descriptive.html#tables"><i class="fa fa-check"></i><b>2.4</b> Tables</a><ul>
<li class="chapter" data-level="2.4.1" data-path="descriptive.html"><a href="descriptive.html#frequency-distribution"><i class="fa fa-check"></i><b>2.4.1</b> Frequency distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="descriptive.html"><a href="descriptive.html#graphs"><i class="fa fa-check"></i><b>2.5</b> Graphs (1 variable)</a><ul>
<li class="chapter" data-level="2.5.1" data-path="descriptive.html"><a href="descriptive.html#histogram"><i class="fa fa-check"></i><b>2.5.1</b> Histograms</a></li>
<li class="chapter" data-level="2.5.2" data-path="descriptive.html"><a href="descriptive.html#stem"><i class="fa fa-check"></i><b>2.5.2</b> Stem-and-leaf plots</a></li>
<li class="chapter" data-level="2.5.3" data-path="descriptive.html"><a href="descriptive.html#dotplots"><i class="fa fa-check"></i><b>2.5.3</b> Dotplots</a></li>
<li class="chapter" data-level="2.5.4" data-path="descriptive.html"><a href="descriptive.html#boxplots"><i class="fa fa-check"></i><b>2.5.4</b> Boxplots</a></li>
<li class="chapter" data-level="2.5.5" data-path="descriptive.html"><a href="descriptive.html#barplots"><i class="fa fa-check"></i><b>2.5.5</b> Barplots</a></li>
<li class="chapter" data-level="2.5.6" data-path="descriptive.html"><a href="descriptive.html#times-series-plots"><i class="fa fa-check"></i><b>2.5.6</b> Times series plots</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="descriptive.html"><a href="descriptive.html#election"><i class="fa fa-check"></i><b>2.6</b> 2000 US Presidential Election</a></li>
<li class="chapter" data-level="2.7" data-path="descriptive.html"><a href="descriptive.html#graphs2"><i class="fa fa-check"></i><b>2.7</b> Graphs (2 variables)</a><ul>
<li class="chapter" data-level="2.7.1" data-path="descriptive.html"><a href="descriptive.html#scatter-plots"><i class="fa fa-check"></i><b>2.7.1</b> Scatter plots</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="descriptive.html"><a href="descriptive.html#transformation-of-data"><i class="fa fa-check"></i><b>2.8</b> Transformation of data</a><ul>
<li class="chapter" data-level="2.8.1" data-path="descriptive.html"><a href="descriptive.html#transsymmetry"><i class="fa fa-check"></i><b>2.8.1</b> Transformation to approximate symmetry</a></li>
<li class="chapter" data-level="2.8.2" data-path="descriptive.html"><a href="descriptive.html#straighten"><i class="fa fa-check"></i><b>2.8.2</b> Straightening scatter plots</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>3</b> Probability</a><ul>
<li class="chapter" data-level="3.1" data-path="probability.html"><a href="probability.html#sids"><i class="fa fa-check"></i><b>3.1</b> Misleading statistical evidence in cot death trials</a></li>
<li class="chapter" data-level="3.2" data-path="probability.html"><a href="probability.html#relative-frequency-definition-of-probability"><i class="fa fa-check"></i><b>3.2</b> Relative frequency definition of probability</a></li>
<li class="chapter" data-level="3.3" data-path="probability.html"><a href="probability.html#basic-properties-of-probability"><i class="fa fa-check"></i><b>3.3</b> Basic properties of probability</a></li>
<li class="chapter" data-level="3.4" data-path="probability.html"><a href="probability.html#conditional-probability"><i class="fa fa-check"></i><b>3.4</b> Conditional probability</a></li>
<li class="chapter" data-level="3.5" data-path="probability.html"><a href="probability.html#addition-rule-of-probability"><i class="fa fa-check"></i><b>3.5</b> Addition rule of probability</a><ul>
<li class="chapter" data-level="3.5.1" data-path="probability.html"><a href="probability.html#mutually-exclusive-events"><i class="fa fa-check"></i><b>3.5.1</b> Mutually exclusive events</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="probability.html"><a href="probability.html#multrule"><i class="fa fa-check"></i><b>3.6</b> Multiplication rule of probability</a></li>
<li class="chapter" data-level="3.7" data-path="probability.html"><a href="probability.html#indepevents"><i class="fa fa-check"></i><b>3.7</b> Independence of events</a><ul>
<li class="chapter" data-level="3.7.1" data-path="probability.html"><a href="probability.html#bloodindep"><i class="fa fa-check"></i><b>3.7.1</b> An example of independence</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="probability.html"><a href="probability.html#law-of-total-probability"><i class="fa fa-check"></i><b>3.8</b> Law of total probability</a></li>
<li class="chapter" data-level="3.9" data-path="probability.html"><a href="probability.html#bayes-theorem"><i class="fa fa-check"></i><b>3.9</b> Bayes’ theorem</a></li>
<li class="chapter" data-level="3.10" data-path="probability.html"><a href="probability.html#dna-identification-evidence"><i class="fa fa-check"></i><b>3.10</b> DNA identification evidence</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="rvs.html"><a href="rvs.html"><i class="fa fa-check"></i><b>4</b> Random variables</a><ul>
<li class="chapter" data-level="4.1" data-path="rvs.html"><a href="rvs.html#discrete"><i class="fa fa-check"></i><b>4.1</b> Discrete random variables</a></li>
<li class="chapter" data-level="4.2" data-path="rvs.html"><a href="rvs.html#continuous"><i class="fa fa-check"></i><b>4.2</b> Continuous random variables</a></li>
<li class="chapter" data-level="4.3" data-path="rvs.html"><a href="rvs.html#expectation"><i class="fa fa-check"></i><b>4.3</b> Expectation</a><ul>
<li class="chapter" data-level="4.3.1" data-path="rvs.html"><a href="rvs.html#expectation-of-a-discrete-random-variable"><i class="fa fa-check"></i><b>4.3.1</b> Expectation of a discrete random variable</a></li>
<li class="chapter" data-level="4.3.2" data-path="rvs.html"><a href="rvs.html#expectation-of-a-continuous-random-variable"><i class="fa fa-check"></i><b>4.3.2</b> Expectation of a continuous random variable</a></li>
<li class="chapter" data-level="4.3.3" data-path="rvs.html"><a href="rvs.html#properties-of-mathrmex"><i class="fa fa-check"></i><b>4.3.3</b> Properties of <span class="math inline">\(\mathrm{E}(X)\)</span></a></li>
<li class="chapter" data-level="4.3.4" data-path="rvs.html"><a href="rvs.html#the-expectation-of-gx"><i class="fa fa-check"></i><b>4.3.4</b> The expectation of <span class="math inline">\(g(X)\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="rvs.html"><a href="rvs.html#variance"><i class="fa fa-check"></i><b>4.4</b> Variance</a><ul>
<li class="chapter" data-level="4.4.1" data-path="rvs.html"><a href="rvs.html#variance-of-a-discrete-random-variable"><i class="fa fa-check"></i><b>4.4.1</b> Variance of a discrete random variable</a></li>
<li class="chapter" data-level="4.4.2" data-path="rvs.html"><a href="rvs.html#variance-of-a-continuous-random-variable"><i class="fa fa-check"></i><b>4.4.2</b> Variance of a continuous random variable</a></li>
<li class="chapter" data-level="4.4.3" data-path="rvs.html"><a href="rvs.html#variance-and-standard-deviation"><i class="fa fa-check"></i><b>4.4.3</b> Variance and standard deviation</a></li>
<li class="chapter" data-level="4.4.4" data-path="rvs.html"><a href="rvs.html#properties-of-mathrmvarx"><i class="fa fa-check"></i><b>4.4.4</b> Properties of <span class="math inline">\(\mathrm{var}(X)\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="rvs.html"><a href="rvs.html#locations"><i class="fa fa-check"></i><b>4.5</b> Other measures of location</a><ul>
<li class="chapter" data-level="4.5.1" data-path="rvs.html"><a href="rvs.html#the-median-of-a-random-variable"><i class="fa fa-check"></i><b>4.5.1</b> The median of a random variable</a></li>
<li class="chapter" data-level="4.5.2" data-path="rvs.html"><a href="rvs.html#the-mode-of-a-random-variable"><i class="fa fa-check"></i><b>4.5.2</b> The mode of a random variable</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="rvs.html"><a href="rvs.html#quantiles"><i class="fa fa-check"></i><b>4.6</b> Quantiles</a></li>
<li class="chapter" data-level="4.7" data-path="rvs.html"><a href="rvs.html#measures-of-shape"><i class="fa fa-check"></i><b>4.7</b> Measures of shape</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="simple.html"><a href="simple.html"><i class="fa fa-check"></i><b>5</b> Simple distributions</a><ul>
<li class="chapter" data-level="5.1" data-path="simple.html"><a href="simple.html#australian-births-data"><i class="fa fa-check"></i><b>5.1</b> Australian births data</a></li>
<li class="chapter" data-level="5.2" data-path="simple.html"><a href="simple.html#the-bernoulli-distribution"><i class="fa fa-check"></i><b>5.2</b> The Bernoulli distribution</a><ul>
<li class="chapter" data-level="5.2.1" data-path="simple.html"><a href="simple.html#summary-of-the-bernoullip-distribution"><i class="fa fa-check"></i><b>5.2.1</b> Summary of the Bernoulli(<span class="math inline">\(p\)</span>) distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="simple.html"><a href="simple.html#binomial"><i class="fa fa-check"></i><b>5.3</b> The binomial distribution</a><ul>
<li class="chapter" data-level="5.3.1" data-path="simple.html"><a href="simple.html#binominf"><i class="fa fa-check"></i><b>5.3.1</b> A brief look at statistical inference about <span class="math inline">\(p\)</span></a></li>
<li class="chapter" data-level="5.3.2" data-path="simple.html"><a href="simple.html#summary-of-the-binomialnp-distribution"><i class="fa fa-check"></i><b>5.3.2</b> Summary of the binomial(<span class="math inline">\(n,p\)</span>) distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="simple.html"><a href="simple.html#the-geometric-distribution"><i class="fa fa-check"></i><b>5.4</b> The geometric distribution</a><ul>
<li class="chapter" data-level="5.4.1" data-path="simple.html"><a href="simple.html#summary-of-the-geometricp-distribution"><i class="fa fa-check"></i><b>5.4.1</b> Summary of the geometric(<span class="math inline">\(p\)</span>) distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="simple.html"><a href="simple.html#Poisson"><i class="fa fa-check"></i><b>5.5</b> The Poisson distribution</a><ul>
<li class="chapter" data-level="5.5.1" data-path="simple.html"><a href="simple.html#summary-of-the-poissonmu-distribution"><i class="fa fa-check"></i><b>5.5.1</b> Summary of the Poisson(<span class="math inline">\(\mu\)</span>) distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="simple.html"><a href="simple.html#summary-of-these-discrete-distributions"><i class="fa fa-check"></i><b>5.6</b> Summary of these discrete distributions</a></li>
<li class="chapter" data-level="5.7" data-path="simple.html"><a href="simple.html#uniform"><i class="fa fa-check"></i><b>5.7</b> The uniform distribution</a><ul>
<li class="chapter" data-level="5.7.1" data-path="simple.html"><a href="simple.html#summary-of-the-uniformab-distribution"><i class="fa fa-check"></i><b>5.7.1</b> Summary of the uniform(<span class="math inline">\(a,b\)</span>) distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="simple.html"><a href="simple.html#exponential"><i class="fa fa-check"></i><b>5.8</b> The exponential distribution</a><ul>
<li class="chapter" data-level="5.8.1" data-path="simple.html"><a href="simple.html#summary-of-the-exponentiallambda-distribution"><i class="fa fa-check"></i><b>5.8.1</b> Summary of the exponential(<span class="math inline">\(\lambda\)</span>) distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="simple.html"><a href="simple.html#normal"><i class="fa fa-check"></i><b>5.9</b> The normal distribution</a><ul>
<li class="chapter" data-level="5.9.1" data-path="simple.html"><a href="simple.html#summary-of-the-mboxnmusigma2-distribution"><i class="fa fa-check"></i><b>5.9.1</b> Summary of the <span class="math inline">\(\mbox{N}(\mu,\sigma^2)\)</span> distribution</a></li>
<li class="chapter" data-level="5.9.2" data-path="simple.html"><a href="simple.html#the-standard-normal-disribution"><i class="fa fa-check"></i><b>5.9.2</b> The standard normal disribution</a></li>
<li class="chapter" data-level="5.9.3" data-path="simple.html"><a href="simple.html#evaluating-the-normal-c.d.f.-and-quantiles"><i class="fa fa-check"></i><b>5.9.3</b> Evaluating the normal c.d.f. and quantiles</a></li>
<li class="chapter" data-level="5.9.4" data-path="simple.html"><a href="simple.html#interpretation-of-sigma"><i class="fa fa-check"></i><b>5.9.4</b> Interpretation of <span class="math inline">\(\sigma\)</span></a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="simple.html"><a href="simple.html#summary-of-these-continuous-distributions"><i class="fa fa-check"></i><b>5.10</b> Summary of these continuous distributions</a></li>
<li class="chapter" data-level="5.11" data-path="simple.html"><a href="simple.html#qq"><i class="fa fa-check"></i><b>5.11</b> QQ plots</a><ul>
<li class="chapter" data-level="5.11.1" data-path="simple.html"><a href="simple.html#normal-qq-plots"><i class="fa fa-check"></i><b>5.11.1</b> Normal QQ plots</a></li>
<li class="chapter" data-level="5.11.2" data-path="simple.html"><a href="simple.html#uniform-qq-plots"><i class="fa fa-check"></i><b>5.11.2</b> Uniform QQ plots</a></li>
<li class="chapter" data-level="5.11.3" data-path="simple.html"><a href="simple.html#exponential-qq-plots"><i class="fa fa-check"></i><b>5.11.3</b> Exponential QQ plots</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>6</b> Statistical Inference</a><ul>
<li class="chapter" data-level="6.1" data-path="inference.html"><a href="inference.html#the-story-so-far"><i class="fa fa-check"></i><b>6.1</b> The story so far</a></li>
<li class="chapter" data-level="6.2" data-path="inference.html"><a href="inference.html#sample-and-populations"><i class="fa fa-check"></i><b>6.2</b> Sample and populations</a></li>
<li class="chapter" data-level="6.3" data-path="inference.html"><a href="inference.html#probability-models"><i class="fa fa-check"></i><b>6.3</b> Probability models</a></li>
<li class="chapter" data-level="6.4" data-path="inference.html"><a href="inference.html#fitting-models"><i class="fa fa-check"></i><b>6.4</b> Fitting models</a></li>
<li class="chapter" data-level="6.5" data-path="inference.html"><a href="inference.html#uncertainty-in-estimation"><i class="fa fa-check"></i><b>6.5</b> Uncertainty in estimation</a><ul>
<li class="chapter" data-level="6.5.1" data-path="inference.html"><a href="inference.html#simulation-coin-tossing-example"><i class="fa fa-check"></i><b>6.5.1</b> Simulation: coin-tossing example</a></li>
<li class="chapter" data-level="6.5.2" data-path="inference.html"><a href="inference.html#simnorm"><i class="fa fa-check"></i><b>6.5.2</b> Simulation: estimating the parameters of a normal distribution</a></li>
<li class="chapter" data-level="6.5.3" data-path="inference.html"><a href="inference.html#simexp"><i class="fa fa-check"></i><b>6.5.3</b> Simulation: estimating the parameters of an exponential distribution</a></li>
<li class="chapter" data-level="6.5.4" data-path="inference.html"><a href="inference.html#central-limit-theorem"><i class="fa fa-check"></i><b>6.5.4</b> Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="inference.html"><a href="inference.html#good"><i class="fa fa-check"></i><b>6.6</b> Properties of estimators</a><ul>
<li class="chapter" data-level="6.6.1" data-path="inference.html"><a href="inference.html#bias"><i class="fa fa-check"></i><b>6.6.1</b> Bias</a></li>
<li class="chapter" data-level="6.6.2" data-path="inference.html"><a href="inference.html#varianceofestimator"><i class="fa fa-check"></i><b>6.6.2</b> Variance</a></li>
<li class="chapter" data-level="6.6.3" data-path="inference.html"><a href="inference.html#mean-squared-error-mse"><i class="fa fa-check"></i><b>6.6.3</b> Mean squared error (MSE)</a></li>
<li class="chapter" data-level="6.6.4" data-path="inference.html"><a href="inference.html#standard-error"><i class="fa fa-check"></i><b>6.6.4</b> Standard error</a></li>
<li class="chapter" data-level="6.6.5" data-path="inference.html"><a href="inference.html#consistency"><i class="fa fa-check"></i><b>6.6.5</b> Consistency</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="inference.html"><a href="inference.html#assessing-goodness-of-fit"><i class="fa fa-check"></i><b>6.7</b> Assessing goodness-of-fit</a><ul>
<li class="chapter" data-level="6.7.1" data-path="inference.html"><a href="inference.html#residuals"><i class="fa fa-check"></i><b>6.7.1</b> Residuals</a></li>
<li class="chapter" data-level="6.7.2" data-path="inference.html"><a href="inference.html#standardised-residuals"><i class="fa fa-check"></i><b>6.7.2</b> Standardised residuals</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="contingency.html"><a href="contingency.html"><i class="fa fa-check"></i><b>7</b> Contingency tables</a><ul>
<li class="chapter" data-level="7.1" data-path="contingency.html"><a href="contingency.html#way2"><i class="fa fa-check"></i><b>7.1</b> 2-way contingency tables</a><ul>
<li class="chapter" data-level="7.1.1" data-path="contingency.html"><a href="contingency.html#indep"><i class="fa fa-check"></i><b>7.1.1</b> Independence</a></li>
<li class="chapter" data-level="7.1.2" data-path="contingency.html"><a href="contingency.html#compprob"><i class="fa fa-check"></i><b>7.1.2</b> Comparing probabilities</a></li>
<li class="chapter" data-level="7.1.3" data-path="contingency.html"><a href="contingency.html#measures"><i class="fa fa-check"></i><b>7.1.3</b> Measures of association</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="contingency.html"><a href="contingency.html#way3"><i class="fa fa-check"></i><b>7.2</b> 3-way contingency tables</a><ul>
<li class="chapter" data-level="7.2.1" data-path="contingency.html"><a href="contingency.html#mutual-independence"><i class="fa fa-check"></i><b>7.2.1</b> Mutual independence</a></li>
<li class="chapter" data-level="7.2.2" data-path="contingency.html"><a href="contingency.html#marginal-independence"><i class="fa fa-check"></i><b>7.2.2</b> Marginal independence</a></li>
<li class="chapter" data-level="7.2.3" data-path="contingency.html"><a href="contingency.html#conditional-independence"><i class="fa fa-check"></i><b>7.2.3</b> Conditional independence</a></li>
<li class="chapter" data-level="7.2.4" data-path="contingency.html"><a href="contingency.html#confounding-variables"><i class="fa fa-check"></i><b>7.2.4</b> Confounding variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="linreg.html"><a href="linreg.html"><i class="fa fa-check"></i><b>8</b> Linear regression</a><ul>
<li class="chapter" data-level="8.1" data-path="linreg.html"><a href="linreg.html#simple-linear-regression"><i class="fa fa-check"></i><b>8.1</b> Simple linear regression</a><ul>
<li class="chapter" data-level="8.1.1" data-path="linreg.html"><a href="linreg.html#simple-linear-regression-model"><i class="fa fa-check"></i><b>8.1.1</b> Simple linear regression model</a></li>
<li class="chapter" data-level="8.1.2" data-path="linreg.html"><a href="linreg.html#least-squares-estimation-of-alpha-and-beta"><i class="fa fa-check"></i><b>8.1.2</b> Least squares estimation of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span></a></li>
<li class="chapter" data-level="8.1.3" data-path="linreg.html"><a href="linreg.html#least-squares-fitting-to-hubbles-data"><i class="fa fa-check"></i><b>8.1.3</b> Least squares fitting to Hubble’s data</a></li>
<li class="chapter" data-level="8.1.4" data-path="linreg.html"><a href="linreg.html#normal-linear-regression-model"><i class="fa fa-check"></i><b>8.1.4</b> Normal linear regression model</a></li>
<li class="chapter" data-level="8.1.5" data-path="linreg.html"><a href="linreg.html#summary-of-the-assumptions-of-a-normal-linear-regression-model"><i class="fa fa-check"></i><b>8.1.5</b> Summary of the assumptions of a (normal) linear regression model</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="linreg.html"><a href="linreg.html#looking-at-scatter-plots"><i class="fa fa-check"></i><b>8.2</b> Looking at scatter plots</a></li>
<li class="chapter" data-level="8.3" data-path="linreg.html"><a href="linreg.html#model-checking"><i class="fa fa-check"></i><b>8.3</b> Model checking</a><ul>
<li class="chapter" data-level="8.3.1" data-path="linreg.html"><a href="linreg.html#outliers"><i class="fa fa-check"></i><b>8.3.1</b> Outliers and influential observations</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="linreg.html"><a href="linreg.html#linregtrans"><i class="fa fa-check"></i><b>8.4</b> Use of transformations</a></li>
<li class="chapter" data-level="8.5" data-path="linreg.html"><a href="linreg.html#over-fitting"><i class="fa fa-check"></i><b>8.5</b> Over-fitting</a></li>
<li class="chapter" data-level="8.6" data-path="linreg.html"><a href="linreg.html#other-aspects-of-regression"><i class="fa fa-check"></i><b>8.6</b> Other aspects of regression</a></li>
<li class="chapter" data-level="8.7" data-path="linreg.html"><a href="linreg.html#uncertainty-in-parameter-estimates"><i class="fa fa-check"></i><b>8.7</b> Uncertainty in parameter estimates</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="correlationchapter.html"><a href="correlationchapter.html"><i class="fa fa-check"></i><b>9</b> Correlation</a><ul>
<li class="chapter" data-level="9.1" data-path="correlationchapter.html"><a href="correlationchapter.html#correlation-a-measure-of-linear-association"><i class="fa fa-check"></i><b>9.1</b> Correlation: a measure of linear association</a></li>
<li class="chapter" data-level="9.2" data-path="correlationchapter.html"><a href="correlationchapter.html#covariance-and-correlation"><i class="fa fa-check"></i><b>9.2</b> Covariance and correlation</a></li>
<li class="chapter" data-level="9.3" data-path="correlationchapter.html"><a href="correlationchapter.html#use-and-misuse-of-correlation"><i class="fa fa-check"></i><b>9.3</b> Use and misuse of correlation</a><ul>
<li class="chapter" data-level="9.3.1" data-path="correlationchapter.html"><a href="correlationchapter.html#correxamples"><i class="fa fa-check"></i><b>9.3.1</b> Examples of correlations of different strengths</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="a-general-strategy-for-statistical-modelling.html"><a href="a-general-strategy-for-statistical-modelling.html"><i class="fa fa-check"></i><b>10</b> A general strategy for statistical modelling</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">STAT0002 Introduction to Probability and Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="linreg" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Linear regression</h1>
<p>In this chapter, and Chapter <a href="correlationchapter.html#correlationchapter">9</a>, we examine the relationship between 2 continuous variables. First we consider <strong>regression</strong> problems, where the distribution of a <strong>response variable</strong> <span class="math inline">\(Y\)</span> is thought to be dependent on the value of an <strong>explanatory variable</strong> <span class="math inline">\(X\)</span>. Possible aims are (a) to understand the relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>, or (b) to predict <span class="math inline">\(Y\)</span> from the value of <span class="math inline">\(X\)</span>. We examine the conditional distribution of the random variable <span class="math inline">\(Y\)</span> given that <span class="math inline">\(X=x\)</span>, that is, <span class="math inline">\(Y \mid X=x\)</span>. In particular, we study a <strong>simple linear regression model</strong> in which the conditional mean <span class="math inline">\(\mbox{E}(Y \mid X=x)\)</span> of <span class="math inline">\(Y\)</span> given that <span class="math inline">\(X=x\)</span> is assumed to be a linear function of <span class="math inline">\(x\)</span> and the conditional variance <span class="math inline">\(\mbox{var}(Y \mid X=x)\)</span> of <span class="math inline">\(Y\)</span> is assumed to be constant. That is,
<span class="math display">\[ \mbox{E}(Y \mid X=x) = \alpha+\beta\,x \qquad \mbox{and} \qquad \mbox{var}(Y \mid X=x)=\sigma^2, \]</span>
for some constants <span class="math inline">\(\alpha, \beta\)</span> and <span class="math inline">\(\sigma^2\)</span>.</p>
<p>In many cases it is clear which variable should be the response variable <span class="math inline">\(Y\)</span> and which should be the explanatory variable <span class="math inline">\(X\)</span>. For example,</p>
<ul>
<li>If changes in <span class="math inline">\(x\)</span> cause changes in <span class="math inline">\(Y\)</span>, so that the direction of dependence is clear. For example, <span class="math inline">\(x\)</span>=river depth influencing <span class="math inline">\(Y\)</span>=flow rate.</li>
<li>If the values of <span class="math inline">\(X\)</span> are controlled by an experimenter and then the value of <span class="math inline">\(Y\)</span> is observed. For example, <span class="math inline">\(x\)</span>=dosage of drug and <span class="math inline">\(Y\)</span>=reduction in blood pressure. This is sometimes called <strong>regression sampling</strong>.</li>
<li>If we wish to predict <span class="math inline">\(Y\)</span> using <span class="math inline">\(x\)</span>. For example, <span class="math inline">\(x\)</span>=share value today and <span class="math inline">\(Y\)</span>=share value tomorrow.</li>
</ul>
<p>In a related, but different, problem the 2 random variables <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> are treated symmetrically. The question is how these random variables are associated. A measure of strength of <strong>linear</strong> association between 2 variables is given by a <strong>correlation coefficient</strong> (see Chapter <a href="correlationchapter.html#correlationchapter">9</a>).</p>
<p>Regression answers the question “How does the conditional distribution of the random variable <span class="math inline">\(Y\)</span> depend on the value <span class="math inline">\(x\)</span> of <span class="math inline">\(X\)</span>?”. Correlation answers the question “How strong is any <strong>linear</strong> association between the random variables <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>?”. In a regression problem we assume that the <span class="math inline">\(Y\)</span> values are random variables (that is, subject to random variability) but the <span class="math inline">\(x\)</span> values are not. When using a correlation coefficient we assume that both <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are random variables.</p>
<div id="simple-linear-regression" class="section level2">
<h2><span class="header-section-number">8.1</span> Simple linear regression</h2>
<p>We use a small set of data, and some physical theory, to estimate the age of the Universe! In 1929 the famous astronomer Edwin Hubble published a paper (<span class="citation">Hubble (<a href="#ref-Hubble1929" role="doc-biblioref">1929</a>)</span>) reporting a relationship he had observed (using a telescope) between the distance of a nebula (a star) from the Earth and the velocity (the <strong>recession velocity</strong>) with which it was moving away from the Earth. Hubble’s data are given in Table <a href="linreg.html#fig:tablehubble">8.1</a>. A scatter plot of distance against velocity is given in Figure <a href="linreg.html#fig:hubblescatter">8.2</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:tablehubble"></span>
<img src="images/hubbletable.PNG" alt="Hubble's data. 1 MPc = 1 megaparsec = $3.086 \times 10^{19}$ km. A megaparsec is a long distance: the distance from the Earth to the Sun is 'only' $1.5 \times 10^8$ km." width="75%" />
<p class="caption">
Figure 8.1: Hubble’s data. 1 MPc = 1 megaparsec = <span class="math inline">\(3.086 \times 10^{19}\)</span> km. A megaparsec is a long distance: the distance from the Earth to the Sun is ‘only’ <span class="math inline">\(1.5 \times 10^8\)</span> km.
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:hubblescatter"></span>
<img src="images/hubble_scatter.png" alt="Scatter plot of distance against recession velocity." width="75%" />
<p class="caption">
Figure 8.2: Scatter plot of distance against recession velocity.
</p>
</div>
<p>It appears that distance and velocity are <strong>positively associated</strong>: the values of distance tend to be larger for nebulae with large velocities than for nebulae with smaller velocities. Also, this relationship appears to be approximately linear, at least over the range of velocities available.</p>
<p>Scientists then wondered how the positive linear association between distance and velocity could have arisen. The result was ‘Big Bang’ theory. This theory proposes that the Universe started with a Big Bang at a single point in space a very long time ago, scattering material around the surface of an ever-expanding sphere. If Big Bang theory is correct then the relationship between distance (<span class="math inline">\(Y\)</span>) and recession velocity (<span class="math inline">\(X\)</span>) should be of the form
<span class="math display">\[ Y = T X, \]</span>
where <span class="math inline">\(T\)</span> is the age of the Universe when the observations were made. This is called Hubble’s Law. In other words, distance, <span class="math inline">\(Y\)</span>, should depend linearly on velocity, <span class="math inline">\(X\)</span>. <span class="math inline">\(H=1/T\)</span> is called <strong>Hubble’s constant</strong>.</p>
<p>The points in Figure <a href="linreg.html#fig:hubblescatter">8.2</a> do not lie exactly on a straight line, partly because the values of distance are not exact: they include measurement error. Also, there may have been astronomical events since the Big Bang which have weakened further the supposed linear relationships between distance and velocity. If we look at nebulae with the same value, <span class="math inline">\(x\)</span>, of velocity the measured value of distance, <span class="math inline">\(Y\)</span>, varies from one nebulae to another. For example, the 4 nebulae with velocities of 500 km/sec have have distances 0.9, 1.1, 1.4 and 2.0 MPc. So, for a given value of velocity there is variability in their distances from the Earth. Therefore, <span class="math inline">\(Y \mid X=x\)</span> is a random variable, with conditional mean <span class="math inline">\(\mbox{E}(Y \mid X=x)\)</span> and conditional variance <span class="math inline">\(\mbox{var}(Y \mid X=x)\)</span>.</p>
<p>In Figure <a href="linreg.html#fig:hubblescatter">8.2</a> it looks possible that there is a straight line relationship between <span class="math inline">\(\mbox{E}(Y \mid X=x)\)</span> and <span class="math inline">\(x\)</span>. Therefore we consider fitting a simple linear regression model of <span class="math inline">\(Y\)</span> on <span class="math inline">\(x\)</span>. You could think of this as a way to draw a ‘line of best fit’ through the points in Figure <a href="linreg.html#fig:hubblescatter">8.2</a>.</p>
<div id="simple-linear-regression-model" class="section level3">
<h3><span class="header-section-number">8.1.1</span> Simple linear regression model</h3>
<p>We assume that
<span class="math display" id="eq:regeqn">\[\begin{equation}
Y_i = \alpha + \beta\,x_i +  \epsilon_i, \qquad i=1,\ldots,n, 
\tag{8.1}
\end{equation}\]</span>
where <span class="math inline">\(\epsilon_i, i=1, \ldots, n\)</span> are error terms, representing random ‘noise’. The <span class="math inline">\(\alpha + \beta\,x_i\)</span> part of the model is the <strong>systematic</strong> part. The <span class="math inline">\(\epsilon_i\)</span> is the <strong>random</strong> part of the model. It is assumed that
<span class="math display">\[  \mbox{E}( \epsilon_i)=0, \qquad \mbox{and} \qquad  \mbox{var}( \epsilon_i)=\sigma^2, \]</span>
and that <span class="math inline">\(\epsilon_1, \ldots, \epsilon_n\)</span> are <strong>uncorrelated</strong>. We will study <strong>correlation</strong> in the next section. It is a measure of the degree of <strong>linear</strong> association between two random variables. Uncorrelated random variables have no linear association.</p>
<p>Another way to write down this model is, for <span class="math inline">\(i=1,\ldots,n\)</span>,
<span class="math display">\[  \mbox{E}(Y_i \mid X=x_i) = \alpha+\beta\,x_i, \qquad\quad  (\mbox{straight line relationship}), \]</span>
and
<span class="math display">\[ \qquad  \mbox{var}(Y_i \mid X=x_i)=\sigma^2, \qquad\quad (\mbox{constant variance}), \]</span>
where, given the values <span class="math inline">\(x_1,\ldots,x_n\)</span>, the random variables <span class="math inline">\(Y_1, \ldots, Y_n\)</span> are uncorrelated.</p>
<p>Figure <a href="linreg.html#fig:regschematic">8.3</a> shows how the conditional distribution
of <span class="math inline">\(Y\)</span> is assumed to vary with the value of <span class="math inline">\(x\)</span>.</p>
<div class="figure" style="text-align: center"><span id="fig:regschematic"></span>
<img src="images/regschematic.png" alt="Conditional distribution of $Y$ given $X=x$ for a linear regression model." width="75%" />
<p class="caption">
Figure 8.3: Conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X=x\)</span> for a linear regression model.
</p>
</div>
<p><strong>Interpretation of parameters</strong></p>
<ul>
<li>Intercept: <span class="math inline">\(\alpha\)</span>. The expected value (mean) of <span class="math inline">\(Y\)</span> when <span class="math inline">\(X=0\)</span>, that is, <span class="math inline">\(\mbox{E}(Y~|~X=0)\)</span>.</li>
<li>Gradient or slope: <span class="math inline">\(\beta\)</span>. The amount by which the mean of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X=x\)</span>, <span class="math inline">\(\mbox{E}(Y~|~X=x)\)</span>,
increases when <span class="math inline">\(x\)</span> is increased by 1 unit. That is,
<span class="math display">\[ \beta =  \mbox{E}(Y~|~X=x+1)- \mbox{E}(Y~|~X=x). \]</span></li>
<li>Error variance: <span class="math inline">\(\sigma^2\)</span>. The variability of the response about the linear regression line (in the vertical direction).</li>
</ul>
</div>
<div id="least-squares-estimation-of-alpha-and-beta" class="section level3">
<h3><span class="header-section-number">8.1.2</span> Least squares estimation of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span></h3>
<p>Suppose that we have paired data <span class="math inline">\((x_1,y_1), \ldots, (x_n,y_n)\)</span>. How can we fit a simple linear regression model to these data? Initially, our aim is to use estimators <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(\hat{\beta}\)</span> of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> to produce an estimated regression line
<span class="math display">\[ y= \hat{\alpha}+\hat{\beta}\,x. \]</span>
There are many possible estimators of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> that could be used. A standard approach, which produces estimators with some nice properties is <strong>least squares estimation</strong>. Firstly, we rearrange equation <a href="linreg.html#eq:regeqn">(8.1)</a> to define <strong>residuals</strong>
<span class="math display">\[ r_i = Y_i-(\hat{\alpha}+\hat{\beta}\,x_i) = Y_i-\hat{Y}_i, \qquad i=1,\ldots,n, \]</span>
the differences between the observed values <span class="math inline">\(Y_i, i = 1, \ldots, n\)</span> and the <strong>fitted values</strong> <span class="math inline">\(\hat{Y}_i=\hat{\alpha}+\hat{\beta}\,x_i\)</span>, <span class="math inline">\(i = 1, \ldots, n\)</span> given by the estimated regression line.</p>
<p>The least squares estimators have the property that they minimise the sum of squared residuals:
<span class="math display">\[ \sum_{i=1}^n \left(Y_i-\hat{\alpha}-\hat{\beta}\,x_i\right)^2. \]</span></p>
<p>It is possible to do this by hand to give
<span class="math display">\[\begin{equation}
\hat{\beta}=\frac{\displaystyle\sum_{i=1}^n \left(x_i- \overline{x}\right)\,\left(Y_i- \overline{Y}\right)}
{\displaystyle\sum_{i=1}^n \left(x_i- \overline{x}\right)^2} = \frac{C_{xY}}{C_{xx}} \qquad
\mbox{and} \qquad  
\hat{\alpha}=  \overline{Y} - \hat{\beta}\, \overline{x}, 
\end{equation}\]</span>
where <span class="math inline">\(\overline{Y} = (1/n)\sum_{i=1}^n Y_i\)</span> and <span class="math inline">\(\overline{x} = (1/n)\sum_{i=1}^n x_i\)</span>. Note: <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(\hat{\beta}\)</span> are each linear combinations of<span class="math inline">\(Y_1,\ldots,Y_n\)</span>.</p>
<p>For a given set of data the minimised sum of squared residuals is called the <strong>residual sum of squares (RSS)</strong>, that is,
<span class="math display">\[ RSS = \sum_{i=1}^n \left(y_i-\hat{\alpha}-\hat{\beta}\,x_i\right)^2 = \sum_{i=1}^n r_i^{\,\,2} 
= \sum_{i=1}^n \left(y_i-\hat{y}_i\right)^2. \]</span></p>
<p><strong>Estimating <span class="math inline">\(\sigma^2\)</span></strong>. There is one remaining parameter to estimate; the error variance <span class="math inline">\(\sigma^2\)</span>. The usual estimator is
<span class="math display">\[\begin{equation}
\hat{\sigma}^2 = \frac{RSS}{n-2}.
\end{equation}\]</span>
An estimate of <span class="math inline">\(\sigma^2\)</span> is important because it quantifies how much variability there is about the assumed straight line relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(x\)</span>.</p>
<p><strong>Properties of estimators.</strong> It can be shown that
<span class="math display">\[  \mbox{E}(\hat{\alpha})=\alpha, \quad  \mbox{E}(\hat{\beta})=\beta, \quad  \mbox{E}(\hat{\sigma}^2)=\sigma^2, \]</span>
that is, these estimators are unbiased for the parameters they are intended to estimate. It can also be shown that the least squares estimators <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(\hat{\beta}\)</span> have the smallest possible variances of all unbiased estimators of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> which are linear combinations of the response <span class="math inline">\(Y_1,\ldots,Y_n\)</span>.</p>
<p><strong>Coefficient of determination.</strong> We may wish to quantify how much of the variability in the responses <span class="math inline">\(Y_1,\ldots,Y_n\)</span> is explained by the values <span class="math inline">\(x_1,\ldots,x_n\)</span> of the explanatory variable. To do this we can compare the variance of the residuals <span class="math inline">\(\{r_i\}\)</span> with the variance of the original observations <span class="math inline">\(\{Y_i\}\)</span>, producing the <strong>coefficient of determination</strong>, <span class="math inline">\(R^2\)</span>, given by
<span class="math display">\[ R^2 = 1- \frac{RSS}{\displaystyle\sum_{i=1}^n \left(Y_i-\bar{Y}\right)^2} 
= 1-\frac{\mbox{variability in $Y$ not explained by $x$}}
{\mbox{total variability of $Y$ about $\bar{Y}$}}, \]</span>
where <span class="math inline">\(0 \leq R^2 \leq 1\)</span>: <span class="math inline">\(R^2=1\)</span> indicates a perfect fit; <span class="math inline">\(R^2=0\)</span> indicates that none of the variability in <span class="math inline">\(Y_1,\ldots,Y_n\)</span> is explained by <span class="math inline">\(x_1,\ldots,x_n\)</span>, producing a horizontal regression line (<span class="math inline">\(\hat{\beta}=0\)</span>). The value of <span class="math inline">\(R^2\)</span>, perhaps expressed as a percentage, is often quoted when a simple linear regression model is fitted. This gives an estimate of the percentage of variability in <span class="math inline">\(Y\)</span> which is explained by <span class="math inline">\(x\)</span>.</p>
</div>
<div id="least-squares-fitting-to-hubbles-data" class="section level3">
<h3><span class="header-section-number">8.1.3</span> Least squares fitting to Hubble’s data</h3>
<p>Figures @ref(fig:hubble_fit_flat), @ref(fig:hubble_fit_origin) and @ref(fig:hubble_fit) show least squares regression lines under 3 different models:</p>
<ul>
<li>Model 1. <span class="math inline">\(Y\)</span> does not depend on <span class="math inline">\(X\)</span>, so that</li>
</ul>
<p><span class="math display">\[ Y_i = \alpha_1 + \epsilon_i, \qquad i=1,\ldots, n. \]</span></p>
<ul>
<li>Model 2. <span class="math inline">\(Y\)</span> depends on <span class="math inline">\(X\)</span> according to Hubble’s law, so that</li>
</ul>
<p><span class="math display">\[ Y_i = \beta_2\,x_i + \epsilon_i, \qquad i=1,\ldots, n, \]</span></p>
<p>where <span class="math inline">\(\beta=T\)</span> is the age of the Universe.</p>
<ul>
<li>Model 3. <span class="math inline">\(Y\)</span> depends on <span class="math inline">\(X\)</span> according to the full linear regression model</li>
</ul>
<p><span class="math display">\[ Y_i = \alpha_3+\beta_3\,x_i + \epsilon_i, \qquad i=1,\ldots, n. \]</span></p>
<div class="figure" style="text-align: center"><span id="fig:hubblefitflat"></span>
<img src="images/hubble_fit_flat.png" alt="Scatter plot of distance against recession velocity, with least squares fit of a horizontal line." width="75%" />
<p class="caption">
Figure 8.4: Scatter plot of distance against recession velocity, with least squares fit of a horizontal line.
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:hubblefitorigin"></span>
<img src="images/hubble_fit_origin.png" alt="Scatter plot of distance against recession velocity, with least squares fit of a line through the origin." width="75%" />
<p class="caption">
Figure 8.5: Scatter plot of distance against recession velocity, with least squares fit of a line through the origin.
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:hubblefit"></span>
<img src="images/hubble_fit.png" alt="Scatter plot of distance against recession velocity, with least squares fit of an unconstrained line." width="75%" />
<p class="caption">
Figure 8.6: Scatter plot of distance against recession velocity, with least squares fit of an unconstrained line.
</p>
</div>
<p>These figures also shows the sizes of the residuals and the residual sums of squares <span class="math inline">\(RSS\)</span>. From the plots, and the relative sizes of <span class="math inline">\(RSS\)</span>, it seems clear that velocity <span class="math inline">\(x\)</span> explains some of the variability in the values of distance <span class="math inline">\(Y\)</span>. The <span class="math inline">\(RSS\)</span>, <span class="math inline">\(RSS_3\)</span>, of model 3 is smaller than the <span class="math inline">\(RSS\)</span>, <span class="math inline">\(RSS_2\)</span>, of model 2. It is impossible that <span class="math inline">\(RSS_3 &gt; RSS_2\)</span>.</p>
<p>A key question is whether <span class="math inline">\(RSS_3\)</span> is so much smaller than <span class="math inline">\(RSS_2\)</span> that we would choose model 3 over model 2, which is a question that is considered in STAT0003. Model 2 is an example of <strong>regression through the origin</strong>, where it is assumed that the intercept equals 0. We should only fit this kind of model if we have a good reason to. Here Hubble’s Law gives us a good reason. Note that for a regression through the origin, we use <span class="math inline">\(\hat{\sigma}^2 = RSS/(n-1)\)</span>.</p>
<p>Since we want to estimate the age of the Universe, we use the estimated regression line for model 2:
<span class="math display">\[ y = 0.00192\,x.  \]</span>
Therefore, the estimated age of the Universe is given by
<span class="math display">\[ \hat{T}=0.00192 \mbox{~Mpc/km/sec}. \]</span>
To clarify the units of <span class="math inline">\(\hat{T}\)</span> we need to convert MPcs to kms by multiplying by <span class="math inline">\(3.086 \times 10^{19}\)</span>. This gives <span class="math inline">\(\hat{T}\)</span> in seconds. We convert to years by dividing by <span class="math inline">\(60 \times 60 \times 24 \times 365.25\)</span>:
<span class="math display">\[ \hat{T}
= 0.00192 \times \frac{3.086 \times 10^{19}}{60 \times 60 \times 24 \times 365.25} 
\approx 2 \mbox{ billion years}. \]</span></p>
<p><strong>Update</strong></p>
<p>Since Hubble’s work, physicists have obtained more data in order to obtain better estimates of distances of nebulae from the Earth and hence the age of the Universe. Figure <a href="linreg.html#fig:hubblescatternew">8.7</a> shows an example of these data (<span class="citation">Freedman et al. (<a href="#ref-Freedman2001" role="doc-biblioref">2001</a>)</span>). Using more powerful telescopes, it has been possible to estimate the distances of nebulae which are further from the Earth. Having a wider range of distances gives a better (smaller variance) estimator of the age of the Universe.</p>
<div class="figure" style="text-align: center"><span id="fig:hubblescatternew"></span>
<img src="images/hubble_scatter_new.png" alt="Scatter plots of new 'Hubble' data with fitted regression through the origin." width="75%" />
<p class="caption">
Figure 8.7: Scatter plots of new ‘Hubble’ data with fitted regression through the origin.
</p>
</div>
<p>Using these data we obtain <span class="math inline">\(\hat{T}=0.0123\)</span> which gives
<span class="math display">\[ 
\hat{T} = 
0.0123 \times \frac{3.086 \times 10^{19}}{60 \times 60 \times 24 \times 365.25} \approx 12 \mbox{ billion years}. \]</span>
This estimate agrees more closely with current scientific understanding of the age of the Universe than Hubble original estimate.</p>
</div>
<div id="normal-linear-regression-model" class="section level3">
<h3><span class="header-section-number">8.1.4</span> Normal linear regression model</h3>
<p>It is common to make the extra assumption that the errors are normally distributed:
<span class="math display">\[ \epsilon_i \sim N(0,\sigma^2), \qquad i=1,\ldots,n. \]</span>
In other words
<span class="math display">\[ Y_i \mid X=x_i \,\stackrel{{\rm indep}}{\sim}\, N(\alpha+\beta\,x_i,\sigma^2), \qquad i=1,\ldots,n. \]</span>
These results are used to enable us to (a) decide whether the explanatory variable <span class="math inline">\(x\)</span> is needed in the model, and (b) produce interval estimates of <span class="math inline">\(\alpha, \beta, \sigma^2\)</span> and for predictions made from the model.</p>
</div>
<div id="summary-of-the-assumptions-of-a-normal-linear-regression-model" class="section level3">
<h3><span class="header-section-number">8.1.5</span> Summary of the assumptions of a (normal) linear regression model</h3>
<ol style="list-style-type: decimal">
<li><strong>Linearity</strong>: the conditional mean of <span class="math inline">\(Y\)</span> given <span class="math inline">\(x\)</span> is a linear function of <span class="math inline">\(x\)</span>.</li>
<li><strong>Constant error variance</strong>: the variability of <span class="math inline">\(Y\)</span> is the same for all values of <span class="math inline">\(x\)</span>.</li>
<li><strong>Uncorrelatedness of errors</strong>: the errors are not linearly associated.</li>
<li><strong>Normality of errors</strong>: for a given value of <span class="math inline">\(x\)</span>, <span class="math inline">\(Y\)</span> has a normal distribution.</li>
</ol>
<p><strong>Uncorrelatedness</strong> and <strong>independence</strong> are related concepts.<br />
If two random variables are independent then they are uncorrelated, that is, independence implies lack of correlation. However, the reverse is not true: two random variables can be uncorrelated but <strong>not</strong> independent (see Section <a href="correlationchapter.html#correxamples">9.3.1</a>, that is, lack of correlation does <strong>not</strong> imply independence. The only exception to this is the (multivariate) normal distribution: for example, if two (jointly) normal random variables are uncorrelated then they are independent. This explains why it is common for an alternative assumption 3. to be used:</p>
<ol start="3" style="list-style-type: decimal">
<li><strong>Independence of errors</strong>: knowledge that one response <span class="math inline">\(Y_i\)</span> is larger than expected based on the model does not give us information about whether a different response <span class="math inline">\(Y_j\)</span> is larger (or smaller) than expected.</li>
</ol>
<p>Notice that, even in the normal linear regression model, we have not made any assumption about the distribution of the <span class="math inline">\(x\)</span>s. In some studies the values of <span class="math inline">\(x\)</span> are chosen by an experimenter, that is, they are not random at all.</p>
</div>
</div>
<div id="looking-at-scatter-plots" class="section level2">
<h2><span class="header-section-number">8.2</span> Looking at scatter plots</h2>
</div>
<div id="model-checking" class="section level2">
<h2><span class="header-section-number">8.3</span> Model checking</h2>
<div id="outliers" class="section level3">
<h3><span class="header-section-number">8.3.1</span> Outliers and influential observations</h3>
</div>
</div>
<div id="linregtrans" class="section level2">
<h2><span class="header-section-number">8.4</span> Use of transformations</h2>
</div>
<div id="over-fitting" class="section level2">
<h2><span class="header-section-number">8.5</span> Over-fitting</h2>
</div>
<div id="other-aspects-of-regression" class="section level2">
<h2><span class="header-section-number">8.6</span> Other aspects of regression</h2>
</div>
<div id="uncertainty-in-parameter-estimates" class="section level2">
<h2><span class="header-section-number">8.7</span> Uncertainty in parameter estimates</h2>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Freedman2001">
<p>Freedman, Wendy L., Barry F. Madore, Brad K. Gibson, Laura Ferrarese, Daniel D. Kelson, Shoko Sakai, Jeremy R. Mould, et al. 2001. “Final Results from theHubble Space TelescopeKey Project to Measure the Hubble Constant.” <em>The Astrophysical Journal</em> 553 (1): 47–72. <a href="https://doi.org/10.1086/320638">https://doi.org/10.1086/320638</a>.</p>
</div>
<div id="ref-Hubble1929">
<p>Hubble, Edwin. 1929. “A Relation Between Distance and Radial Velocity Among Extra-Galactic Nebulae.” <em>Proceedings of the National Academy of Sciences</em> 15 (3): 168–73. <a href="https://doi.org/10.1073/pnas.15.3.168">https://doi.org/10.1073/pnas.15.3.168</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="contingency.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="correlationchapter.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["stat0002book.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
