<!DOCTYPE html>
<html lang="en-gb" xml:lang="en-gb">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Correlation | STAT0002 Introduction to Probability and Statistics</title>
  <meta name="description" content="Produces STAT0002 notes in an accessible format" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Correlation | STAT0002 Introduction to Probability and Statistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Produces STAT0002 notes in an accessible format" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Correlation | STAT0002 Introduction to Probability and Statistics" />
  
  <meta name="twitter:description" content="Produces STAT0002 notes in an accessible format" />
  

<meta name="author" content="Dr Paul Northrop" />


<meta name="date" content="2020-12-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="linreg.html"/>
<link rel="next" href="a-general-strategy-for-statistical-modelling.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">STAT0002 2020-21</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>The purpose of these notes</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#real"><i class="fa fa-check"></i><b>1.1</b> Real statistical investigations</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#shuttle"><i class="fa fa-check"></i><b>1.2</b> Challenger Space Shuttle Catastrophe</a><ul>
<li class="chapter" data-level="1.2.1" data-path="introduction.html"><a href="introduction.html#uncertainty"><i class="fa fa-check"></i><b>1.2.1</b> Uncertainty</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#a-very-brief-introduction-to-stochastic-simulation"><i class="fa fa-check"></i><b>1.3</b> A very brief introduction to stochastic simulation</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="descriptive.html"><a href="descriptive.html"><i class="fa fa-check"></i><b>2</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="2.1" data-path="descriptive.html"><a href="descriptive.html#types-of-data"><i class="fa fa-check"></i><b>2.1</b> Types of data</a><ul>
<li class="chapter" data-level="2.1.1" data-path="descriptive.html"><a href="descriptive.html#qualitative-or-categorical-data"><i class="fa fa-check"></i><b>2.1.1</b> Qualitative or categorical data</a></li>
<li class="chapter" data-level="2.1.2" data-path="descriptive.html"><a href="descriptive.html#quantitative-or-numerical-data"><i class="fa fa-check"></i><b>2.1.2</b> Quantitative or numerical data</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="descriptive.html"><a href="descriptive.html#describing-distributions"><i class="fa fa-check"></i><b>2.2</b> Describing distributions</a><ul>
<li class="chapter" data-level="" data-path="descriptive.html"><a href="descriptive.html#example-oxford-births-data"><i class="fa fa-check"></i>Example: Oxford births data</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="descriptive.html"><a href="descriptive.html#summary-statistics"><i class="fa fa-check"></i><b>2.3</b> Summary Statistics</a><ul>
<li class="chapter" data-level="2.3.1" data-path="descriptive.html"><a href="descriptive.html#fivenumber"><i class="fa fa-check"></i><b>2.3.1</b> Five number summary</a></li>
<li class="chapter" data-level="2.3.2" data-path="descriptive.html"><a href="descriptive.html#meanstdev"><i class="fa fa-check"></i><b>2.3.2</b> Mean and standard deviation</a></li>
<li class="chapter" data-level="2.3.3" data-path="descriptive.html"><a href="descriptive.html#mode"><i class="fa fa-check"></i><b>2.3.3</b> Mode</a></li>
<li class="chapter" data-level="2.3.4" data-path="descriptive.html"><a href="descriptive.html#symmetry"><i class="fa fa-check"></i><b>2.3.4</b> Symmetry</a></li>
<li class="chapter" data-level="2.3.5" data-path="descriptive.html"><a href="descriptive.html#corr1"><i class="fa fa-check"></i><b>2.3.5</b> Correlation</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="descriptive.html"><a href="descriptive.html#tables"><i class="fa fa-check"></i><b>2.4</b> Tables</a><ul>
<li class="chapter" data-level="2.4.1" data-path="descriptive.html"><a href="descriptive.html#frequency-distribution"><i class="fa fa-check"></i><b>2.4.1</b> Frequency distribution</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="descriptive.html"><a href="descriptive.html#graphs"><i class="fa fa-check"></i><b>2.5</b> Graphs (1 variable)</a><ul>
<li class="chapter" data-level="2.5.1" data-path="descriptive.html"><a href="descriptive.html#histogram"><i class="fa fa-check"></i><b>2.5.1</b> Histograms</a></li>
<li class="chapter" data-level="2.5.2" data-path="descriptive.html"><a href="descriptive.html#stem"><i class="fa fa-check"></i><b>2.5.2</b> Stem-and-leaf plots</a></li>
<li class="chapter" data-level="2.5.3" data-path="descriptive.html"><a href="descriptive.html#dotplots"><i class="fa fa-check"></i><b>2.5.3</b> Dotplots</a></li>
<li class="chapter" data-level="2.5.4" data-path="descriptive.html"><a href="descriptive.html#boxplots"><i class="fa fa-check"></i><b>2.5.4</b> Boxplots</a></li>
<li class="chapter" data-level="2.5.5" data-path="descriptive.html"><a href="descriptive.html#barplots"><i class="fa fa-check"></i><b>2.5.5</b> Barplots</a></li>
<li class="chapter" data-level="2.5.6" data-path="descriptive.html"><a href="descriptive.html#times-series-plots"><i class="fa fa-check"></i><b>2.5.6</b> Times series plots</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="descriptive.html"><a href="descriptive.html#election"><i class="fa fa-check"></i><b>2.6</b> 2000 US Presidential Election</a></li>
<li class="chapter" data-level="2.7" data-path="descriptive.html"><a href="descriptive.html#graphs2"><i class="fa fa-check"></i><b>2.7</b> Graphs (2 variables)</a><ul>
<li class="chapter" data-level="2.7.1" data-path="descriptive.html"><a href="descriptive.html#scatter-plots"><i class="fa fa-check"></i><b>2.7.1</b> Scatter plots</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="descriptive.html"><a href="descriptive.html#transformation"><i class="fa fa-check"></i><b>2.8</b> Transformation of data</a><ul>
<li class="chapter" data-level="2.8.1" data-path="descriptive.html"><a href="descriptive.html#transsymmetry"><i class="fa fa-check"></i><b>2.8.1</b> Transformation to approximate symmetry</a></li>
<li class="chapter" data-level="2.8.2" data-path="descriptive.html"><a href="descriptive.html#straighten"><i class="fa fa-check"></i><b>2.8.2</b> Straightening scatter plots</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>3</b> Probability</a><ul>
<li class="chapter" data-level="3.1" data-path="probability.html"><a href="probability.html#sids"><i class="fa fa-check"></i><b>3.1</b> Misleading statistical evidence in cot death trials</a></li>
<li class="chapter" data-level="3.2" data-path="probability.html"><a href="probability.html#relative-frequency-definition-of-probability"><i class="fa fa-check"></i><b>3.2</b> Relative frequency definition of probability</a></li>
<li class="chapter" data-level="3.3" data-path="probability.html"><a href="probability.html#basic-properties-of-probability"><i class="fa fa-check"></i><b>3.3</b> Basic properties of probability</a></li>
<li class="chapter" data-level="3.4" data-path="probability.html"><a href="probability.html#conditional-probability"><i class="fa fa-check"></i><b>3.4</b> Conditional probability</a></li>
<li class="chapter" data-level="3.5" data-path="probability.html"><a href="probability.html#addition-rule-of-probability"><i class="fa fa-check"></i><b>3.5</b> Addition rule of probability</a><ul>
<li class="chapter" data-level="3.5.1" data-path="probability.html"><a href="probability.html#mutually-exclusive-events"><i class="fa fa-check"></i><b>3.5.1</b> Mutually exclusive events</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="probability.html"><a href="probability.html#multrule"><i class="fa fa-check"></i><b>3.6</b> Multiplication rule of probability</a></li>
<li class="chapter" data-level="3.7" data-path="probability.html"><a href="probability.html#indepevents"><i class="fa fa-check"></i><b>3.7</b> Independence of events</a><ul>
<li class="chapter" data-level="3.7.1" data-path="probability.html"><a href="probability.html#bloodindep"><i class="fa fa-check"></i><b>3.7.1</b> An example of independence</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="probability.html"><a href="probability.html#law-of-total-probability"><i class="fa fa-check"></i><b>3.8</b> Law of total probability</a></li>
<li class="chapter" data-level="3.9" data-path="probability.html"><a href="probability.html#bayes-theorem"><i class="fa fa-check"></i><b>3.9</b> Bayes’ theorem</a></li>
<li class="chapter" data-level="3.10" data-path="probability.html"><a href="probability.html#dna-identification-evidence"><i class="fa fa-check"></i><b>3.10</b> DNA identification evidence</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="rvs.html"><a href="rvs.html"><i class="fa fa-check"></i><b>4</b> Random variables</a><ul>
<li class="chapter" data-level="4.1" data-path="rvs.html"><a href="rvs.html#discrete"><i class="fa fa-check"></i><b>4.1</b> Discrete random variables</a></li>
<li class="chapter" data-level="4.2" data-path="rvs.html"><a href="rvs.html#continuous"><i class="fa fa-check"></i><b>4.2</b> Continuous random variables</a></li>
<li class="chapter" data-level="4.3" data-path="rvs.html"><a href="rvs.html#expectation"><i class="fa fa-check"></i><b>4.3</b> Expectation</a><ul>
<li class="chapter" data-level="4.3.1" data-path="rvs.html"><a href="rvs.html#expectation-of-a-discrete-random-variable"><i class="fa fa-check"></i><b>4.3.1</b> Expectation of a discrete random variable</a></li>
<li class="chapter" data-level="4.3.2" data-path="rvs.html"><a href="rvs.html#expectation-of-a-continuous-random-variable"><i class="fa fa-check"></i><b>4.3.2</b> Expectation of a continuous random variable</a></li>
<li class="chapter" data-level="4.3.3" data-path="rvs.html"><a href="rvs.html#properties-of-mathrmex"><i class="fa fa-check"></i><b>4.3.3</b> Properties of <span class="math inline">\(\mathrm{E}(X)\)</span></a></li>
<li class="chapter" data-level="4.3.4" data-path="rvs.html"><a href="rvs.html#EgX"><i class="fa fa-check"></i><b>4.3.4</b> The expectation of <span class="math inline">\(g(X)\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="rvs.html"><a href="rvs.html#variance"><i class="fa fa-check"></i><b>4.4</b> Variance</a><ul>
<li class="chapter" data-level="4.4.1" data-path="rvs.html"><a href="rvs.html#variance-of-a-discrete-random-variable"><i class="fa fa-check"></i><b>4.4.1</b> Variance of a discrete random variable</a></li>
<li class="chapter" data-level="4.4.2" data-path="rvs.html"><a href="rvs.html#variance-of-a-continuous-random-variable"><i class="fa fa-check"></i><b>4.4.2</b> Variance of a continuous random variable</a></li>
<li class="chapter" data-level="4.4.3" data-path="rvs.html"><a href="rvs.html#variance-and-standard-deviation"><i class="fa fa-check"></i><b>4.4.3</b> Variance and standard deviation</a></li>
<li class="chapter" data-level="4.4.4" data-path="rvs.html"><a href="rvs.html#properties-of-mathrmvarx"><i class="fa fa-check"></i><b>4.4.4</b> Properties of <span class="math inline">\(\mathrm{var}(X)\)</span></a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="rvs.html"><a href="rvs.html#locations"><i class="fa fa-check"></i><b>4.5</b> Other measures of location</a><ul>
<li class="chapter" data-level="4.5.1" data-path="rvs.html"><a href="rvs.html#the-median-of-a-random-variable"><i class="fa fa-check"></i><b>4.5.1</b> The median of a random variable</a></li>
<li class="chapter" data-level="4.5.2" data-path="rvs.html"><a href="rvs.html#the-mode-of-a-random-variable"><i class="fa fa-check"></i><b>4.5.2</b> The mode of a random variable</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="rvs.html"><a href="rvs.html#quantiles"><i class="fa fa-check"></i><b>4.6</b> Quantiles</a></li>
<li class="chapter" data-level="4.7" data-path="rvs.html"><a href="rvs.html#measures-of-shape"><i class="fa fa-check"></i><b>4.7</b> Measures of shape</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="simple.html"><a href="simple.html"><i class="fa fa-check"></i><b>5</b> Simple distributions</a><ul>
<li class="chapter" data-level="5.1" data-path="simple.html"><a href="simple.html#australian-births-data"><i class="fa fa-check"></i><b>5.1</b> Australian births data</a></li>
<li class="chapter" data-level="5.2" data-path="simple.html"><a href="simple.html#the-bernoulli-distribution"><i class="fa fa-check"></i><b>5.2</b> The Bernoulli distribution</a><ul>
<li class="chapter" data-level="5.2.1" data-path="simple.html"><a href="simple.html#summary-of-the-bernoullip-distribution"><i class="fa fa-check"></i><b>5.2.1</b> Summary of the Bernoulli(<span class="math inline">\(p\)</span>) distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="simple.html"><a href="simple.html#binomial"><i class="fa fa-check"></i><b>5.3</b> The binomial distribution</a><ul>
<li class="chapter" data-level="5.3.1" data-path="simple.html"><a href="simple.html#binominf"><i class="fa fa-check"></i><b>5.3.1</b> A brief look at statistical inference about <span class="math inline">\(p\)</span></a></li>
<li class="chapter" data-level="5.3.2" data-path="simple.html"><a href="simple.html#summary-of-the-binomialnp-distribution"><i class="fa fa-check"></i><b>5.3.2</b> Summary of the binomial(<span class="math inline">\(n,p\)</span>) distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="simple.html"><a href="simple.html#the-geometric-distribution"><i class="fa fa-check"></i><b>5.4</b> The geometric distribution</a><ul>
<li class="chapter" data-level="5.4.1" data-path="simple.html"><a href="simple.html#summary-of-the-geometricp-distribution"><i class="fa fa-check"></i><b>5.4.1</b> Summary of the geometric(<span class="math inline">\(p\)</span>) distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="simple.html"><a href="simple.html#Poisson"><i class="fa fa-check"></i><b>5.5</b> The Poisson distribution</a><ul>
<li class="chapter" data-level="5.5.1" data-path="simple.html"><a href="simple.html#summary-of-the-poissonmu-distribution"><i class="fa fa-check"></i><b>5.5.1</b> Summary of the Poisson(<span class="math inline">\(\mu\)</span>) distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="simple.html"><a href="simple.html#summary-of-these-discrete-distributions"><i class="fa fa-check"></i><b>5.6</b> Summary of these discrete distributions</a></li>
<li class="chapter" data-level="5.7" data-path="simple.html"><a href="simple.html#uniform"><i class="fa fa-check"></i><b>5.7</b> The uniform distribution</a><ul>
<li class="chapter" data-level="5.7.1" data-path="simple.html"><a href="simple.html#summary-of-the-uniformab-distribution"><i class="fa fa-check"></i><b>5.7.1</b> Summary of the uniform(<span class="math inline">\(a,b\)</span>) distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="simple.html"><a href="simple.html#exponential"><i class="fa fa-check"></i><b>5.8</b> The exponential distribution</a><ul>
<li class="chapter" data-level="5.8.1" data-path="simple.html"><a href="simple.html#summary-of-the-exponentiallambda-distribution"><i class="fa fa-check"></i><b>5.8.1</b> Summary of the exponential(<span class="math inline">\(\lambda\)</span>) distribution</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="simple.html"><a href="simple.html#normal"><i class="fa fa-check"></i><b>5.9</b> The normal distribution</a><ul>
<li class="chapter" data-level="5.9.1" data-path="simple.html"><a href="simple.html#summary-of-the-mboxnmusigma2-distribution"><i class="fa fa-check"></i><b>5.9.1</b> Summary of the <span class="math inline">\(\mbox{N}(\mu,\sigma^2)\)</span> distribution</a></li>
<li class="chapter" data-level="5.9.2" data-path="simple.html"><a href="simple.html#the-standard-normal-disribution"><i class="fa fa-check"></i><b>5.9.2</b> The standard normal disribution</a></li>
<li class="chapter" data-level="5.9.3" data-path="simple.html"><a href="simple.html#evaluating-the-normal-c.d.f.-and-quantiles"><i class="fa fa-check"></i><b>5.9.3</b> Evaluating the normal c.d.f. and quantiles</a></li>
<li class="chapter" data-level="5.9.4" data-path="simple.html"><a href="simple.html#interpretation-of-sigma"><i class="fa fa-check"></i><b>5.9.4</b> Interpretation of <span class="math inline">\(\sigma\)</span></a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="simple.html"><a href="simple.html#summary-of-these-continuous-distributions"><i class="fa fa-check"></i><b>5.10</b> Summary of these continuous distributions</a></li>
<li class="chapter" data-level="5.11" data-path="simple.html"><a href="simple.html#qq"><i class="fa fa-check"></i><b>5.11</b> QQ plots</a><ul>
<li class="chapter" data-level="5.11.1" data-path="simple.html"><a href="simple.html#normal-qq-plots"><i class="fa fa-check"></i><b>5.11.1</b> Normal QQ plots</a></li>
<li class="chapter" data-level="5.11.2" data-path="simple.html"><a href="simple.html#uniform-qq-plots"><i class="fa fa-check"></i><b>5.11.2</b> Uniform QQ plots</a></li>
<li class="chapter" data-level="5.11.3" data-path="simple.html"><a href="simple.html#exponential-qq-plots"><i class="fa fa-check"></i><b>5.11.3</b> Exponential QQ plots</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>6</b> Statistical Inference</a><ul>
<li class="chapter" data-level="6.1" data-path="inference.html"><a href="inference.html#the-story-so-far"><i class="fa fa-check"></i><b>6.1</b> The story so far</a></li>
<li class="chapter" data-level="6.2" data-path="inference.html"><a href="inference.html#sample-and-populations"><i class="fa fa-check"></i><b>6.2</b> Sample and populations</a></li>
<li class="chapter" data-level="6.3" data-path="inference.html"><a href="inference.html#probmodels"><i class="fa fa-check"></i><b>6.3</b> Probability models</a></li>
<li class="chapter" data-level="6.4" data-path="inference.html"><a href="inference.html#fitting-models"><i class="fa fa-check"></i><b>6.4</b> Fitting models</a></li>
<li class="chapter" data-level="6.5" data-path="inference.html"><a href="inference.html#uncertainty-in-estimation"><i class="fa fa-check"></i><b>6.5</b> Uncertainty in estimation</a><ul>
<li class="chapter" data-level="6.5.1" data-path="inference.html"><a href="inference.html#simulation-coin-tossing-example"><i class="fa fa-check"></i><b>6.5.1</b> Simulation: coin-tossing example</a></li>
<li class="chapter" data-level="6.5.2" data-path="inference.html"><a href="inference.html#simnorm"><i class="fa fa-check"></i><b>6.5.2</b> Simulation: estimating the parameters of a normal distribution</a></li>
<li class="chapter" data-level="6.5.3" data-path="inference.html"><a href="inference.html#simexp"><i class="fa fa-check"></i><b>6.5.3</b> Simulation: estimating the parameters of an exponential distribution</a></li>
<li class="chapter" data-level="6.5.4" data-path="inference.html"><a href="inference.html#central-limit-theorem"><i class="fa fa-check"></i><b>6.5.4</b> Central Limit Theorem</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="inference.html"><a href="inference.html#good"><i class="fa fa-check"></i><b>6.6</b> Properties of estimators</a><ul>
<li class="chapter" data-level="6.6.1" data-path="inference.html"><a href="inference.html#bias"><i class="fa fa-check"></i><b>6.6.1</b> Bias</a></li>
<li class="chapter" data-level="6.6.2" data-path="inference.html"><a href="inference.html#varianceofestimator"><i class="fa fa-check"></i><b>6.6.2</b> Variance</a></li>
<li class="chapter" data-level="6.6.3" data-path="inference.html"><a href="inference.html#mean-squared-error-mse"><i class="fa fa-check"></i><b>6.6.3</b> Mean squared error (MSE)</a></li>
<li class="chapter" data-level="6.6.4" data-path="inference.html"><a href="inference.html#standard-error"><i class="fa fa-check"></i><b>6.6.4</b> Standard error</a></li>
<li class="chapter" data-level="6.6.5" data-path="inference.html"><a href="inference.html#consistency"><i class="fa fa-check"></i><b>6.6.5</b> Consistency</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="inference.html"><a href="inference.html#assessing-goodness-of-fit"><i class="fa fa-check"></i><b>6.7</b> Assessing goodness-of-fit</a><ul>
<li class="chapter" data-level="6.7.1" data-path="inference.html"><a href="inference.html#residuals"><i class="fa fa-check"></i><b>6.7.1</b> Residuals</a></li>
<li class="chapter" data-level="6.7.2" data-path="inference.html"><a href="inference.html#standardised-residuals"><i class="fa fa-check"></i><b>6.7.2</b> Standardised residuals</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="contingency.html"><a href="contingency.html"><i class="fa fa-check"></i><b>7</b> Contingency tables</a><ul>
<li class="chapter" data-level="7.1" data-path="contingency.html"><a href="contingency.html#way2"><i class="fa fa-check"></i><b>7.1</b> 2-way contingency tables</a><ul>
<li class="chapter" data-level="7.1.1" data-path="contingency.html"><a href="contingency.html#indep"><i class="fa fa-check"></i><b>7.1.1</b> Independence</a></li>
<li class="chapter" data-level="7.1.2" data-path="contingency.html"><a href="contingency.html#compprob"><i class="fa fa-check"></i><b>7.1.2</b> Comparing probabilities</a></li>
<li class="chapter" data-level="7.1.3" data-path="contingency.html"><a href="contingency.html#measures"><i class="fa fa-check"></i><b>7.1.3</b> Measures of association</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="contingency.html"><a href="contingency.html#way3"><i class="fa fa-check"></i><b>7.2</b> 3-way contingency tables</a><ul>
<li class="chapter" data-level="7.2.1" data-path="contingency.html"><a href="contingency.html#mutual-independence"><i class="fa fa-check"></i><b>7.2.1</b> Mutual independence</a></li>
<li class="chapter" data-level="7.2.2" data-path="contingency.html"><a href="contingency.html#marginal-independence"><i class="fa fa-check"></i><b>7.2.2</b> Marginal independence</a></li>
<li class="chapter" data-level="7.2.3" data-path="contingency.html"><a href="contingency.html#conditional-independence"><i class="fa fa-check"></i><b>7.2.3</b> Conditional independence</a></li>
<li class="chapter" data-level="7.2.4" data-path="contingency.html"><a href="contingency.html#confounding-variables"><i class="fa fa-check"></i><b>7.2.4</b> Confounding variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="linreg.html"><a href="linreg.html"><i class="fa fa-check"></i><b>8</b> Linear regression</a><ul>
<li class="chapter" data-level="8.1" data-path="linreg.html"><a href="linreg.html#simple-linear-regression"><i class="fa fa-check"></i><b>8.1</b> Simple linear regression</a><ul>
<li class="chapter" data-level="8.1.1" data-path="linreg.html"><a href="linreg.html#simple-linear-regression-model"><i class="fa fa-check"></i><b>8.1.1</b> Simple linear regression model</a></li>
<li class="chapter" data-level="8.1.2" data-path="linreg.html"><a href="linreg.html#least-squares-estimation-of-alpha-and-beta"><i class="fa fa-check"></i><b>8.1.2</b> Least squares estimation of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span></a></li>
<li class="chapter" data-level="8.1.3" data-path="linreg.html"><a href="linreg.html#least-squares-fitting-to-hubbles-data"><i class="fa fa-check"></i><b>8.1.3</b> Least squares fitting to Hubble’s data</a></li>
<li class="chapter" data-level="8.1.4" data-path="linreg.html"><a href="linreg.html#normal-linear-regression-model"><i class="fa fa-check"></i><b>8.1.4</b> Normal linear regression model</a></li>
<li class="chapter" data-level="8.1.5" data-path="linreg.html"><a href="linreg.html#lmsummary"><i class="fa fa-check"></i><b>8.1.5</b> Summary of the assumptions of a (normal) linear regression model</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="linreg.html"><a href="linreg.html#looking"><i class="fa fa-check"></i><b>8.2</b> Looking at scatter plots</a></li>
<li class="chapter" data-level="8.3" data-path="linreg.html"><a href="linreg.html#model-checking"><i class="fa fa-check"></i><b>8.3</b> Model checking</a><ul>
<li class="chapter" data-level="8.3.1" data-path="linreg.html"><a href="linreg.html#departures-from-assumptions"><i class="fa fa-check"></i><b>8.3.1</b> Departures from assumptions</a></li>
<li class="chapter" data-level="8.3.2" data-path="linreg.html"><a href="linreg.html#outliers"><i class="fa fa-check"></i><b>8.3.2</b> Outliers and influential observations</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="linreg.html"><a href="linreg.html#linregtrans"><i class="fa fa-check"></i><b>8.4</b> Use of transformations</a><ul>
<li class="chapter" data-level="8.4.1" data-path="linreg.html"><a href="linreg.html#interpretation-after-transformation"><i class="fa fa-check"></i><b>8.4.1</b> Interpretation after transformation</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="linreg.html"><a href="linreg.html#over-fitting"><i class="fa fa-check"></i><b>8.5</b> Over-fitting</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="correlationchapter.html"><a href="correlationchapter.html"><i class="fa fa-check"></i><b>9</b> Correlation</a><ul>
<li class="chapter" data-level="9.1" data-path="correlationchapter.html"><a href="correlationchapter.html#correlation-a-measure-of-linear-association"><i class="fa fa-check"></i><b>9.1</b> Correlation: a measure of linear association</a></li>
<li class="chapter" data-level="9.2" data-path="correlationchapter.html"><a href="correlationchapter.html#covariance-and-correlation"><i class="fa fa-check"></i><b>9.2</b> Covariance and correlation</a><ul>
<li class="chapter" data-level="9.2.1" data-path="correlationchapter.html"><a href="correlationchapter.html#estimation"><i class="fa fa-check"></i><b>9.2.1</b> Estimation</a></li>
<li class="chapter" data-level="9.2.2" data-path="correlationchapter.html"><a href="correlationchapter.html#links-between-regression-and-correlation"><i class="fa fa-check"></i><b>9.2.2</b> Links between regression and correlation</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="correlationchapter.html"><a href="correlationchapter.html#use-and-misuse-of-correlation"><i class="fa fa-check"></i><b>9.3</b> Use and misuse of correlation</a><ul>
<li class="chapter" data-level="9.3.1" data-path="correlationchapter.html"><a href="correlationchapter.html#do-not-use-correlation-for-regression-sampling-schemes"><i class="fa fa-check"></i><b>9.3.1</b> Do not use correlation for regression sampling schemes</a></li>
<li class="chapter" data-level="9.3.2" data-path="correlationchapter.html"><a href="correlationchapter.html#correxamples"><i class="fa fa-check"></i><b>9.3.2</b> Examples of correlations of different strengths</a></li>
<li class="chapter" data-level="9.3.3" data-path="correlationchapter.html"><a href="correlationchapter.html#beware-missing-data-codes"><i class="fa fa-check"></i><b>9.3.3</b> Beware missing data codes</a></li>
<li class="chapter" data-level="9.3.4" data-path="correlationchapter.html"><a href="correlationchapter.html#more-guessing-sample-correlations"><i class="fa fa-check"></i><b>9.3.4</b> More guessing sample correlations</a></li>
<li class="chapter" data-level="9.3.5" data-path="correlationchapter.html"><a href="correlationchapter.html#summary"><i class="fa fa-check"></i><b>9.3.5</b> Summary</a></li>
<li class="chapter" data-level="9.3.6" data-path="correlationchapter.html"><a href="correlationchapter.html#anscombes-datasets"><i class="fa fa-check"></i><b>9.3.6</b> Anscombe’s datasets</a></li>
<li class="chapter" data-level="9.3.7" data-path="correlationchapter.html"><a href="correlationchapter.html#we-must-interpret-correlation-with-care."><i class="fa fa-check"></i><b>9.3.7</b> We must interpret correlation with care.</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="a-general-strategy-for-statistical-modelling.html"><a href="a-general-strategy-for-statistical-modelling.html"><i class="fa fa-check"></i><b>10</b> A general strategy for statistical modelling</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">STAT0002 Introduction to Probability and Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="correlationchapter" class="section level1">
<h1><span class="header-section-number">Chapter 9</span> Correlation</h1>
<p>Now we consider the situation where objects are sampled randomly from a population, and, for each object, we observe the values of random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Therefore, unlike regression sampling, where the values of <span class="math inline">\(X\)</span> are chosen by an experimenter, both <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are random variables. Unlike a regression problem, we treat <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> symmetrically, that is, we do not identify one variable as a response and one as explanatory. We are interested in the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. How are they associated, and how strongly?</p>
<p>When estimating correlation from data we will concentrate on the sample correlation coefficient <span class="math inline">\(r\)</span> defined in equation <a href="descriptive.html#eq:corr">(2.1)</a> of Section <a href="descriptive.html#corr1">2.3.5</a>, which is a measure of the strength of <strong>linear</strong> association. However, the general comments that we make will also apply to Spearman’s rank correlation coefficient <span class="math inline">\(r_S\)</span> (also defined in section <a href="descriptive.html#corr1">2.3.5</a>.</p>
<div id="correlation-a-measure-of-linear-association" class="section level2">
<h2><span class="header-section-number">9.1</span> Correlation: a measure of linear association</h2>
<p><strong>UK/USA/Canadian exchange rates</strong></p>
<p>Figure <a href="correlationchapter.html#fig:exchangeraw">9.1</a> is a plot the UK Pound/Canadian dollar exchange rate vs. the UK Pound/US dollar exchange rate for each day between 2nd January 1997 and 21 November 2000.</p>
<div class="figure" style="text-align: center"><span id="fig:exchangeraw"></span>
<img src="images/exchange_raw.png" alt="Value of 1 UK pound in Canadian dollars against value of 1 UK pound in US dollars." width="75%" />
<p class="caption">
Figure 9.1: Value of 1 UK pound in Canadian dollars against value of 1 UK pound in US dollars.
</p>
</div>
<p>The relationship between these two variables is complicated. However, when dealing with exchange rates it is common to (a) work with <span class="math inline">\(\log\)</span>(exchange rate), and (b) use the <strong>change</strong> in <span class="math inline">\(\log\)</span>(exchange rate) from one day to the next. This produces quantities called <strong>log-returns</strong>. In Figure <a href="correlationchapter.html#fig:exchangetrans">9.2</a> the log-returns for the UK Pound/Canadian dollar exchange rate (<span class="math inline">\(Y\)</span>) are plotted against the log-returns <span class="math inline">\(X\)</span> for the UK Pound/US exchange rate (<span class="math inline">\(X\)</span>).</p>
<div class="figure" style="text-align: center"><span id="fig:exchangetrans"></span>
<img src="images/exchange.png" alt="Plot of $Y$ against $X$.  The vertical line is drawn at the sample mean of the $X$ data and the horizontal line at the sample mean of the $Y$ data." width="75%" />
<p class="caption">
Figure 9.2: Plot of <span class="math inline">\(Y\)</span> against <span class="math inline">\(X\)</span>. The vertical line is drawn at the sample mean of the <span class="math inline">\(X\)</span> data and the horizontal line at the sample mean of the <span class="math inline">\(Y\)</span> data.
</p>
</div>
<p>This plot has a much clearer pattern. There is positive, approximately linear, association between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>. Large values of the variables tend to occur together, as do small values. There are more points in the quadrants labelled B and C than in A and D. This positive association is due to the link between the US and Canadian financial markets. If you are investing in these markets it is important that you are aware of this positive association, since it means that when you lose money you are likely to lose it in both the US market and the Canadian market.</p>
<p>Figure <a href="correlationchapter.html#fig:exchangetrans">9.2</a> suggests that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are approximately positively, linearly associated. Correlation measures the strength of this linear association.</p>
</div>
<div id="covariance-and-correlation" class="section level2">
<h2><span class="header-section-number">9.2</span> Covariance and correlation</h2>
<p><strong>Definition</strong>. Let <span class="math inline">\(\mu_X=\mbox{E}(X)\)</span> and <span class="math inline">\(\mu_Y=\mbox{E}(Y)\)</span>. The <strong>covariance</strong> <span class="math inline">\(\mbox{cov}(X,Y)\)</span> between two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is given by
<span class="math display">\[
\mbox{cov}(X,Y)=\mbox{E}\left[\left(X-\mu_X\right)\left(Y-\mu_Y\right)\right]=\mbox{E}(XY)-\mbox{E}(X)\mbox{E}(Y). 
\]</span>
The value of <span class="math inline">\(\mbox{cov}(X,Y)\)</span> depends on the scale of measurement of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. For example, if we were to multiply all the values of <span class="math inline">\(X\)</span> by 2 then <span class="math inline">\(\mbox{cov}(X,Y)\)</span> is also multiplied by 2. Therefore we define a standardised version of covariance: correlation.</p>
<p><strong>Definition</strong>. The <strong>correlation (coefficient)</strong> between two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is given by
<span class="math display">\[
\rho = \mbox{corr}(X,Y) = \frac{\mbox{cov}(X,Y)}{\mbox{sd}(X)\mbox{sd}(Y)} 
= \frac{\mbox{E}\left[\left(X-\mu_X\right)\left(Y-\mu_Y\right)\right]}{\mbox{sd}(X)\mbox{sd}(Y)}, 
\]</span>
provided that both <span class="math inline">\(\mbox{sd}(X)\)</span> and <span class="math inline">\(\mbox{sd}(Y)\)</span> are positive. Correlation is dimensionless (it has no units) and its value does not change if we scale <span class="math inline">\(X\)</span> and/or <span class="math inline">\(Y\)</span>. Correlation (and covariance) are measures of <strong>linear</strong> association between random variables. They are a sensible measure of the strength of association between two random variables only if there is a linear relationship between them.</p>
<p>Interpretation of <span class="math inline">\(\rho\)</span>:</p>
<ul>
<li><span class="math inline">\(-1 \leq \rho \leq 1\)</span>.</li>
<li>If <span class="math inline">\(0 &lt; \rho \leq 1\)</span> then there is positive association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.
The larger the value of <span class="math inline">\(\rho\)</span> the stronger the positive association.</li>
<li>If <span class="math inline">\(-1 \leq \rho &lt; 0\)</span> then there is negative association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.
The smaller the value of <span class="math inline">\(\rho\)</span> the stronger the negative association.</li>
<li>If <span class="math inline">\(|\rho|=1\)</span> there is a perfect linear relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, that is
<span class="math inline">\(Y=\alpha\,X+\beta\)</span>. If <span class="math inline">\(\rho=1\)</span> then <span class="math inline">\(\alpha&gt;0\)</span>. If <span class="math inline">\(\rho=-1\)</span> then <span class="math inline">\(\alpha&lt;0\)</span>.</li>
<li><span class="math inline">\(\rho = 0\)</span> does <strong>not</strong> indicate no association, just a lack of
<strong>linear</strong> association. If <span class="math inline">\(\rho\)</span> is zero, or close to zero, it could be that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are strongly non-linearly associated.</li>
<li>If <span class="math inline">\(\rho=0\)</span> we say that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <strong>uncorrelated</strong>.</li>
</ul>
<div id="estimation" class="section level3">
<h3><span class="header-section-number">9.2.1</span> Estimation</h3>
<p>Suppose that we have a random sample <span class="math inline">\((X_1,Y_1), ..., (X_n,Y_n)\)</span> from the joint distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. The <strong>sample correlation coefficient</strong>
<span class="math display">\[
R = \hat{\rho} = \frac{\displaystyle\sum_{i=1}^n (X_i-\bar{X})(Y_i-\bar{Y})}
{\sqrt{\displaystyle\sum_{i=1}^n (X_i-\bar{X})^2 \displaystyle\sum_{i=1}^n(Y_i-\bar{Y})^2}} 
= \frac{C_{XY}}{\sqrt{C_{XX}\,C_{YY}}}, 
\]</span>
is an estimator of the correlation coefficient <span class="math inline">\(\rho\)</span>.</p>
<p>For the data in Figure <a href="correlationchapter.html#fig:exchangetrans">9.2</a> we get <span class="math inline">\(r=0.82\)</span>. This confirms the quite strong positive linear association apparent in the plot. Why do these data give a value of <span class="math inline">\(r\)</span> which is positive? We consider contributions of individual data points to the numerator
<span class="math display">\[  C_{xy}=\displaystyle\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y}) \]</span>
of <span class="math inline">\(r\)</span>. Points in quadrant</p>
<ul>
<li>B will have <span class="math inline">\(x_i-\bar{x}&gt;0\)</span> and <span class="math inline">\(y_i-\bar{y}&gt;0\)</span>. Therefore <span class="math inline">\((x_i-\bar{x})(y_i-\bar{y})&gt;0\)</span>.<br />
</li>
<li>C will have <span class="math inline">\(x_i-\bar{x}&lt;0\)</span> and <span class="math inline">\(y_i-\bar{y}&lt;0\)</span>. Therefore <span class="math inline">\((x_i-\bar{x})(y_i-\bar{y})&gt;0\)</span>.<br />
</li>
<li>A will have <span class="math inline">\(x_i-\bar{x}&lt;0\)</span> and <span class="math inline">\(y_i-\bar{y}&gt;0\)</span>. Therefore <span class="math inline">\((x_i-\bar{x})(y_i-\bar{y})&lt;0\)</span>.<br />
</li>
<li>D will have <span class="math inline">\(x_i-\bar{x}&gt;0\)</span> and <span class="math inline">\(y_i-\bar{y}&lt;0\)</span>. Therefore <span class="math inline">\((x_i-\bar{x})(y_i-\bar{y})&lt;0\)</span>.</li>
</ul>
<p>Therefore, since <span class="math inline">\(\sqrt{C_{xx}\,C_{yy}}&gt;0\)</span>, and <span class="math inline">\(C_{xy}\)</span> is a sum of contributions from all pairs <span class="math inline">\((x_i,y_i)\)</span>:</p>
<ul>
<li>If most of the observed points lie in quadrants B and C then <span class="math inline">\(r\)</span> is positive.</li>
<li>If most of the observed points lie in quadrants A and D then <span class="math inline">\(r\)</span> is negative.</li>
<li>If the numbers of points in each quadrant are approximately equal then <span class="math inline">\(r\)</span> is close to 0.</li>
</ul>
</div>
<div id="links-between-regression-and-correlation" class="section level3">
<h3><span class="header-section-number">9.2.2</span> Links between regression and correlation</h3>
<p>Regression and correlation answer different questions. However, there are links between them. If we regress <span class="math inline">\(Y\)</span> on <span class="math inline">\(x\)</span> then</p>
<ol style="list-style-type: decimal">
<li>the sample coefficient of determination <span class="math inline">\(R^2\)</span> (<span class="math inline">\(R^2\)</span> is just a name: this is not literally the square of a quantity <span class="math inline">\(R\)</span>) is equal to <span class="math inline">\(r^2\)</span>, the square of the sample correlation coefficient,</li>
<li>the estimate of the regression slope <span class="math inline">\(\beta\)</span> is related to the sample correlation coefficient via
<span class="math display">\[ \hat{\beta} = r \,\sqrt{\frac{C_{yy}}{C_{xx}}}. \]</span></li>
</ol>
<p>These relationships make general sense.</p>
<ul>
<li>If <span class="math inline">\(R^2\)</span> is large then the linear regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(x\)</span> explains a lot of variability in the <span class="math inline">\(Y\)</span> data and so the correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> must be strong.</li>
<li>The value of <span class="math inline">\(\hat{\beta}\)</span> and the value of <span class="math inline">\(r\)</span> have the same sign.</li>
</ul>
</div>
</div>
<div id="use-and-misuse-of-correlation" class="section level2">
<h2><span class="header-section-number">9.3</span> Use and misuse of correlation</h2>
<p>We must be careful to use correlation only when it is appropriate. When it is used we must be careful to interpret its value carefully. In this section we give at some examples to show why we must use correlation with care.</p>
<p>One golden rule is that we should <strong>always plots the data</strong>.</p>
<div id="do-not-use-correlation-for-regression-sampling-schemes" class="section level3">
<h3><span class="header-section-number">9.3.1</span> Do not use correlation for regression sampling schemes</h3>
<p>We should <strong>not</strong> to use correlation to summarise association between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> when regression sampling has been used, that is, where the values of <span class="math inline">\(X\)</span> are <strong>chosen</strong> and then the values of <span class="math inline">\(Y\)</span> are observed. We use simulation to show us why this is. Figure <a href="correlationchapter.html#fig:xdesigncorr0">9.3</a> shows a random sample of size 100 from a joint distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> for which <span class="math inline">\(\rho=0.5\)</span>. The sample correlation coefficient is 0.51.</p>
<div class="figure" style="text-align: center"><span id="fig:xdesigncorr0"></span>
<img src="images/design_corr0.png" alt="A random sample of size 100 from a distribution with $\rho=0.5$." width="50%" />
<p class="caption">
Figure 9.3: A random sample of size 100 from a distribution with <span class="math inline">\(\rho=0.5\)</span>.
</p>
</div>
<p>In Figure <a href="correlationchapter.html#fig:xdesigncorr">9.4</a> regression samples of size 10 have been taken from the <strong>same</strong> joint distribution, but</p>
<ul>
<li>on the left the values of <span class="math inline">\(x\)</span> are <strong>chosen</strong> to be <span class="math inline">\(-2\)</span> and <span class="math inline">\(+2\)</span>. We get <span class="math inline">\(r=0.84\)</span>.</li>
<li>on the right the values of <span class="math inline">\(x\)</span> are <strong>chosen</strong> to be <span class="math inline">\(-0.1\)</span> and <span class="math inline">\(+0.1\)</span>. We get <span class="math inline">\(r=-0.46\)</span>.</li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:xdesigncorr"></span>
<img src="images/design_corr.PNG" alt="Left: $x$-values chosen to be only -2 or 2.  Right: $x$-values chosen to be only -0.1 or 0.1." width="100%" />
<p class="caption">
Figure 9.4: Left: <span class="math inline">\(x\)</span>-values chosen to be only -2 or 2. Right: <span class="math inline">\(x\)</span>-values chosen to be only -0.1 or 0.1.
</p>
</div>
<p>The values chosen for <span class="math inline">\(x\)</span> have a huge effect on <span class="math inline">\(r\)</span>. By choosing the values of <span class="math inline">\(x\)</span> to be spread far apart we can make <span class="math inline">\(r\)</span> as close to <span class="math inline">\(+1\)</span> as we wish. If the values of <span class="math inline">\(x\)</span> are chosen to be close together, <span class="math inline">\(r\)</span> will be quite variable, but close to zero on average.</p>
<p><strong>Summary</strong></p>
<p>If regression sampling is used the value of <span class="math inline">\(r\)</span> depends on the values of <span class="math inline">\(x\)</span> chosen.</p>
</div>
<div id="correxamples" class="section level3">
<h3><span class="header-section-number">9.3.2</span> Examples of correlations of different strengths</h3>
<p>Figure <a href="correlationchapter.html#fig:corr8">9.5</a> gives random samples from distributions with correlations <span class="math inline">\(\rho=0\)</span> (2 of them), <span class="math inline">\(\rho=\pm 0.3, \rho=\pm 0.7\)</span> and <span class="math inline">\(\rho=\pm 1\)</span>. The data in the bottom plots in Figure <a href="correlationchapter.html#fig:corr8">9.5</a> are both sampled from distributions with <span class="math inline">\(\rho=0\)</span>. However, the associations between the variables are very different. On the left there is no association. On the right there is quite strong non-linear association.</p>
<div class="figure" style="text-align: center"><span id="fig:corr8"></span>
<img src="images/corr8c.png" alt="Samples from distributions with correlations $
ho=0$ (2 of them), $
ho=\pm 0.3, 
ho=\pm 0.7$ and $
ho=\pm 1$." width="80%" /><img src="images/corr8d.png" alt="Samples from distributions with correlations $
ho=0$ (2 of them), $
ho=\pm 0.3, 
ho=\pm 0.7$ and $
ho=\pm 1$." width="80%" />
<p class="caption">
Figure 9.5: Samples from distributions with correlations $
ho=0$ (2 of them), $
ho=0.3,
ho=0.7$ and $
ho=1$.
</p>
</div>
</div>
<div id="beware-missing-data-codes" class="section level3">
<h3><span class="header-section-number">9.3.3</span> Beware missing data codes</h3>
<p>Most real datasets have <strong>missing observations</strong>. For example, people may decide not to answer some of the questions on a questionnaire. It is quite common to use a missing value code (such as -9) to identify a missing value. We need to be careful not to confuse a numerical missing value code with actual data.</p>
<p>Consider the following situation, which is based on a real example.Someone is given data on two variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. They calculate the sample correlation coefficient <span class="math inline">\(r\)</span> and find that it is 0.95. They go back to the person who gave them the data and tell them that<span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are strongly positively associated. The person is surprised: they expected the variables to be negatively associated. So they produce a plot of the data, shown on the left side of Figure <a href="correlationchapter.html#fig:corrmissing">9.6</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:corrmissing"></span>
<img src="images/corr_missing1.png" alt="Example data. Left: including the 'datum' (-9,9). Right: with the missing value removed." width="50%" /><img src="images/corr_missing2.png" alt="Example data. Left: including the 'datum' (-9,9). Right: with the missing value removed." width="50%" />
<p class="caption">
Figure 9.6: Example data. Left: including the ‘datum’ (-9,9). Right: with the missing value removed.
</p>
</div>
<p>It is then clear that the large positive value of <span class="math inline">\(r=0.95\)</span> is due the missing value (-9,-9) being included in the plot. Once this point is removed the sample correlation is negative, <span class="math inline">\(r=-0.75\)</span>, as expected. A plot of the data with the missing value removed in shown on the right side of Figure <a href="correlationchapter.html#fig:corrmissing">9.6</a>.</p>
</div>
<div id="more-guessing-sample-correlations" class="section level3">
<h3><span class="header-section-number">9.3.4</span> More guessing sample correlations</h3>
<p>Figure <a href="correlationchapter.html#fig:guesscorr">9.7</a> gives some scatter plots with their sample correlation coefficients.</p>
<div class="figure" style="text-align: center"><span id="fig:guesscorr"></span>
<img src="images/guess_corr.png" alt="Some scatter plots and their sample correlation coefficient." width="80%" />
<p class="caption">
Figure 9.7: Some scatter plots and their sample correlation coefficient.
</p>
</div>
<p>Based on these plots, you will probably be surprised by the fact that for the data plotted in Figure <a href="correlationchapter.html#fig:guesscorrnew2">9.8</a> the value of the sample correlation coefficient is 0.991. Can you guess what has caused this surprising value?</p>
<div class="figure" style="text-align: center"><span id="fig:guesscorrnew2"></span>
<img src="images/guess_corr_new2.png" alt="Can you guess the value of the sample correlation coefficient?" width="50%" />
<p class="caption">
Figure 9.8: Can you guess the value of the sample correlation coefficient?
</p>
</div>
</div>
<div id="summary" class="section level3">
<h3><span class="header-section-number">9.3.5</span> Summary</h3>
<p>Correlation can be used when</p>
<ul>
<li>the data are a random sample from the joint distribution of <span class="math inline">\((X,Y)\)</span>,</li>
<li>the association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is approximately linear.</li>
</ul>
<p>Correlation should <strong>not</strong> be used, that is, it can be misleading, if</p>
<ul>
<li>the values of either of the variables are controlled by an experimenter,</li>
<li><span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are associated non-linearly.</li>
</ul>
</div>
<div id="anscombes-datasets" class="section level3">
<h3><span class="header-section-number">9.3.6</span> Anscombe’s datasets</h3>
<p>It is very important to plot data before fitting a linear regression model or estimating a correlation coefficient. In 1973 F.J. Anscombe (<span class="citation">Anscombe (<a href="#ref-Anscombe1973" role="doc-biblioref">1973</a>)</span>) created 4 datasets to illustrate the need for this. His data are given in Table <a href="correlationchapter.html#fig:anscombetable">9.9</a>. You discussed these data, and some other examples, in Tutorial 1.</p>
<div class="figure" style="text-align: center"><span id="fig:anscombetable"></span>
<img src="images/anscombetable.PNG" alt="Anscombe's datasets." width="60%" />
<p class="caption">
Figure 9.9: Anscombe’s datasets.
</p>
</div>
<p>The datasets have many things in common. For each of the 4 datasets:</p>
<ul>
<li>the sample size is 11,</li>
<li><span class="math inline">\(\bar{x}=9.00\)</span>, <span class="math inline">\(s_x=3.32\)</span>, <span class="math inline">\(\bar{y}=7.50\)</span> and <span class="math inline">\(s_y=2.03\)</span>,</li>
<li>the least squares linear regression line is <span class="math inline">\(y=3+0.5\,x\)</span>, with <span class="math inline">\(RSS\)</span>=13.75,</li>
<li>the sample correlation coefficient <span class="math inline">\(r=0.816\)</span>.</li>
<li>(By the way, the values of Spearman’s rank correlation coefficient <span class="math inline">\(r_S\)</span> are
0.818, 0.691, 0.991 and 0.500 respectively.)</li>
</ul>
<p>Based on these sample statistics we might be tempted to conclude that the association between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> is similar for each of the 4 datasets. However, when we look at scatter plots of the data (Figure <a href="correlationchapter.html#fig:anscombeplot">9.10</a>) it is clear that this is not the case. This demonstrates that summary statistics alone do not describe adequately a probability distribution.</p>
<div class="figure" style="text-align: center"><span id="fig:anscombeplot"></span>
<img src="images/anscombe.png" alt="Scatter plots of Anscombe's datasets. Top left: approximately linear association.  Top right: perfect non-linear association. Bottom left: Perfect linear association apart from 1 outlier.  Bottom right: the increase of 0.5 units in $y$ for each unit increase in $x$ suggested by the regression equation is due to a single observation." width="80%" />
<p class="caption">
Figure 9.10: Scatter plots of Anscombe’s datasets. Top left: approximately linear association. Top right: perfect non-linear association. Bottom left: Perfect linear association apart from 1 outlier. Bottom right: the increase of 0.5 units in <span class="math inline">\(y\)</span> for each unit increase in <span class="math inline">\(x\)</span> suggested by the regression equation is due to a single observation.
</p>
</div>
</div>
<div id="we-must-interpret-correlation-with-care." class="section level3">
<h3><span class="header-section-number">9.3.7</span> We must interpret correlation with care.</h3>
<p>Suppose that we find that two variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have a sample correlation which is not close to 0.
There are several ways that an apparent linear association can arise.</p>
<ol style="list-style-type: decimal">
<li>Changes in <span class="math inline">\(X\)</span> cause changes in <span class="math inline">\(Y\)</span>.</li>
<li>Changes in <span class="math inline">\(Y\)</span> cause changes in <span class="math inline">\(X\)</span>.</li>
<li>Changes in a third variable <span class="math inline">\(Z\)</span> causes changes in both <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</li>
<li>The observed association is just a coincidence.</li>
</ol>
<p>Points 3. and 4. show that correlation does <strong>not</strong> imply causation.</p>
<p>We finish with an example which may illustrate point 3. We have already seen the time series plots in Figure <a href="correlationchapter.html#fig:fluandFTSE">9.11</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:fluandFTSE"></span>
<img src="images/ftse_weekly_tufte.png" alt="Weekly FTSE 100 closing prices against numbers of people seeing their doctor about flu over 4-week periods." width="80%" /><img src="images/flu_tufte.png" alt="Weekly FTSE 100 closing prices against numbers of people seeing their doctor about flu over 4-week periods." width="80%" />
<p class="caption">
Figure 9.11: Weekly FTSE 100 closing prices against numbers of people seeing their doctor about flu over 4-week periods.
</p>
</div>
<p>For the period of time over which these series overlap, we extract from the FTSE 100 data the weekly closing price that corresponds to each of the 4-weekly flu values. In Figure <a href="correlationchapter.html#fig:fluvsFTSE">9.12</a> we plot (on a log-log scale) these FTSE values against the flu values.</p>
<div class="figure" style="text-align: center"><span id="fig:fluvsFTSE"></span>
<img src="images/flu_FTSE.png" alt="Weekly FTSE 100 closing prices against numbers of people seeing their doctor about flu over 4-week periods." width="60%" />
<p class="caption">
Figure 9.12: Weekly FTSE 100 closing prices against numbers of people seeing their doctor about flu over 4-week periods.
</p>
</div>
<p>We see that there is weak negative association (<span class="math inline">\(r=-0.32\)</span>) between the FTSE 100 and flu values. Does this suggest that flu causes the FTSE 100 share index to drop, or vice versa? This is not a ridiculous suggestion, but the time series plots suggest that the negative association is due to the fact that the FTSE 100 index has <strong>increased</strong> over time, whereas the number of people seeing their doctor about flu has <strong>decreased</strong> over time. There is a third variable, time, which causes changes in both the FTSE 100 and the incidence of flu.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Anscombe1973">
<p>Anscombe, F. J. 1973. “Graphs in Statistical Analysis.” <em>The American Statistician</em> 27 (1): 17–21. <a href="https://doi.org/10.1080/00031305.1973.10478966">https://doi.org/10.1080/00031305.1973.10478966</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linreg.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="a-general-strategy-for-statistical-modelling.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["stat0002book.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
