<!DOCTYPE html>
<html lang="en-gb" xml:lang="en-gb">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 10 Correlation | STAT0002 Introduction to Probability and Statistics</title>
  <meta name="description" content="Produces STAT0002 notes in an accessible format" />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 10 Correlation | STAT0002 Introduction to Probability and Statistics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Produces STAT0002 notes in an accessible format" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 10 Correlation | STAT0002 Introduction to Probability and Statistics" />
  
  <meta name="twitter:description" content="Produces STAT0002 notes in an accessible format" />
  

<meta name="author" content="Dr Paul Northrop" />


<meta name="date" content="2022-09-07" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="linreg.html"/>
<link rel="next" href="a-general-strategy-for-statistical-modelling.html"/>
<script src="libs/header-attrs-2.14/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">STAT0002 2020-21</a></li>

<li class="divider"></li>
<li><a href="index.html#the-purpose-of-these-notes" id="toc-the-purpose-of-these-notes">The purpose of these notes<span></span></a></li>
<li><a href="introduction.html#introduction" id="toc-introduction"><span class="toc-section-number">1</span> Introduction<span></span></a>
<ul>
<li><a href="introduction.html#real" id="toc-real"><span class="toc-section-number">1.1</span> Real statistical investigations<span></span></a></li>
<li><a href="introduction.html#shuttle" id="toc-shuttle"><span class="toc-section-number">1.2</span> Challenger Space Shuttle Catastrophe<span></span></a>
<ul>
<li><a href="introduction.html#uncertainty" id="toc-uncertainty"><span class="toc-section-number">1.2.1</span> Uncertainty<span></span></a></li>
</ul></li>
<li><a href="introduction.html#a-very-brief-introduction-to-stochastic-simulation" id="toc-a-very-brief-introduction-to-stochastic-simulation"><span class="toc-section-number">1.3</span> A very brief introduction to stochastic simulation<span></span></a></li>
</ul></li>
<li><a href="descriptive.html#descriptive" id="toc-descriptive"><span class="toc-section-number">2</span> Descriptive Statistics<span></span></a>
<ul>
<li><a href="descriptive.html#types-of-data" id="toc-types-of-data"><span class="toc-section-number">2.1</span> Types of data<span></span></a>
<ul>
<li><a href="descriptive.html#qualitative-or-categorical-data" id="toc-qualitative-or-categorical-data"><span class="toc-section-number">2.1.1</span> Qualitative or categorical data<span></span></a></li>
<li><a href="descriptive.html#quantitative-or-numerical-data" id="toc-quantitative-or-numerical-data"><span class="toc-section-number">2.1.2</span> Quantitative or numerical data<span></span></a></li>
</ul></li>
<li><a href="descriptive.html#describing-distributions" id="toc-describing-distributions"><span class="toc-section-number">2.2</span> Describing distributions<span></span></a>
<ul>
<li><a href="descriptive.html#example-oxford-births-data" id="toc-example-oxford-births-data">Example: Oxford births data<span></span></a></li>
</ul></li>
<li><a href="descriptive.html#summary-statistics" id="toc-summary-statistics"><span class="toc-section-number">2.3</span> Summary Statistics<span></span></a>
<ul>
<li><a href="descriptive.html#fivenumber" id="toc-fivenumber"><span class="toc-section-number">2.3.1</span> Five number summary<span></span></a></li>
<li><a href="descriptive.html#meanstdev" id="toc-meanstdev"><span class="toc-section-number">2.3.2</span> Mean and standard deviation<span></span></a></li>
<li><a href="descriptive.html#mode" id="toc-mode"><span class="toc-section-number">2.3.3</span> Mode<span></span></a></li>
<li><a href="descriptive.html#symmetry" id="toc-symmetry"><span class="toc-section-number">2.3.4</span> Symmetry<span></span></a></li>
<li><a href="descriptive.html#corr1" id="toc-corr1"><span class="toc-section-number">2.3.5</span> Correlation<span></span></a></li>
</ul></li>
<li><a href="descriptive.html#tables" id="toc-tables"><span class="toc-section-number">2.4</span> Tables<span></span></a>
<ul>
<li><a href="descriptive.html#frequency-distribution" id="toc-frequency-distribution"><span class="toc-section-number">2.4.1</span> Frequency distribution<span></span></a></li>
</ul></li>
<li><a href="descriptive.html#graphs" id="toc-graphs"><span class="toc-section-number">2.5</span> Graphs (1 variable)<span></span></a>
<ul>
<li><a href="descriptive.html#histogram" id="toc-histogram"><span class="toc-section-number">2.5.1</span> Histograms<span></span></a></li>
<li><a href="descriptive.html#stem" id="toc-stem"><span class="toc-section-number">2.5.2</span> Stem-and-leaf plots<span></span></a></li>
<li><a href="descriptive.html#dotplots" id="toc-dotplots"><span class="toc-section-number">2.5.3</span> Dotplots<span></span></a></li>
<li><a href="descriptive.html#boxplots" id="toc-boxplots"><span class="toc-section-number">2.5.4</span> Boxplots<span></span></a></li>
<li><a href="descriptive.html#barplots" id="toc-barplots"><span class="toc-section-number">2.5.5</span> Barplots<span></span></a></li>
<li><a href="descriptive.html#times-series-plots" id="toc-times-series-plots"><span class="toc-section-number">2.5.6</span> Times series plots<span></span></a></li>
</ul></li>
<li><a href="descriptive.html#election" id="toc-election"><span class="toc-section-number">2.6</span> 2000 US Presidential Election<span></span></a></li>
<li><a href="descriptive.html#graphs2" id="toc-graphs2"><span class="toc-section-number">2.7</span> Graphs (2 variables)<span></span></a>
<ul>
<li><a href="descriptive.html#scatter-plots" id="toc-scatter-plots"><span class="toc-section-number">2.7.1</span> Scatter plots<span></span></a></li>
</ul></li>
<li><a href="descriptive.html#transformation" id="toc-transformation"><span class="toc-section-number">2.8</span> Transformation of data<span></span></a>
<ul>
<li><a href="descriptive.html#transsymmetry" id="toc-transsymmetry"><span class="toc-section-number">2.8.1</span> Transformation to approximate symmetry<span></span></a></li>
<li><a href="descriptive.html#straighten" id="toc-straighten"><span class="toc-section-number">2.8.2</span> Straightening scatter plots<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="probability.html#probability" id="toc-probability"><span class="toc-section-number">3</span> Probability I<span></span></a>
<ul>
<li><a href="probability.html#sids" id="toc-sids"><span class="toc-section-number">3.1</span> Misleading statistical evidence in cot death trials<span></span></a></li>
<li><a href="probability.html#relative-frequency-definition-of-probability" id="toc-relative-frequency-definition-of-probability"><span class="toc-section-number">3.2</span> Relative frequency definition of probability<span></span></a></li>
<li><a href="probability.html#basic-properties-of-probability" id="toc-basic-properties-of-probability"><span class="toc-section-number">3.3</span> Basic properties of probability<span></span></a></li>
<li><a href="probability.html#conditional-probability" id="toc-conditional-probability"><span class="toc-section-number">3.4</span> Conditional probability<span></span></a></li>
<li><a href="probability.html#addition-rule-of-probability" id="toc-addition-rule-of-probability"><span class="toc-section-number">3.5</span> Addition rule of probability<span></span></a>
<ul>
<li><a href="probability.html#mutually-exclusive-events" id="toc-mutually-exclusive-events"><span class="toc-section-number">3.5.1</span> Mutually exclusive events<span></span></a></li>
</ul></li>
<li><a href="probability.html#multrule" id="toc-multrule"><span class="toc-section-number">3.6</span> Multiplication rule of probability<span></span></a></li>
<li><a href="probability.html#indepevents" id="toc-indepevents"><span class="toc-section-number">3.7</span> Independence of events<span></span></a>
<ul>
<li><a href="probability.html#bloodindep" id="toc-bloodindep"><span class="toc-section-number">3.7.1</span> An example of independence<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="probability-ii.html#probability-ii" id="toc-probability-ii"><span class="toc-section-number">4</span> Probability II<span></span></a>
<ul>
<li><a href="probability-ii.html#law-of-total-probability" id="toc-law-of-total-probability"><span class="toc-section-number">4.1</span> Law of total probability<span></span></a></li>
<li><a href="probability-ii.html#bayes-theorem" id="toc-bayes-theorem"><span class="toc-section-number">4.2</span> Bayes’ theorem<span></span></a></li>
<li><a href="probability-ii.html#dna-identification-evidence" id="toc-dna-identification-evidence"><span class="toc-section-number">4.3</span> DNA identification evidence<span></span></a></li>
</ul></li>
<li><a href="rvs.html#rvs" id="toc-rvs"><span class="toc-section-number">5</span> Random variables<span></span></a>
<ul>
<li><a href="rvs.html#discrete" id="toc-discrete"><span class="toc-section-number">5.1</span> Discrete random variables<span></span></a></li>
<li><a href="rvs.html#continuous" id="toc-continuous"><span class="toc-section-number">5.2</span> Continuous random variables<span></span></a></li>
<li><a href="rvs.html#expectation" id="toc-expectation"><span class="toc-section-number">5.3</span> Expectation<span></span></a>
<ul>
<li><a href="rvs.html#expectation-of-a-discrete-random-variable" id="toc-expectation-of-a-discrete-random-variable"><span class="toc-section-number">5.3.1</span> Expectation of a discrete random variable<span></span></a></li>
<li><a href="rvs.html#expectation-of-a-continuous-random-variable" id="toc-expectation-of-a-continuous-random-variable"><span class="toc-section-number">5.3.2</span> Expectation of a continuous random variable<span></span></a></li>
<li><a href="rvs.html#properties-of-mathrmex" id="toc-properties-of-mathrmex"><span class="toc-section-number">5.3.3</span> Properties of <span class="math inline">\(\mathrm{E}(X)\)</span><span></span></a></li>
<li><a href="rvs.html#EgX" id="toc-EgX"><span class="toc-section-number">5.3.4</span> The expectation of <span class="math inline">\(g(X)\)</span><span></span></a></li>
</ul></li>
<li><a href="rvs.html#variance" id="toc-variance"><span class="toc-section-number">5.4</span> Variance<span></span></a>
<ul>
<li><a href="rvs.html#variance-of-a-discrete-random-variable" id="toc-variance-of-a-discrete-random-variable"><span class="toc-section-number">5.4.1</span> Variance of a discrete random variable<span></span></a></li>
<li><a href="rvs.html#variance-of-a-continuous-random-variable" id="toc-variance-of-a-continuous-random-variable"><span class="toc-section-number">5.4.2</span> Variance of a continuous random variable<span></span></a></li>
<li><a href="rvs.html#variance-and-standard-deviation" id="toc-variance-and-standard-deviation"><span class="toc-section-number">5.4.3</span> Variance and standard deviation<span></span></a></li>
<li><a href="rvs.html#properties-of-mathrmvarx" id="toc-properties-of-mathrmvarx"><span class="toc-section-number">5.4.4</span> Properties of <span class="math inline">\(\mathrm{var}(X)\)</span><span></span></a></li>
</ul></li>
<li><a href="rvs.html#locations" id="toc-locations"><span class="toc-section-number">5.5</span> Other measures of location<span></span></a>
<ul>
<li><a href="rvs.html#the-median-of-a-random-variable" id="toc-the-median-of-a-random-variable"><span class="toc-section-number">5.5.1</span> The median of a random variable<span></span></a></li>
<li><a href="rvs.html#the-mode-of-a-random-variable" id="toc-the-mode-of-a-random-variable"><span class="toc-section-number">5.5.2</span> The mode of a random variable<span></span></a></li>
</ul></li>
<li><a href="rvs.html#quantiles" id="toc-quantiles"><span class="toc-section-number">5.6</span> Quantiles<span></span></a></li>
<li><a href="rvs.html#measures-of-shape" id="toc-measures-of-shape"><span class="toc-section-number">5.7</span> Measures of shape<span></span></a></li>
</ul></li>
<li><a href="simple.html#simple" id="toc-simple"><span class="toc-section-number">6</span> Simple distributions<span></span></a>
<ul>
<li><a href="simple.html#australian-births-data" id="toc-australian-births-data"><span class="toc-section-number">6.1</span> Australian births data<span></span></a></li>
<li><a href="simple.html#the-bernoulli-distribution" id="toc-the-bernoulli-distribution"><span class="toc-section-number">6.2</span> The Bernoulli distribution<span></span></a>
<ul>
<li><a href="simple.html#summary-of-the-bernoullip-distribution" id="toc-summary-of-the-bernoullip-distribution"><span class="toc-section-number">6.2.1</span> Summary of the Bernoulli(<span class="math inline">\(p\)</span>) distribution<span></span></a></li>
</ul></li>
<li><a href="simple.html#binomial" id="toc-binomial"><span class="toc-section-number">6.3</span> The binomial distribution<span></span></a>
<ul>
<li><a href="simple.html#binominf" id="toc-binominf"><span class="toc-section-number">6.3.1</span> A brief look at statistical inference about <span class="math inline">\(p\)</span><span></span></a></li>
<li><a href="simple.html#summary-of-the-binomialnp-distribution" id="toc-summary-of-the-binomialnp-distribution"><span class="toc-section-number">6.3.2</span> Summary of the binomial(<span class="math inline">\(n,p\)</span>) distribution<span></span></a></li>
</ul></li>
<li><a href="simple.html#the-geometric-distribution" id="toc-the-geometric-distribution"><span class="toc-section-number">6.4</span> The geometric distribution<span></span></a>
<ul>
<li><a href="simple.html#summary-of-the-geometricp-distribution" id="toc-summary-of-the-geometricp-distribution"><span class="toc-section-number">6.4.1</span> Summary of the geometric(<span class="math inline">\(p\)</span>) distribution<span></span></a></li>
</ul></li>
<li><a href="simple.html#Poisson" id="toc-Poisson"><span class="toc-section-number">6.5</span> The Poisson distribution<span></span></a>
<ul>
<li><a href="simple.html#summary-of-the-poissonmu-distribution" id="toc-summary-of-the-poissonmu-distribution"><span class="toc-section-number">6.5.1</span> Summary of the Poisson(<span class="math inline">\(\mu\)</span>) distribution<span></span></a></li>
</ul></li>
<li><a href="simple.html#summary-of-these-discrete-distributions" id="toc-summary-of-these-discrete-distributions"><span class="toc-section-number">6.6</span> Summary of these discrete distributions<span></span></a></li>
<li><a href="simple.html#uniform" id="toc-uniform"><span class="toc-section-number">6.7</span> The uniform distribution<span></span></a>
<ul>
<li><a href="simple.html#summary-of-the-uniformab-distribution" id="toc-summary-of-the-uniformab-distribution"><span class="toc-section-number">6.7.1</span> Summary of the uniform(<span class="math inline">\(a,b\)</span>) distribution<span></span></a></li>
</ul></li>
<li><a href="simple.html#exponential" id="toc-exponential"><span class="toc-section-number">6.8</span> The exponential distribution<span></span></a>
<ul>
<li><a href="simple.html#summary-of-the-exponentiallambda-distribution" id="toc-summary-of-the-exponentiallambda-distribution"><span class="toc-section-number">6.8.1</span> Summary of the exponential(<span class="math inline">\(\lambda\)</span>) distribution<span></span></a></li>
</ul></li>
<li><a href="simple.html#normal" id="toc-normal"><span class="toc-section-number">6.9</span> The normal distribution<span></span></a>
<ul>
<li><a href="simple.html#summary-of-the-mboxnmusigma2-distribution" id="toc-summary-of-the-mboxnmusigma2-distribution"><span class="toc-section-number">6.9.1</span> Summary of the <span class="math inline">\(\mbox{N}(\mu,\sigma^2)\)</span> distribution<span></span></a></li>
<li><a href="simple.html#the-standard-normal-disribution" id="toc-the-standard-normal-disribution"><span class="toc-section-number">6.9.2</span> The standard normal disribution<span></span></a></li>
<li><a href="simple.html#evaluating-the-normal-c.d.f.-and-quantiles" id="toc-evaluating-the-normal-c.d.f.-and-quantiles"><span class="toc-section-number">6.9.3</span> Evaluating the normal c.d.f. and quantiles<span></span></a></li>
<li><a href="simple.html#interpretation-of-sigma" id="toc-interpretation-of-sigma"><span class="toc-section-number">6.9.4</span> Interpretation of <span class="math inline">\(\sigma\)</span><span></span></a></li>
</ul></li>
<li><a href="simple.html#summary-of-these-continuous-distributions" id="toc-summary-of-these-continuous-distributions"><span class="toc-section-number">6.10</span> Summary of these continuous distributions<span></span></a></li>
<li><a href="simple.html#qq" id="toc-qq"><span class="toc-section-number">6.11</span> QQ plots<span></span></a>
<ul>
<li><a href="simple.html#normal-qq-plots" id="toc-normal-qq-plots"><span class="toc-section-number">6.11.1</span> Normal QQ plots<span></span></a></li>
<li><a href="simple.html#uniform-qq-plots" id="toc-uniform-qq-plots"><span class="toc-section-number">6.11.2</span> Uniform QQ plots<span></span></a></li>
<li><a href="simple.html#exponential-qq-plots" id="toc-exponential-qq-plots"><span class="toc-section-number">6.11.3</span> Exponential QQ plots<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="inference.html#inference" id="toc-inference"><span class="toc-section-number">7</span> Statistical Inference<span></span></a>
<ul>
<li><a href="inference.html#the-story-so-far" id="toc-the-story-so-far"><span class="toc-section-number">7.1</span> The story so far<span></span></a></li>
<li><a href="inference.html#sample-and-populations" id="toc-sample-and-populations"><span class="toc-section-number">7.2</span> Sample and populations<span></span></a></li>
<li><a href="inference.html#probmodels" id="toc-probmodels"><span class="toc-section-number">7.3</span> Probability models<span></span></a></li>
<li><a href="inference.html#fitting-models" id="toc-fitting-models"><span class="toc-section-number">7.4</span> Fitting models<span></span></a></li>
<li><a href="inference.html#uncertainty-in-estimation" id="toc-uncertainty-in-estimation"><span class="toc-section-number">7.5</span> Uncertainty in estimation<span></span></a>
<ul>
<li><a href="inference.html#simulation-coin-tossing-example" id="toc-simulation-coin-tossing-example"><span class="toc-section-number">7.5.1</span> Simulation: coin-tossing example<span></span></a></li>
<li><a href="inference.html#simnorm" id="toc-simnorm"><span class="toc-section-number">7.5.2</span> Simulation: estimating the parameters of a normal distribution<span></span></a></li>
<li><a href="inference.html#simexp" id="toc-simexp"><span class="toc-section-number">7.5.3</span> Simulation: estimating the parameters of an exponential distribution<span></span></a></li>
<li><a href="inference.html#central-limit-theorem" id="toc-central-limit-theorem"><span class="toc-section-number">7.5.4</span> Central Limit Theorem<span></span></a></li>
</ul></li>
<li><a href="inference.html#good" id="toc-good"><span class="toc-section-number">7.6</span> Properties of estimators<span></span></a>
<ul>
<li><a href="inference.html#bias" id="toc-bias"><span class="toc-section-number">7.6.1</span> Bias<span></span></a></li>
<li><a href="inference.html#varianceofestimator" id="toc-varianceofestimator"><span class="toc-section-number">7.6.2</span> Variance<span></span></a></li>
<li><a href="inference.html#mean-squared-error-mse" id="toc-mean-squared-error-mse"><span class="toc-section-number">7.6.3</span> Mean squared error (MSE)<span></span></a></li>
<li><a href="inference.html#standard-error" id="toc-standard-error"><span class="toc-section-number">7.6.4</span> Standard error<span></span></a></li>
<li><a href="inference.html#consistency" id="toc-consistency"><span class="toc-section-number">7.6.5</span> Consistency<span></span></a></li>
</ul></li>
<li><a href="inference.html#assessing-goodness-of-fit" id="toc-assessing-goodness-of-fit"><span class="toc-section-number">7.7</span> Assessing goodness-of-fit<span></span></a>
<ul>
<li><a href="inference.html#residuals" id="toc-residuals"><span class="toc-section-number">7.7.1</span> Residuals<span></span></a></li>
<li><a href="inference.html#standardised-residuals" id="toc-standardised-residuals"><span class="toc-section-number">7.7.2</span> Standardised residuals<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="contingency.html#contingency" id="toc-contingency"><span class="toc-section-number">8</span> Contingency tables<span></span></a>
<ul>
<li><a href="contingency.html#way2" id="toc-way2"><span class="toc-section-number">8.1</span> 2-way contingency tables<span></span></a>
<ul>
<li><a href="contingency.html#indep" id="toc-indep"><span class="toc-section-number">8.1.1</span> Independence<span></span></a></li>
<li><a href="contingency.html#compprob" id="toc-compprob"><span class="toc-section-number">8.1.2</span> Comparing probabilities<span></span></a></li>
<li><a href="contingency.html#measures" id="toc-measures"><span class="toc-section-number">8.1.3</span> Measures of association<span></span></a></li>
</ul></li>
<li><a href="contingency.html#way3" id="toc-way3"><span class="toc-section-number">8.2</span> 3-way contingency tables<span></span></a>
<ul>
<li><a href="contingency.html#mutual-independence" id="toc-mutual-independence"><span class="toc-section-number">8.2.1</span> Mutual independence<span></span></a></li>
<li><a href="contingency.html#marginal-independence" id="toc-marginal-independence"><span class="toc-section-number">8.2.2</span> Marginal independence<span></span></a></li>
<li><a href="contingency.html#conditional-independence" id="toc-conditional-independence"><span class="toc-section-number">8.2.3</span> Conditional independence<span></span></a></li>
<li><a href="contingency.html#confounding-variables" id="toc-confounding-variables"><span class="toc-section-number">8.2.4</span> Confounding variables<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="linreg.html#linreg" id="toc-linreg"><span class="toc-section-number">9</span> Linear regression<span></span></a>
<ul>
<li><a href="linreg.html#simple-linear-regression" id="toc-simple-linear-regression"><span class="toc-section-number">9.1</span> Simple linear regression<span></span></a>
<ul>
<li><a href="linreg.html#simple-linear-regression-model" id="toc-simple-linear-regression-model"><span class="toc-section-number">9.1.1</span> Simple linear regression model<span></span></a></li>
<li><a href="linreg.html#least-squares-estimation-of-alpha-and-beta" id="toc-least-squares-estimation-of-alpha-and-beta"><span class="toc-section-number">9.1.2</span> Least squares estimation of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span><span></span></a></li>
<li><a href="linreg.html#least-squares-fitting-to-hubbles-data" id="toc-least-squares-fitting-to-hubbles-data"><span class="toc-section-number">9.1.3</span> Least squares fitting to Hubble’s data<span></span></a></li>
<li><a href="linreg.html#normal-linear-regression-model" id="toc-normal-linear-regression-model"><span class="toc-section-number">9.1.4</span> Normal linear regression model<span></span></a></li>
<li><a href="linreg.html#lmsummary" id="toc-lmsummary"><span class="toc-section-number">9.1.5</span> Summary of the assumptions of a (normal) linear regression model<span></span></a></li>
</ul></li>
<li><a href="linreg.html#looking" id="toc-looking"><span class="toc-section-number">9.2</span> Looking at scatter plots<span></span></a></li>
<li><a href="linreg.html#model-checking" id="toc-model-checking"><span class="toc-section-number">9.3</span> Model checking<span></span></a>
<ul>
<li><a href="linreg.html#departures-from-assumptions" id="toc-departures-from-assumptions"><span class="toc-section-number">9.3.1</span> Departures from assumptions<span></span></a></li>
<li><a href="linreg.html#outliers" id="toc-outliers"><span class="toc-section-number">9.3.2</span> Outliers and influential observations<span></span></a></li>
</ul></li>
<li><a href="linreg.html#linregtrans" id="toc-linregtrans"><span class="toc-section-number">9.4</span> Use of transformations<span></span></a>
<ul>
<li><a href="linreg.html#interpretation-after-transformation" id="toc-interpretation-after-transformation"><span class="toc-section-number">9.4.1</span> Interpretation after transformation<span></span></a></li>
</ul></li>
<li><a href="linreg.html#over-fitting" id="toc-over-fitting"><span class="toc-section-number">9.5</span> Over-fitting<span></span></a></li>
</ul></li>
<li><a href="correlationchapter.html#correlationchapter" id="toc-correlationchapter"><span class="toc-section-number">10</span> Correlation<span></span></a>
<ul>
<li><a href="correlationchapter.html#correlation-a-measure-of-linear-association" id="toc-correlation-a-measure-of-linear-association"><span class="toc-section-number">10.1</span> Correlation: a measure of linear association<span></span></a></li>
<li><a href="correlationchapter.html#covariance-and-correlation" id="toc-covariance-and-correlation"><span class="toc-section-number">10.2</span> Covariance and correlation<span></span></a>
<ul>
<li><a href="correlationchapter.html#estimation" id="toc-estimation"><span class="toc-section-number">10.2.1</span> Estimation<span></span></a></li>
<li><a href="correlationchapter.html#links-between-regression-and-correlation" id="toc-links-between-regression-and-correlation"><span class="toc-section-number">10.2.2</span> Links between regression and correlation<span></span></a></li>
</ul></li>
<li><a href="correlationchapter.html#use-and-misuse-of-correlation" id="toc-use-and-misuse-of-correlation"><span class="toc-section-number">10.3</span> Use and misuse of correlation<span></span></a>
<ul>
<li><a href="correlationchapter.html#do-not-use-correlation-for-regression-sampling-schemes" id="toc-do-not-use-correlation-for-regression-sampling-schemes"><span class="toc-section-number">10.3.1</span> Do not use correlation for regression sampling schemes<span></span></a></li>
<li><a href="correlationchapter.html#correxamples" id="toc-correxamples"><span class="toc-section-number">10.3.2</span> Examples of correlations of different strengths<span></span></a></li>
<li><a href="correlationchapter.html#beware-missing-data-codes" id="toc-beware-missing-data-codes"><span class="toc-section-number">10.3.3</span> Beware missing data codes<span></span></a></li>
<li><a href="correlationchapter.html#more-guessing-sample-correlations" id="toc-more-guessing-sample-correlations"><span class="toc-section-number">10.3.4</span> More guessing sample correlations<span></span></a></li>
<li><a href="correlationchapter.html#summary" id="toc-summary"><span class="toc-section-number">10.3.5</span> Summary<span></span></a></li>
<li><a href="correlationchapter.html#anscombes-datasets" id="toc-anscombes-datasets"><span class="toc-section-number">10.3.6</span> Anscombe’s datasets<span></span></a></li>
<li><a href="correlationchapter.html#we-must-interpret-correlation-with-care." id="toc-we-must-interpret-correlation-with-care."><span class="toc-section-number">10.3.7</span> We must interpret correlation with care.<span></span></a></li>
</ul></li>
</ul></li>
<li><a href="a-general-strategy-for-statistical-modelling.html#a-general-strategy-for-statistical-modelling" id="toc-a-general-strategy-for-statistical-modelling"><span class="toc-section-number">11</span> A general strategy for statistical modelling<span></span></a></li>
<li><a href="references.html#references" id="toc-references">References<span></span></a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">STAT0002 Introduction to Probability and Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="correlationchapter" class="section level1 hasAnchor" number="10">
<h1><span class="header-section-number">Chapter 10</span> Correlation<a href="correlationchapter.html#correlationchapter" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Now we consider the situation where objects are sampled randomly from a population, and, for each object, we observe the values of random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Therefore, unlike regression sampling, where the values of <span class="math inline">\(X\)</span> are chosen by an experimenter, both <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are random variables. Unlike a regression problem, we treat <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> symmetrically, that is, we do not identify one variable as a response and one as explanatory. We are interested in the relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. How are they associated, and how strongly?</p>
<p>When estimating correlation from data we will concentrate on the sample correlation coefficient <span class="math inline">\(r\)</span> defined in equation <a href="descriptive.html#eq:corr">(2.1)</a> of Section <a href="descriptive.html#corr1">2.3.5</a>, which is a measure of the strength of <strong>linear</strong> association. However, the general comments that we make will also apply to Spearman’s rank correlation coefficient <span class="math inline">\(r_S\)</span> (also defined in section <a href="descriptive.html#corr1">2.3.5</a>.</p>
<div id="correlation-a-measure-of-linear-association" class="section level2 hasAnchor" number="10.1">
<h2><span class="header-section-number">10.1</span> Correlation: a measure of linear association<a href="correlationchapter.html#correlation-a-measure-of-linear-association" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>UK/USA/Canadian exchange rates</strong></p>
<p>Figure <a href="correlationchapter.html#fig:exchangeraw">10.1</a> is a plot the UK Pound/Canadian dollar exchange rate vs. the UK Pound/US dollar exchange rate for each day between 2nd January 1997 and 21 November 2000.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:exchangeraw"></span>
<img src="images/exchange_raw.png" alt="Value of 1 UK pound in Canadian dollars against value of 1 UK pound in US dollars." width="75%" />
<p class="caption">
Figure 10.1: Value of 1 UK pound in Canadian dollars against value of 1 UK pound in US dollars.
</p>
</div>
<p>The relationship between these two variables is complicated. However, when dealing with exchange rates it is common to (a) work with <span class="math inline">\(\log\)</span>(exchange rate), and (b) use the <strong>change</strong> in <span class="math inline">\(\log\)</span>(exchange rate) from one day to the next. This produces quantities called <strong>log-returns</strong>. In Figure <a href="correlationchapter.html#fig:exchangetrans">10.2</a> the log-returns for the UK Pound/Canadian dollar exchange rate (<span class="math inline">\(Y\)</span>) are plotted against the log-returns <span class="math inline">\(X\)</span> for the UK Pound/US exchange rate (<span class="math inline">\(X\)</span>).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:exchangetrans"></span>
<img src="images/exchange.png" alt="Plot of $Y$ against $X$.  The vertical line is drawn at the sample mean of the $X$ data and the horizontal line at the sample mean of the $Y$ data." width="75%" />
<p class="caption">
Figure 10.2: Plot of <span class="math inline">\(Y\)</span> against <span class="math inline">\(X\)</span>. The vertical line is drawn at the sample mean of the <span class="math inline">\(X\)</span> data and the horizontal line at the sample mean of the <span class="math inline">\(Y\)</span> data.
</p>
</div>
<p>This plot has a much clearer pattern. There is positive, approximately linear, association between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>. Large values of the variables tend to occur together, as do small values. There are more points in the quadrants labelled B and C than in A and D. This positive association is due to the link between the US and Canadian financial markets. If you are investing in these markets it is important that you are aware of this positive association, since it means that when you lose money you are likely to lose it in both the US market and the Canadian market.</p>
<p>Figure <a href="correlationchapter.html#fig:exchangetrans">10.2</a> suggests that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are approximately positively, linearly associated. Correlation measures the strength of this linear association.</p>
</div>
<div id="covariance-and-correlation" class="section level2 hasAnchor" number="10.2">
<h2><span class="header-section-number">10.2</span> Covariance and correlation<a href="correlationchapter.html#covariance-and-correlation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Definition</strong>. Let <span class="math inline">\(\mu_X=\mbox{E}(X)\)</span> and <span class="math inline">\(\mu_Y=\mbox{E}(Y)\)</span>. The <strong>covariance</strong> <span class="math inline">\(\mbox{cov}(X,Y)\)</span> between two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is given by
<span class="math display">\[
\mbox{cov}(X,Y)=\mbox{E}\left[\left(X-\mu_X\right)\left(Y-\mu_Y\right)\right]=\mbox{E}(XY)-\mbox{E}(X)\mbox{E}(Y).
\]</span>
The value of <span class="math inline">\(\mbox{cov}(X,Y)\)</span> depends on the scale of measurement of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. For example, if we were to multiply all the values of <span class="math inline">\(X\)</span> by 2 then <span class="math inline">\(\mbox{cov}(X,Y)\)</span> is also multiplied by 2. Therefore we define a standardised version of covariance: correlation.</p>
<p><strong>Definition</strong>. The <strong>correlation (coefficient)</strong> between two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is given by
<span class="math display">\[
\rho = \mbox{corr}(X,Y) = \frac{\mbox{cov}(X,Y)}{\mbox{sd}(X)\mbox{sd}(Y)}
= \frac{\mbox{E}\left[\left(X-\mu_X\right)\left(Y-\mu_Y\right)\right]}{\mbox{sd}(X)\mbox{sd}(Y)},
\]</span>
provided that both <span class="math inline">\(\mbox{sd}(X)\)</span> and <span class="math inline">\(\mbox{sd}(Y)\)</span> are positive. Correlation is dimensionless (it has no units) and its value does not change if we scale <span class="math inline">\(X\)</span> and/or <span class="math inline">\(Y\)</span>. Correlation (and covariance) are measures of <strong>linear</strong> association between random variables. They are a sensible measure of the strength of association between two random variables only if there is a linear relationship between them.</p>
<p>Interpretation of <span class="math inline">\(\rho\)</span>:</p>
<ul>
<li><span class="math inline">\(-1 \leq \rho \leq 1\)</span>.</li>
<li>If <span class="math inline">\(0 &lt; \rho \leq 1\)</span> then there is positive association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.
The larger the value of <span class="math inline">\(\rho\)</span> the stronger the positive association.</li>
<li>If <span class="math inline">\(-1 \leq \rho &lt; 0\)</span> then there is negative association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.
The smaller the value of <span class="math inline">\(\rho\)</span> the stronger the negative association.</li>
<li>If <span class="math inline">\(|\rho|=1\)</span> there is a perfect linear relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, that is
<span class="math inline">\(Y=\alpha\,X+\beta\)</span>. If <span class="math inline">\(\rho=1\)</span> then <span class="math inline">\(\alpha&gt;0\)</span>. If <span class="math inline">\(\rho=-1\)</span> then <span class="math inline">\(\alpha&lt;0\)</span>.</li>
<li><span class="math inline">\(\rho = 0\)</span> does <strong>not</strong> indicate no association, just a lack of
<strong>linear</strong> association. If <span class="math inline">\(\rho\)</span> is zero, or close to zero, it could be that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are strongly non-linearly associated.</li>
<li>If <span class="math inline">\(\rho=0\)</span> we say that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <strong>uncorrelated</strong>.</li>
</ul>
<div id="estimation" class="section level3 hasAnchor" number="10.2.1">
<h3><span class="header-section-number">10.2.1</span> Estimation<a href="correlationchapter.html#estimation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose that we have a random sample <span class="math inline">\((X_1,Y_1), ..., (X_n,Y_n)\)</span> from the joint distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. The <strong>sample correlation coefficient</strong>
<span class="math display">\[
R = \hat{\rho} = \frac{\displaystyle\sum_{i=1}^n (X_i-\bar{X})(Y_i-\bar{Y})}
{\sqrt{\displaystyle\sum_{i=1}^n (X_i-\bar{X})^2 \displaystyle\sum_{i=1}^n(Y_i-\bar{Y})^2}}
= \frac{C_{XY}}{\sqrt{C_{XX}\,C_{YY}}},
\]</span>
is an estimator of the correlation coefficient <span class="math inline">\(\rho\)</span>.</p>
<p>For the data in Figure <a href="correlationchapter.html#fig:exchangetrans">10.2</a> we get <span class="math inline">\(r=0.82\)</span>. This confirms the quite strong positive linear association apparent in the plot. Why do these data give a value of <span class="math inline">\(r\)</span> which is positive? We consider contributions of individual data points to the numerator
<span class="math display">\[  C_{xy}=\displaystyle\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y}) \]</span>
of <span class="math inline">\(r\)</span>. Points in quadrant</p>
<ul>
<li>B will have <span class="math inline">\(x_i-\bar{x}&gt;0\)</span> and <span class="math inline">\(y_i-\bar{y}&gt;0\)</span>. Therefore <span class="math inline">\((x_i-\bar{x})(y_i-\bar{y})&gt;0\)</span>.<br />
</li>
<li>C will have <span class="math inline">\(x_i-\bar{x}&lt;0\)</span> and <span class="math inline">\(y_i-\bar{y}&lt;0\)</span>. Therefore <span class="math inline">\((x_i-\bar{x})(y_i-\bar{y})&gt;0\)</span>.<br />
</li>
<li>A will have <span class="math inline">\(x_i-\bar{x}&lt;0\)</span> and <span class="math inline">\(y_i-\bar{y}&gt;0\)</span>. Therefore <span class="math inline">\((x_i-\bar{x})(y_i-\bar{y})&lt;0\)</span>.<br />
</li>
<li>D will have <span class="math inline">\(x_i-\bar{x}&gt;0\)</span> and <span class="math inline">\(y_i-\bar{y}&lt;0\)</span>. Therefore <span class="math inline">\((x_i-\bar{x})(y_i-\bar{y})&lt;0\)</span>.</li>
</ul>
<p>Therefore, since <span class="math inline">\(\sqrt{C_{xx}\,C_{yy}}&gt;0\)</span>, and <span class="math inline">\(C_{xy}\)</span> is a sum of contributions from all pairs <span class="math inline">\((x_i,y_i)\)</span>:</p>
<ul>
<li>If most of the observed points lie in quadrants B and C then <span class="math inline">\(r\)</span> is positive.</li>
<li>If most of the observed points lie in quadrants A and D then <span class="math inline">\(r\)</span> is negative.</li>
<li>If the numbers of points in each quadrant are approximately equal then <span class="math inline">\(r\)</span> is close to 0.</li>
</ul>
</div>
<div id="links-between-regression-and-correlation" class="section level3 hasAnchor" number="10.2.2">
<h3><span class="header-section-number">10.2.2</span> Links between regression and correlation<a href="correlationchapter.html#links-between-regression-and-correlation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Regression and correlation answer different questions. However, in the case of simple linear regression there are simple links between them. If we regress <span class="math inline">\(Y\)</span> on <span class="math inline">\(x\)</span> then</p>
<ol style="list-style-type: decimal">
<li>the sample coefficient of determination <span class="math inline">\(R^2\)</span> (<span class="math inline">\(R^2\)</span> is just a name: this is not literally the square of a quantity <span class="math inline">\(R\)</span>) is equal to <span class="math inline">\(r^2\)</span>, the square of the sample correlation coefficient,</li>
<li>the estimate of the regression slope <span class="math inline">\(\beta\)</span> is related to the sample correlation coefficient via
<span class="math display">\[ \hat{\beta} = r \,\sqrt{\frac{C_{yy}}{C_{xx}}}. \]</span></li>
</ol>
<p>These relationships make general sense.</p>
<ul>
<li>If <span class="math inline">\(R^2\)</span> is large then the linear regression of <span class="math inline">\(Y\)</span> on <span class="math inline">\(x\)</span> explains a lot of variability in the <span class="math inline">\(Y\)</span> data and so the correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> must be strong.</li>
<li>The value of <span class="math inline">\(\hat{\beta}\)</span> and the value of <span class="math inline">\(r\)</span> have the same sign.</li>
</ul>
</div>
</div>
<div id="use-and-misuse-of-correlation" class="section level2 hasAnchor" number="10.3">
<h2><span class="header-section-number">10.3</span> Use and misuse of correlation<a href="correlationchapter.html#use-and-misuse-of-correlation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We must be careful to use correlation only when it is appropriate. When it is used we must be careful to interpret its value carefully. In this section we give at some examples to show why we must use correlation with care.</p>
<p>One golden rule is that we should <strong>always plots the data</strong>.</p>
<div id="do-not-use-correlation-for-regression-sampling-schemes" class="section level3 hasAnchor" number="10.3.1">
<h3><span class="header-section-number">10.3.1</span> Do not use correlation for regression sampling schemes<a href="correlationchapter.html#do-not-use-correlation-for-regression-sampling-schemes" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We should <strong>not</strong> to use correlation to summarise association between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> when regression sampling has been used, that is, where the values of <span class="math inline">\(X\)</span> are <strong>chosen</strong> and then the values of <span class="math inline">\(Y\)</span> are observed. We use simulation to show us why this is. Figure <a href="correlationchapter.html#fig:xdesigncorr0">10.3</a> shows a random sample of size 100 from a joint distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> for which <span class="math inline">\(\rho=0.5\)</span>. The sample correlation coefficient is 0.51.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:xdesigncorr0"></span>
<img src="images/design_corr0.png" alt="A random sample of size 100 from a distribution with $\rho=0.5$." width="50%" />
<p class="caption">
Figure 10.3: A random sample of size 100 from a distribution with <span class="math inline">\(\rho=0.5\)</span>.
</p>
</div>
<p>In Figure <a href="correlationchapter.html#fig:xdesigncorr">10.4</a> regression samples of size 10 have been taken from the <strong>same</strong> joint distribution, but</p>
<ul>
<li>on the left the values of <span class="math inline">\(x\)</span> are <strong>chosen</strong> to be <span class="math inline">\(-2\)</span> and <span class="math inline">\(+2\)</span>. We get <span class="math inline">\(r=0.84\)</span>.</li>
<li>on the right the values of <span class="math inline">\(x\)</span> are <strong>chosen</strong> to be <span class="math inline">\(-0.1\)</span> and <span class="math inline">\(+0.1\)</span>. We get <span class="math inline">\(r=-0.46\)</span>.</li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:xdesigncorr"></span>
<img src="images/design_corr.PNG" alt="Left: $x$-values chosen to be only -2 or 2.  Right: $x$-values chosen to be only -0.1 or 0.1." width="100%" />
<p class="caption">
Figure 10.4: Left: <span class="math inline">\(x\)</span>-values chosen to be only -2 or 2. Right: <span class="math inline">\(x\)</span>-values chosen to be only -0.1 or 0.1.
</p>
</div>
<p>The values chosen for <span class="math inline">\(x\)</span> have a huge effect on <span class="math inline">\(r\)</span>. By choosing the values of <span class="math inline">\(x\)</span> to be spread far apart we can make <span class="math inline">\(r\)</span> as close to <span class="math inline">\(+1\)</span> as we wish. If the values of <span class="math inline">\(x\)</span> are chosen to be close together, <span class="math inline">\(r\)</span> will be quite variable, but close to zero on average.</p>
<p><strong>Summary</strong></p>
<p>If regression sampling is used the value of <span class="math inline">\(r\)</span> depends on the values of <span class="math inline">\(x\)</span> chosen.</p>
</div>
<div id="correxamples" class="section level3 hasAnchor" number="10.3.2">
<h3><span class="header-section-number">10.3.2</span> Examples of correlations of different strengths<a href="correlationchapter.html#correxamples" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Figure <a href="correlationchapter.html#fig:corr8">10.5</a> gives random samples from distributions with correlations <span class="math inline">\(\rho=0\)</span> (2 of them), <span class="math inline">\(\rho=\pm 0.3, \rho=\pm 0.7\)</span> and <span class="math inline">\(\rho=\pm 1\)</span>. The data in the bottom plots in Figure <a href="correlationchapter.html#fig:corr8">10.5</a> are both sampled from distributions with <span class="math inline">\(\rho=0\)</span>. However, the associations between the variables are very different. On the left there is no association. On the right there is quite strong non-linear association.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:corr8"></span>
<img src="images/corr8c.png" alt="Samples from distributions with correlations $
ho=0$ (2 of them), $
ho=\pm 0.3, 
ho=\pm 0.7$ and $
ho=\pm 1$." width="80%" /><img src="images/corr8d.png" alt="Samples from distributions with correlations $
ho=0$ (2 of them), $
ho=\pm 0.3, 
ho=\pm 0.7$ and $
ho=\pm 1$." width="80%" />
<p class="caption">
Figure 10.5: Samples from distributions with correlations $
ho=0$ (2 of them), $
ho=,
ho=$ and $
ho=$.
</p>
</div>
</div>
<div id="beware-missing-data-codes" class="section level3 hasAnchor" number="10.3.3">
<h3><span class="header-section-number">10.3.3</span> Beware missing data codes<a href="correlationchapter.html#beware-missing-data-codes" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Most real datasets have <strong>missing observations</strong>. For example, people may decide not to answer some of the questions on a questionnaire. It is quite common to use a missing value code (such as -9) to identify a missing value. We need to be careful not to confuse a numerical missing value code with actual data.</p>
<p>Consider the following situation, which is based on a real example.Someone is given data on two variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. They calculate the sample correlation coefficient <span class="math inline">\(r\)</span> and find that it is 0.95. They go back to the person who gave them the data and tell them that<span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are strongly positively associated. The person is surprised: they expected the variables to be negatively associated. So they produce a plot of the data, shown on the left side of Figure <a href="correlationchapter.html#fig:corrmissing">10.6</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:corrmissing"></span>
<img src="images/corr_missing1.png" alt="Example data. Left: including the 'datum' (-9,9). Right: with the missing value removed." width="50%" /><img src="images/corr_missing2.png" alt="Example data. Left: including the 'datum' (-9,9). Right: with the missing value removed." width="50%" />
<p class="caption">
Figure 10.6: Example data. Left: including the ‘datum’ (-9,9). Right: with the missing value removed.
</p>
</div>
<p>It is then clear that the large positive value of <span class="math inline">\(r=0.95\)</span> is due the missing value (-9,-9) being included in the plot. Once this point is removed the sample correlation is negative, <span class="math inline">\(r=-0.75\)</span>, as expected. A plot of the data with the missing value removed in shown on the right side of Figure <a href="correlationchapter.html#fig:corrmissing">10.6</a>.</p>
</div>
<div id="more-guessing-sample-correlations" class="section level3 hasAnchor" number="10.3.4">
<h3><span class="header-section-number">10.3.4</span> More guessing sample correlations<a href="correlationchapter.html#more-guessing-sample-correlations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Figure <a href="correlationchapter.html#fig:guesscorr">10.7</a> gives some scatter plots with their sample correlation coefficients.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:guesscorr"></span>
<img src="images/guess_corr.png" alt="Some scatter plots and their sample correlation coefficient." width="80%" />
<p class="caption">
Figure 10.7: Some scatter plots and their sample correlation coefficient.
</p>
</div>
<p>Based on these plots, you will probably be surprised by the fact that for the data plotted in Figure <a href="correlationchapter.html#fig:guesscorrnew2">10.8</a> the value of the sample correlation coefficient is 0.991. Can you guess what has caused this surprising value?</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:guesscorrnew2"></span>
<img src="images/guess_corr_new2.png" alt="Can you guess the value of the sample correlation coefficient?" width="50%" />
<p class="caption">
Figure 10.8: Can you guess the value of the sample correlation coefficient?
</p>
</div>
</div>
<div id="summary" class="section level3 hasAnchor" number="10.3.5">
<h3><span class="header-section-number">10.3.5</span> Summary<a href="correlationchapter.html#summary" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Correlation can be used when</p>
<ul>
<li>the data are a random sample from the joint distribution of <span class="math inline">\((X,Y)\)</span>,</li>
<li>the association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is approximately linear.</li>
</ul>
<p>Correlation should <strong>not</strong> be used, that is, it can be misleading, if</p>
<ul>
<li>the values of either of the variables are controlled by an experimenter,</li>
<li><span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are associated non-linearly.</li>
</ul>
</div>
<div id="anscombes-datasets" class="section level3 hasAnchor" number="10.3.6">
<h3><span class="header-section-number">10.3.6</span> Anscombe’s datasets<a href="correlationchapter.html#anscombes-datasets" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>It is very important to plot data before fitting a linear regression model or estimating a correlation coefficient. In 1973 F.J. Anscombe (<span class="citation">Anscombe (<a href="#ref-Anscombe1973" role="doc-biblioref">1973</a>)</span>) created 4 datasets to illustrate the need for this. His data are given in Table <a href="correlationchapter.html#fig:anscombetable">10.9</a>. You discussed these data, and some other examples, in Tutorial 1.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:anscombetable"></span>
<img src="images/anscombetable.PNG" alt="Anscombe's datasets." width="60%" />
<p class="caption">
Figure 10.9: Anscombe’s datasets.
</p>
</div>
<p>The datasets have many things in common. For each of the 4 datasets:</p>
<ul>
<li>the sample size is 11,</li>
<li><span class="math inline">\(\bar{x}=9.00\)</span>, <span class="math inline">\(s_x=3.32\)</span>, <span class="math inline">\(\bar{y}=7.50\)</span> and <span class="math inline">\(s_y=2.03\)</span>,</li>
<li>the least squares linear regression line is <span class="math inline">\(y=3+0.5\,x\)</span>, with <span class="math inline">\(RSS\)</span>=13.75,</li>
<li>the sample correlation coefficient <span class="math inline">\(r=0.816\)</span>.</li>
<li>(By the way, the values of Spearman’s rank correlation coefficient <span class="math inline">\(r_S\)</span> are
0.818, 0.691, 0.991 and 0.500 respectively.)</li>
</ul>
<p>Based on these sample statistics we might be tempted to conclude that the association between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> is similar for each of the 4 datasets. However, when we look at scatter plots of the data (Figure <a href="correlationchapter.html#fig:anscombeplot">10.10</a>) it is clear that this is not the case. This demonstrates that summary statistics alone do not describe adequately a probability distribution.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:anscombeplot"></span>
<img src="images/anscombe.png" alt="Scatter plots of Anscombe's datasets. Top left: approximately linear association.  Top right: perfect non-linear association. Bottom left: Perfect linear association apart from 1 outlier.  Bottom right: the increase of 0.5 units in $y$ for each unit increase in $x$ suggested by the regression equation is due to a single observation." width="80%" />
<p class="caption">
Figure 10.10: Scatter plots of Anscombe’s datasets. Top left: approximately linear association. Top right: perfect non-linear association. Bottom left: Perfect linear association apart from 1 outlier. Bottom right: the increase of 0.5 units in <span class="math inline">\(y\)</span> for each unit increase in <span class="math inline">\(x\)</span> suggested by the regression equation is due to a single observation.
</p>
</div>
</div>
<div id="we-must-interpret-correlation-with-care." class="section level3 hasAnchor" number="10.3.7">
<h3><span class="header-section-number">10.3.7</span> We must interpret correlation with care.<a href="correlationchapter.html#we-must-interpret-correlation-with-care." class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose that we find that two variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have a sample correlation which is not close to 0 and that a plot of the sample data suggests that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> may be linearly related. There are several ways that an apparent linear association can arise.</p>
<ol style="list-style-type: decimal">
<li>Changes in <span class="math inline">\(X\)</span> cause changes in <span class="math inline">\(Y\)</span>.</li>
<li>Changes in <span class="math inline">\(Y\)</span> cause changes in <span class="math inline">\(X\)</span>.</li>
<li>Changes in a third variable <span class="math inline">\(Z\)</span> causes changes in both <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</li>
<li>The observed association is just a coincidence.</li>
</ol>
<p>Points 3. and 4. show that correlation does <strong>not</strong> imply causation.</p>
<p>We finish with an example which may illustrate point 3. We have already seen the time series plots in Figure <a href="correlationchapter.html#fig:fluandFTSE">10.11</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fluandFTSE"></span>
<img src="images/ftse_weekly_tufte.png" alt="Weekly FTSE 100 closing prices against numbers of people seeing their doctor about flu over 4-week periods." width="80%" /><img src="images/flu_tufte.png" alt="Weekly FTSE 100 closing prices against numbers of people seeing their doctor about flu over 4-week periods." width="80%" />
<p class="caption">
Figure 10.11: Weekly FTSE 100 closing prices against numbers of people seeing their doctor about flu over 4-week periods.
</p>
</div>
<p>For the period of time over which these series overlap, we extract from the FTSE 100 data the weekly closing price that corresponds to each of the 4-weekly flu values. In Figure <a href="correlationchapter.html#fig:fluvsFTSE">10.12</a> we plot (on a log-log scale) these FTSE values against the flu values.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fluvsFTSE"></span>
<img src="images/flu_FTSE.png" alt="Weekly FTSE 100 closing prices against numbers of people seeing their doctor about flu over 4-week periods." width="60%" />
<p class="caption">
Figure 10.12: Weekly FTSE 100 closing prices against numbers of people seeing their doctor about flu over 4-week periods.
</p>
</div>
<p>We see that there is weak negative association (<span class="math inline">\(r=-0.32\)</span>) between the FTSE 100 and flu values. Does this suggest that flu causes the FTSE 100 share index to drop, or vice versa? This is not a ridiculous suggestion, but the time series plots suggest that the negative association is due to the fact that the FTSE 100 index has <strong>increased</strong> over time, whereas the number of people seeing their doctor about flu has <strong>decreased</strong> over time. There is a third variable, time, which causes changes in both the FTSE 100 and the incidence of flu.</p>

</div>
</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Anscombe1973" class="csl-entry">
Anscombe, F. J. 1973. <span>‘Graphs in Statistical Analysis’</span>. <em>The American Statistician</em> 27 (1): 17–21. <a href="https://doi.org/10.1080/00031305.1973.10478966">https://doi.org/10.1080/00031305.1973.10478966</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linreg.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="a-general-strategy-for-statistical-modelling.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["stat0002book.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
